{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"for subject {subject}: {grouped['Amplitude (x3)']}\")\n",
    "        # for group_key, group_data in grouped:\n",
    "        #     if group_data['Level(dB) (x2)'].min() < thresh:\n",
    "        #         continue\n",
    "            # 1 attempt\n",
    "            # for _, row in group_data.iterrows():\n",
    "            #     x1_val = row['Freq(kHz) (x1)']\n",
    "            #     x3_val = row['Amplitude (x3)']\n",
    "            #     y1_val = y_fold_train.loc[row.name]\n",
    "\n",
    "            #     print(type(y1_val))\n",
    "            #     print(y1_val)\n",
    "            #     subjects_profiles_CVsplit_LR[subject].add_observation_y1(\n",
    "            #         x1=x1_val,\n",
    "            #         x3=x3_val,\n",
    "            #         y1=float(y1_val)\n",
    "            #     )\n",
    "\n",
    "            # # 2 attempt\n",
    "            # for idx in group_data.index:\n",
    "            #     row = group_data.loc[idx]\n",
    "            #     x1_val = row['Freq(kHz) (x1)']\n",
    "            #     x3_val = row['Amplitude (x3)']\n",
    "            #     y1_val = y_fold_train.loc[idx]\n",
    "\n",
    "            #     print(type(x1_val))\n",
    "            #     print(type(x3_val))\n",
    "            #     print(y1_val)\n",
    "\n",
    "            # indices = group_data.index\n",
    "            # y_values = y_fold_train.loc[indices]\n",
    "\n",
    "            # if len(y_values) > 1:\n",
    "            #     for i in [0,1]: \n",
    "            #         x1_val = group_data['Freq(kHz) (x1)'].iloc[i]\n",
    "            #         x3_val = group_data['Amplitude (x3)'].iloc[i]\n",
    "\n",
    "            #         for idx, y1_val in zip(indices, y_values):\n",
    "            #             subjects_profiles_CVsplit_LR[subject].add_observation_y1(\n",
    "            #                 x1=x1_val,x3=x3_val,\n",
    "            #                 y1=float(y1_val),\n",
    "            #             )\n",
    "            # elif len(y_values) == 1:\n",
    "            #         x1_val = group_data['Freq(kHz) (x1)'].values\n",
    "            #         x3_val = group_data['Amplitude (x3)'].values\n",
    "\n",
    "            # for idx, y1_val in zip(indices, y_values):\n",
    "            #     subjects_profiles_CVsplit_LR[subject].add_observation_y1(\n",
    "            #         x1=x1_val,x3=x3_val,\n",
    "            #         y1=float(y1_val),\n",
    "            #     )\n",
    "\n",
    "            # 3 attempt\n",
    "        # for idx in group_data.index:\n",
    "        #     row = group_data.loc[idx]\n",
    "        #     x1_val = row['Freq(kHz) (x1)']\n",
    "        #     x3_val = row['Amplitude (x3)']\n",
    "        #     y1_val = y_fold_train.loc[idx]\n",
    "\n",
    "        #     print(type(x1_val))\n",
    "        #     print(type(x3_val))\n",
    "        #     print(y1_val)\n",
    "\n",
    "        # indices = group_data.index\n",
    "        # y_values = y_fold_train.loc[indices]\n",
    "\n",
    "        # if len(y_values) > 1:\n",
    "\n",
    "        #     if row['vx (x4)'] == 'v1':\n",
    "        #         x1_val = group_data['Freq(kHz) (x1)'].values\n",
    "        #         x3_val = group_data['Amplitude (x3)'].values # iloc[0]\n",
    "\n",
    "        #         subjects_profiles_CVsplit_LR[subject].add_observation_y1(\n",
    "        #             x1=x1_val,x3=x3_val,\n",
    "        #             y1=float(y1_val.iloc[0]),\n",
    "        #         )\n",
    "\n",
    "        #     elif row['vx (x4)'] == 'v2':\n",
    "        #         x1_val = group_data['Freq(kHz) (x1)'].values\n",
    "        #         x3_val = group_data['Amplitude (x3)'].values # iloc[1]\n",
    "\n",
    "        #         subjects_profiles_CVsplit_LR[subject].add_observation_y1(\n",
    "        #             x1=x1_val,x3=x3_val,\n",
    "        #             y1=float(y1_val.iloc[1]),\n",
    "        #         )\n",
    "\n",
    "        # elif len(y_values) == 1:\n",
    "        #     x1_val = group_data['Freq(kHz) (x1)'].values\n",
    "        #     x3_val = group_data['Amplitude (x3)'].values\n",
    "\n",
    "        #     for idx, y1_val in zip(indices, y_values):\n",
    "        #         subjects_profiles_CVsplit_LR[subject].add_observation_y1(\n",
    "        #             x1=x1_val,x3=x3_val,\n",
    "        #             y1=float(y1_val),\n",
    "        #         )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # fixed to have subjects stay within one fold\n",
    "\n",
    "# thresh = 0\n",
    "# k_folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# train_rmse_scores = []\n",
    "# val_rmse_scores = []\n",
    "# subjects_profiles_CVsplit_LR = {}\n",
    "# all_train_data = {}\n",
    "# all_val_data = {}\n",
    "\n",
    "# # Groups setup\n",
    "# groups = [str(group) for group in np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed['Group'])]\n",
    "# group_train_rmse = {group: [] for group in groups}\n",
    "# group_val_rmse = {group: [] for group in groups}\n",
    "\n",
    "# group_k_fold = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# for fold_idx, (train_idx, val_idx) in enumerate(group_k_fold.split(X_train10, groups=X_train10['Subject'])):\n",
    "#     X_fold_train, X_fold_val = X_train10.iloc[train_idx], X_train10.iloc[val_idx]\n",
    "#     y_fold_train, y_fold_val = y1_train10.iloc[train_idx], y1_train10.iloc[val_idx]\n",
    "    \n",
    "#     print(f\"\\nProcessing Fold {fold_idx+1}\")\n",
    "    \n",
    "#     # First, build the Bayesian profiles for each subject\n",
    "#     for subject in np.unique(X_fold_train['Subject']):\n",
    "#         profile_key = f\"{subject}\"\n",
    "#         subjects_profiles_CVsplit_LR[profile_key] = BayesianProfile_LRcompare()\n",
    "        \n",
    "#         subject_data = X_fold_train[X_fold_train['Subject'] == subject]\n",
    "        \n",
    "#         # Group by the combination of features that define unique conditions\n",
    "#         grouped = subject_data.groupby([\n",
    "#             'Freq(kHz) (x1)', 'Amplitude (x3)', 'Strain (x5)',   \n",
    "#             'Group'\n",
    "#         ])\n",
    "        \n",
    "#         # Process each group (condition)\n",
    "#         for group_key, group_data in grouped:\n",
    "#             if group_data['Level(dB) (x2)'].min() < thresh:\n",
    "#                 continue\n",
    "                \n",
    "#             # Get all y1 values for this condition\n",
    "#             indices = group_data.index\n",
    "#             y_values = y_fold_train.loc[indices]\n",
    "            \n",
    "#             # Extract feature values (same for all rows in group)\n",
    "#             x1_val = group_data['Freq(kHz) (x1)'].iloc[0]\n",
    "#             x2_val = group_data['Level(dB) (x2)'].iloc[0]\n",
    "#             x3_val = group_data['Amplitude (x3)'].iloc[0]\n",
    "            \n",
    "#             # Add each observation with same features but different y values\n",
    "#             for idx, y1_val in zip(indices, y_values):\n",
    "#                 subjects_profiles_CVsplit_LR[profile_key].add_observation_y1(\n",
    "#                     x1=x1_val, x2=x2_val, x3=x3_val,\n",
    "#                     y1=float(y1_val),\n",
    "#                 )\n",
    "    \n",
    "#     # Now calculate RMSE for this fold (both training and validation)\n",
    "#     # Store predictions in a dictionary keyed by index\n",
    "#         train_predictions = {}\n",
    "        \n",
    "#         for idx, row in X_fold_train.iterrows():\n",
    "#             if row['Level(dB) (x2)'] >= thresh:\n",
    "#                 subject = row['Subject']\n",
    "#                 profile_key = f\"{subject}\"\n",
    "#                 x1_train = row['Freq(kHz) (x1)']\n",
    "#                 x2_train = row['Level(dB) (x2)']\n",
    "#                 x3_train = row['Amplitude (x3)']\n",
    "#             if profile_key in subjects_profiles_CVsplit_LR:\n",
    "#                 pred = subjects_profiles_CVsplit_LR[profile_key].predict_y1(x1= x1_train, x2=x2_train, x3=x3_train)\n",
    "#                 train_predictions[idx] = float(pred)  # Ensure it's a scalar\n",
    "\n",
    "#         # Create pandas Series with predictions matched to proper indices\n",
    "#         successful_train_indices = list(train_predictions.keys())\n",
    "        \n",
    "#         if successful_train_indices:\n",
    "#             y_train_true = y_fold_train.loc[successful_train_indices]\n",
    "#             y_train_pred = pd.Series([train_predictions[idx] for idx in successful_train_indices], \n",
    "#                                     index=successful_train_indices)\n",
    "            \n",
    "#             # Do the same for validation\n",
    "#         val_predictions = {}\n",
    "        \n",
    "#         for idx, row in X_fold_val.iterrows():\n",
    "#             if row['Level(dB) (x2)'] >= thresh:\n",
    "#                 subject = row['Subject']\n",
    "#                 profile_key = f\"{subject}\"\n",
    "#                 x1_val = row['Freq(kHz) (x1)']\n",
    "#                 x2_val = row['Level(dB) (x2)']\n",
    "#                 x3_val = row['Amplitude (x3)']\n",
    "#             if profile_key in subjects_profiles_CVsplit_LR:\n",
    "#                 pred = subjects_profiles_CVsplit_LR[profile_key].predict_y1(x1= x1_val, x2=x2_val, x3=x3_val)\n",
    "#                 val_predictions[idx] = float(pred)  # Ensure it's a scalar\n",
    "\n",
    "        \n",
    "#         # Create properly indexed validation predictions\n",
    "#         successful_val_indices = list(val_predictions.keys())\n",
    "        \n",
    "#         if successful_val_indices:\n",
    "#             y_val_true = y_fold_val.loc[successful_val_indices]\n",
    "#             y_val_pred = pd.Series([val_predictions[idx] for idx in successful_val_indices], \n",
    "#                                 index=successful_val_indices)\n",
    "            \n",
    "#             # Print basic information\n",
    "#             print(f\"Train: {len(successful_train_indices)} successful predictions out of {len(X_fold_train)}\")\n",
    "#             print(f\"Validation: {len(successful_val_indices)} successful predictions out of {len(X_fold_val)}\")\n",
    "            \n",
    "#             # Calculate RMSE\n",
    "#             true_name = f'{fold_idx} - train - true'\n",
    "#             pred_name = f'{fold_idx} - train - pred'\n",
    "#             all_train_data[true_name] = y_train_true\n",
    "#             all_train_data[pred_name] = y_train_pred\n",
    "\n",
    "#             fold_train_rmse = np.sqrt(np.mean((y_train_true - y_train_pred)**2))\n",
    "#             train_rmse_scores.append(fold_train_rmse)\n",
    "            \n",
    "#             true_name = f'{fold_idx} - val - true'\n",
    "#             pred_name = f'{fold_idx} - val - pred'\n",
    "#             all_val_data[true_name] = y_val_true\n",
    "#             all_val_data[pred_name] = y_val_pred\n",
    "\n",
    "#             fold_val_rmse = np.sqrt(np.mean((y_val_true - y_val_pred)**2))\n",
    "#             val_rmse_scores.append(fold_val_rmse)\n",
    "            \n",
    "#             print(f\"Fold {fold_idx+1}: Train RMSE = {fold_train_rmse:.4f}, Validation RMSE = {fold_val_rmse:.4f}\")\n",
    "                \n",
    "#                 # Calculate group-specific RMSE\n",
    "#     for group in groups:\n",
    "#         # Get the subject groups\n",
    "#         train_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed.loc[successful_train_indices]\n",
    "#         val_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed.loc[successful_val_indices]\n",
    "        \n",
    "#         # Filter for successful predictions for this group\n",
    "#         group_train_mask = train_subjects_df['Group'] == group\n",
    "#         if group_train_mask.any():\n",
    "#             group_indices = group_train_mask.index[group_train_mask]\n",
    "#             group_rmse = np.sqrt(np.mean((y_train_true.loc[group_indices] - y_train_pred.loc[group_indices])**2))\n",
    "#             group_train_rmse[group].append(group_rmse)\n",
    "        \n",
    "#         group_val_mask = val_subjects_df['Group'] == group\n",
    "#         if group_val_mask.any():\n",
    "#             group_indices = group_val_mask.index[group_val_mask]\n",
    "#             group_rmse = np.sqrt(np.mean((y_val_true.loc[group_indices] - y_val_pred.loc[group_indices])**2))\n",
    "#             group_val_rmse[group].append(group_rmse)\n",
    "#     else:\n",
    "#         print(f\"Fold {fold_idx+1}: No successful training predictions\")\n",
    "\n",
    "# # Calculate average RMSE across all folds\n",
    "# if train_rmse_scores:\n",
    "#     avg_train_rmse = np.mean(train_rmse_scores)\n",
    "#     std_train_rmse = np.std(train_rmse_scores)\n",
    "#     print(f\"\\nAverage Train RMSE across folds: {avg_train_rmse:.4f} ± {std_train_rmse:.4f}\")\n",
    "\n",
    "# if val_rmse_scores:\n",
    "#     avg_val_rmse = np.mean(val_rmse_scores)\n",
    "#     std_val_rmse = np.std(val_rmse_scores)\n",
    "#     print(f\"Average Validation RMSE across folds: {avg_val_rmse:.4f} ± {std_val_rmse:.4f}\")\n",
    "\n",
    "# # Calculate group-specific RMSE \n",
    "# print(\"\\nGroup-specific RMSE:\")\n",
    "# print(\"Group | Train RMSE (Mean ± Std) | Validation RMSE (Mean ± Std)\")\n",
    "# print(\"----- | ----------------------- | -----------------------------\")\n",
    "# for group in groups:\n",
    "#     train_values = group_train_rmse[group]\n",
    "#     val_values = group_val_rmse[group]\n",
    "    \n",
    "#     if train_values:\n",
    "#         train_mean = np.mean(train_values)\n",
    "#         train_std = np.std(train_values)\n",
    "#         train_str = f\"{train_mean:.6f} ± {train_std:.6f}\"\n",
    "#     else:\n",
    "#         train_str = \"N/A\"\n",
    "        \n",
    "#     if val_values:\n",
    "#         val_mean = np.mean(val_values)\n",
    "#         val_std = np.std(val_values)\n",
    "#         val_str = f\"{val_mean:.6f} ± {val_std:.6f}\"\n",
    "#     else:\n",
    "#         val_str = \"N/A\"\n",
    "        \n",
    "#     print(f\"{group:15s} | {train_str:25s} | {val_str:25s}\")\n",
    "\n",
    "# # Visualization of group-specific RMSE\n",
    "# if any(group_train_rmse[group] for group in groups) and any(group_val_rmse[group] for group in groups):\n",
    "#     # Prepare data for plotting\n",
    "#     plot_groups = []\n",
    "#     plot_train_rmse = []\n",
    "#     plot_val_rmse = []\n",
    "#     plot_train_std = []\n",
    "#     plot_val_std = []\n",
    "    \n",
    "#     for group in groups:\n",
    "#         if group_train_rmse[group] and group_val_rmse[group]:\n",
    "#             plot_groups.append(group)\n",
    "#             plot_train_rmse.append(np.mean(group_train_rmse[group]))\n",
    "#             plot_val_rmse.append(np.mean(group_val_rmse[group]))\n",
    "#             plot_train_std.append(np.std(group_train_rmse[group]))\n",
    "#             plot_val_std.append(np.std(group_val_rmse[group]))\n",
    "    \n",
    "#     # Create plot if we have data\n",
    "#     if plot_groups:\n",
    "#         plt.figure(figsize=(14, 7))\n",
    "#         x = np.arange(len(plot_groups))\n",
    "#         width = 0.35\n",
    "        \n",
    "#         plt.bar(x - width/2, plot_train_rmse, width, yerr=plot_train_std, \n",
    "#                 label='Train RMSE', color='blue', alpha=0.7, capsize=5)\n",
    "#         plt.bar(x + width/2, plot_val_rmse, width, yerr=plot_val_std,\n",
    "#                 label='Validation RMSE', color='red', alpha=0.7, capsize=5)\n",
    "        \n",
    "#         plt.xlabel('Group')\n",
    "#         plt.ylabel('RMSE')\n",
    "#         plt.ylim((0,5))\n",
    "#         plt.title('Bayesian Model Performance by Group (Mean ± Std)')\n",
    "#         plt.xticks(x, plot_groups, rotation=90)\n",
    "#         plt.legend()\n",
    "#         plt.grid(True, axis='y', alpha=0.3)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "# groups_list = []\n",
    "# train_mean_list = []\n",
    "# train_std_list = []\n",
    "# val_mean_list = []\n",
    "# val_std_list = []\n",
    "\n",
    "# # Extract values for each group\n",
    "# for group in sorted(group_train_rmse.keys()):\n",
    "#     train_values = group_train_rmse[group]\n",
    "#     val_values = group_val_rmse[group]\n",
    "    \n",
    "#     groups_list.append(group)\n",
    "    \n",
    "#     if train_values:\n",
    "#         train_mean_list.append(np.mean(train_values))\n",
    "#         train_std_list.append(np.std(train_values))\n",
    "#     else:\n",
    "#         train_mean_list.append(np.nan)\n",
    "#         train_std_list.append(np.nan)\n",
    "        \n",
    "#     if val_values:\n",
    "#         val_mean_list.append(np.mean(val_values))\n",
    "#         val_std_list.append(np.std(val_values))\n",
    "#     else:\n",
    "#         val_mean_list.append(np.nan)\n",
    "#         val_std_list.append(np.nan)\n",
    "\n",
    "\n",
    "# # Create lists to store the data\n",
    "# groups_list = []\n",
    "# train_mean_list = []\n",
    "# train_std_list = []\n",
    "# val_mean_list = []\n",
    "# val_std_list = []\n",
    "\n",
    "# # Extract values for each group\n",
    "# for group in sorted(group_train_rmse.keys()):\n",
    "#     train_values = group_train_rmse[group]\n",
    "#     val_values = group_val_rmse[group]\n",
    "    \n",
    "#     groups_list.append(group)\n",
    "    \n",
    "#     if train_values:\n",
    "#         train_mean_list.append(np.mean(train_values))\n",
    "#         train_std_list.append(np.std(train_values))\n",
    "#     else:\n",
    "#         train_mean_list.append(np.nan)\n",
    "#         train_std_list.append(np.nan)\n",
    "        \n",
    "#     if val_values:\n",
    "#         val_mean_list.append(np.mean(val_values))\n",
    "#         val_std_list.append(np.std(val_values))\n",
    "#     else:\n",
    "#         val_mean_list.append(np.nan)\n",
    "#         val_std_list.append(np.nan)\n",
    "\n",
    "# group_metrics_df = pd.DataFrame({\n",
    "#     'Group': groups_list,\n",
    "#     'Train_RMSE_Mean': train_mean_list,\n",
    "#     'Val_RMSE_Mean': val_mean_list,\n",
    "#     'Train_RMSE_Std': train_std_list,\n",
    "#     'Val_RMSE_Std': val_std_list\n",
    "# })\n",
    "\n",
    "# group_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 10:57:05.149 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.439 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/new_manorimagepred/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-14 10:57:05.440 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.440 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.440 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.441 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.441 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.442 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.442 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.443 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.443 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.444 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.445 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.445 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.446 Session state does not function when running a script without `streamlit run`\n",
      "2025-04-14 10:57:05.447 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.448 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.449 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.449 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.449 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.450 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.450 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-14 10:57:05.450 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.distributed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ABRA_35 import interpolate_and_smooth, CNN, plot_wave, calculate_and_plot_wave, plot_waves_single_frequency, arfread, get_str, calculate_hearing_threshold, all_thresholds, peak_finding\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import os\n",
    "import struct\n",
    "import datetime\n",
    "# from skfda import FDataGrid\n",
    "# from skfda.preprocessing.dim_reduction import FPCA\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import torch.nn as nn\n",
    "import splitfolders\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.signal import find_peaks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np4\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(1, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (fc1): Linear(in_features=1952, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (dropout_fc): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter1 = 128\n",
    "filter2 = 32\n",
    "dropout1 = 0.5\n",
    "dropout2 = 0.3\n",
    "dropout_fc = 0.1\n",
    "\n",
    "# Model initialization\n",
    "peak_finding_model = CNN(filter1, filter2, dropout1, dropout2, dropout_fc)\n",
    "model_loader = torch.load('./models/waveI_cnn.pth')\n",
    "peak_finding_model.load_state_dict(model_loader)\n",
    "peak_finding_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finding(wave):\n",
    "    # Prepare waveform\n",
    "    waveform=interpolate_and_smooth(wave) # Added indexing per calculate and plot wave function\n",
    "    # waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0) archived ABRA\n",
    "    waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).unsqueeze(0) #newer ABRA\n",
    "    # print(waveform_torch)\n",
    "    # Get prediction from model\n",
    "    outputs = peak_finding_model(waveform_torch)\n",
    "    prediction = int(round(outputs.detach().numpy()[0][0], 0))\n",
    "    # prediction_test = int(round(outputs.detach().numpy()[0], 0))\n",
    "    # print(\"Model output:\", outputs, \"Prediction true start:\", prediction)\n",
    "\n",
    "    # Apply Gaussian smoothing\n",
    "    smoothed_waveform = gaussian_filter1d(waveform, sigma=1)\n",
    "\n",
    "    # Find peaks and troughs\n",
    "    n = 18\n",
    "    t = 14\n",
    "    # start_point = prediction - 9 archived ABRA\n",
    "    start_point = prediction - 6 #newer ABRA\n",
    "    smoothed_peaks, _ = find_peaks(smoothed_waveform[start_point:], distance=n)\n",
    "    smoothed_troughs, _ = find_peaks(-smoothed_waveform, distance=t)\n",
    "    sorted_indices = np.argsort(smoothed_waveform[smoothed_peaks+start_point])\n",
    "    highest_smoothed_peaks = np.sort(smoothed_peaks[sorted_indices[-5:]] + start_point)\n",
    "    relevant_troughs = np.array([])\n",
    "    for p in range(len(highest_smoothed_peaks)):\n",
    "        c = 0\n",
    "        for t in smoothed_troughs:\n",
    "            if t > highest_smoothed_peaks[p]:\n",
    "                if p != 4:\n",
    "                    try:\n",
    "                        if t < highest_smoothed_peaks[p+1]:\n",
    "                            relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                else:\n",
    "                    relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                    break\n",
    "    relevant_troughs = relevant_troughs.astype('i')\n",
    "    return highest_smoothed_peaks, relevant_troughs\n",
    "\n",
    "def extract_metadata(metadata_lines):\n",
    "    # Dictionary to store extracted metadata\n",
    "    metadata = {}\n",
    "    \n",
    "    for line in metadata_lines:\n",
    "        # Extract SW FREQ\n",
    "        freq_match = re.search(r'SW FREQ:\\s*(\\d+\\.?\\d*)', line)\n",
    "        if freq_match:\n",
    "            metadata['SW_FREQ'] = float(freq_match.group(1))\n",
    "        \n",
    "        # Extract LEVELS\n",
    "        levels_match = re.search(r':LEVELS:\\s*([^:]+)', line)\n",
    "        if levels_match:\n",
    "            # Split levels and convert to list of floats\n",
    "            metadata['LEVELS'] = [float(level) for level in levels_match.group(1).split(';') if level]\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def read_custom_tsv(file_path):\n",
    "    # Read the entire file\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split the content into metadata and data sections\n",
    "    metadata_lines = []\n",
    "    data_section = None\n",
    "    \n",
    "    # Find the ':DATA' marker\n",
    "    data_start = content.find(':DATA')\n",
    "    \n",
    "    if data_start != -1:\n",
    "        # Extract metadata (lines before ':DATA')\n",
    "        metadata_lines = content[:data_start].split('\\n')\n",
    "        \n",
    "        # Extract data section\n",
    "        data_section = content[data_start:].split(':DATA')[1].strip()\n",
    "    \n",
    "    # Extract specific metadata\n",
    "    metadata = extract_metadata(metadata_lines)\n",
    "    \n",
    "    # Read the data section directly\n",
    "    try:\n",
    "        # Use StringIO to create a file-like object from the data section\n",
    "        raw_data = pd.read_csv(\n",
    "            io.StringIO(data_section), \n",
    "            sep='\\s+',  # Use whitespace as separator\n",
    "            header=None\n",
    "        )\n",
    "        raw_data = raw_data.T\n",
    "        # Add metadata columns to the DataFrame\n",
    "        if 'SW_FREQ' in metadata:\n",
    "            raw_data['Freq(kHz)'] = metadata['SW_FREQ']\n",
    "            # raw_data['Freq(Hz)'] = raw_data['Freq(Hz)'].apply(lambda x: x*1000)\n",
    "        \n",
    "        if 'LEVELS' in metadata:\n",
    "            # Repeat levels to match the number of rows\n",
    "            levels_repeated = metadata['LEVELS'] * (len(raw_data) // len(metadata['LEVELS']) + 1)\n",
    "            raw_data['Level(dB)'] = levels_repeated[:len(raw_data)]\n",
    "        \n",
    "        filtered_data = raw_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "        filtered_data.columns = filtered_data.columns.map(str)\n",
    "\n",
    "        columns = ['Freq(kHz)'] + ['Level(dB)'] + [col for col in filtered_data.columns if col.isnumeric() == True]\n",
    "        filtered_data = filtered_data[columns]\n",
    "        return filtered_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        return None, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks_troughs_amp_final(df, freq, db, time_scale=10, multiply_y_factor=1.0, units='Microvolts'):\n",
    "    db_column = 'Level(dB)'\n",
    "    \n",
    "    khz = df[(df['Freq(kHz)'] == freq) & (df[db_column] == db)]\n",
    "    if not khz.empty:\n",
    "        index = khz.index.values[0]\n",
    "        final = df.loc[index, '0':].dropna()\n",
    "        final = pd.to_numeric(final, errors='coerce').dropna()\n",
    "\n",
    "        target = int(244 * (time_scale / 10))\n",
    "        \n",
    "        # Process the wave as in calculate_and_plot_wave\n",
    "        y_values = interpolate_and_smooth(final, target)\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        y_values *= multiply_y_factor\n",
    "        \n",
    "        # Handle units conversion if needed\n",
    "        if units == 'Nanovolts':\n",
    "            y_values /= 1000\n",
    "            \n",
    "        # Generate normalized version for peak finding\n",
    "        y_values_fpf = interpolate_and_smooth(y_values[:244])\n",
    "        \n",
    "        # Standardize and normalize for peak finding, exactly as in the original\n",
    "        flattened_data = y_values_fpf.flatten().reshape(-1, 1)\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(flattened_data)\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = min_max_scaler.fit_transform(standardized_data).reshape(y_values_fpf.shape)\n",
    "        y_values_fpf = interpolate_and_smooth(scaled_data[:244])\n",
    "        \n",
    "        # Find peaks using the normalized data\n",
    "        highest_peaks, relevant_troughs = peak_finding(y_values_fpf)\n",
    "        \n",
    "        # Calculate amplitude on the processed but non-normalized data\n",
    "        if highest_peaks.size > 0 and relevant_troughs.size > 0:\n",
    "            # Following the same approach as in the display_metrics_table function\n",
    "            first_peak_amplitude = y_values[highest_peaks[0]] - y_values[relevant_troughs[0]]\n",
    "            return highest_peaks, relevant_troughs, first_peak_amplitude\n",
    "    \n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del int\n",
    "time_scale = 18\n",
    "amp_per_freq = {'Subject': [], 'Freq(kHz) (x1)': [], 'Level(dB) (x2)': [], 'Amplitude (x3)':[]}\n",
    "start_path = '/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/WPZ Electrophysiology'\n",
    "for subject in os.listdir(start_path):\n",
    "    # print(\"Subject:\",subject)\n",
    "    for fq in os.listdir(os.path.join(start_path,subject)):\n",
    "        # print(fq)\n",
    "        if fq.startswith('ABR') and fq.endswith('.tsv'):\n",
    "            path = os.path.join(start_path,subject,fq)\n",
    "            data_df = read_custom_tsv(path)\n",
    "            # print(data_df)\n",
    "            freqs = data_df['Freq(kHz)'].unique().tolist()\n",
    "            levels = data_df['Level(dB)'].unique().tolist()\n",
    "            for freq in freqs:\n",
    "                for lvl in levels:\n",
    "                    # print(\"Frequency=\",freq, \"Level=\", lvl)\n",
    "                    _, _, amp = peaks_troughs_amp_final(df=data_df, freq=freq, db=lvl, time_scale=time_scale)\n",
    "                    # print(f'Amplitude: {amp}\\n')\n",
    "                    amp_per_freq['Subject'].append(subject)\n",
    "                    amp_per_freq['Freq(kHz) (x1)'].append(freq)\n",
    "                    amp_per_freq['Level(dB) (x2)'].append(lvl)\n",
    "                    amp_per_freq['Amplitude (x3)'].append(amp)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "amp_df_full = pd.DataFrame(data=amp_per_freq)\n",
    "\n",
    "raw_synapse_counts = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Ribbon and Synapse Counts.xlsx')\n",
    "raw_synapse_counts = raw_synapse_counts.mask(lambda x: x.isnull()).dropna()\n",
    "raw_synapse_counts['Synapses to IHC (y1)'] = raw_synapse_counts.iloc[:,6]\n",
    "raw_synapse_counts['vx (x4)'] = raw_synapse_counts['vx']\n",
    "raw_synapse_counts.drop(columns=['vx'], inplace=True)\n",
    "raw_synapse_counts.rename(columns={'Freq':'Freq(kHz) (x1)'}, inplace=True)\n",
    "# raw_synapse_counts['Freq(Hz) (x1)'] = raw_synapse_counts['Freq(Hz) (x1)'].apply(lambda x: x*1000) # PUTTING BACK\n",
    "raw_synapse_counts.rename(columns={'Case':'Subject'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 - values per vx\n",
    "\n",
    "paired = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(kHz) (x1)']), on=['Subject', 'Freq(kHz) (x1)'])\n",
    "# slice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs (y2)']]\n",
    "final = paired[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs']]\n",
    "final_clean = final.dropna()\n",
    "\n",
    "# adding in the strain feature\n",
    "strains = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Mouse groups.xlsx')\n",
    "final_clean_strained = final_clean.join(strains.set_index('ID#'), on='Subject')\n",
    "final_clean_strained['Strain'] = final_clean_strained['Strain'].str.strip()\n",
    "final_clean_strained = final_clean_strained.rename(columns={'Strain': 'Strain (x5)'})\n",
    "final_clean_strained = final_clean_strained.dropna()\n",
    "final_clean_strained = final_clean_strained[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Synapses to IHC (y1)', 'Group']]\n",
    "\n",
    "final_clean_strained_grouped = final_clean_strained.copy()\n",
    "final_clean_strained_grouped['Group - dB'] = final_clean_strained_grouped['Group'].apply(lambda x: x.split(' ')[0] if x.split(' ')[0].endswith('dB') else 'Control')\n",
    "final_clean_strained_grouped['Group - Time Elapsed'] = final_clean_strained_grouped['Group'].apply(lambda x: x.split(' ')[1] if x.split(' ')[1].endswith(('h', 'wks', 'w')) else x.split(' ')[0])\n",
    "final_clean_strained_grouped.head()\n",
    "\n",
    "final_clean_strained_grouped_pos = final_clean_strained_grouped.copy()\n",
    "final_clean_strained_grouped_pos['Amplitude (x3)'] = final_clean_strained_grouped['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup = final_clean_strained_grouped_pos.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup['Group'] = final_clean_strained_grouped_pos_cleangroup['Group'].apply(lambda x: x.strip())\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup.head()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs = final_clean_strained_grouped_pos_cleangroup.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - dB']\n",
    "# final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed']\n",
    "final_clean_strained_grouped_pos_cleangroup_vs = final_clean_strained_grouped_pos_cleangroup_vs[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)',\n",
    "       'vx (x4)', 'Strain (x5)','Group - dB (x6)', 'Group - Time Elapsed', 'Group','Synapses to IHC (y1)']]\n",
    "\n",
    "def split_on_number(input_string):\n",
    "    return re.findall(r\"[A-Za-z]+|\\d+\", input_string)\n",
    "\n",
    "hrs_week = 24*7\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed = final_clean_strained_grouped_pos_cleangroup_vs.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: '0dB' if x == 'Control' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'].apply(lambda x: x[1])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'].apply(lambda x: \"wks\" if x == 'w' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Hours Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed.apply(lambda row: row['Group - Time Elapsed - Magn.']* hrs_week if row['Group - Time Elapsed - Unit'] == 'wks' else row['Group - Time Elapsed - Magn.'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        9.831461\n",
       "0        9.831461\n",
       "1        9.831461\n",
       "1        9.831461\n",
       "2        9.831461\n",
       "          ...    \n",
       "7328    16.170213\n",
       "7329    16.170213\n",
       "7329    16.170213\n",
       "7330    16.170213\n",
       "7330    16.170213\n",
       "Name: Synapse to IHC Ratio per Freq (y2), Length: 12187, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2 - Averaged per Vx\n",
    "\n",
    "paired2 = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(kHz) (x1)']), on=['Subject', 'Freq(kHz) (x1)'])\n",
    "# lilslice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs']]\n",
    "final2 = paired2[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'Synapses', 'IHCs']]\n",
    "final_clean2 = final2.dropna()\n",
    "\n",
    "# adding in the strain feature\n",
    "strains = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Mouse groups.xlsx')\n",
    "final_clean_strained2 = final_clean2.join(strains.set_index('ID#'), on='Subject')\n",
    "final_clean_strained2['Strain'] = final_clean_strained2['Strain'].str.strip()\n",
    "final_clean_strained2 = final_clean_strained2.rename(columns={'Strain': 'Strain (x5)'})\n",
    "final_clean_strained2 = final_clean_strained2.dropna()\n",
    "final_clean_strained2 = final_clean_strained2[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Synapses to IHC (y1)', 'Group', 'Synapses', 'IHCs']]\n",
    "# np.unique(final_clean_strained2['Group'])\n",
    "\n",
    "# final_clean_70 = final_clean[final_clean['Level(dB) (x2)'] >= 70.0]\n",
    "# final_clean_strained_70 = final_clean_strained[final_clean_strained['Level(dB) (x2)'] >= 70.0]\n",
    "# # np.unique(final_clean['Level(dB) (x2)']) max level is 80 db\n",
    "# len(final_clean), len(final_clean_70) # 10000 less data points!!!\n",
    "\n",
    "final_clean_strained_grouped2 = final_clean_strained2.copy()\n",
    "final_clean_strained_grouped2['Group - dB'] = final_clean_strained_grouped2['Group'].apply(lambda x: x.split(' ')[0] if x.split(' ')[0].endswith('dB') else 'Control')\n",
    "final_clean_strained_grouped2['Group - Time Elapsed'] = final_clean_strained_grouped2['Group'].apply(lambda x: x.split(' ')[1] if x.split(' ')[1].endswith(('h', 'wks', 'w')) else x.split(' ')[0])\n",
    "final_clean_strained_grouped2.head()\n",
    "\n",
    "final_clean_strained_grouped_pos2 = final_clean_strained_grouped2.copy()\n",
    "final_clean_strained_grouped_pos2['Amplitude (x3)'] = final_clean_strained_grouped2['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "len(final_clean_strained_grouped_pos2[final_clean_strained_grouped_pos2['Amplitude (x3)'] < 0])\n",
    "\n",
    "final_clean_strained_grouped_pos2['Amplitude (x3)'] = final_clean_strained_grouped2['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "# final_clean_strained_grouped_pos[(final_clean_strained_grouped_pos['Subject'] == 'WPZ66') & (final_clean_strained_grouped_pos['Amplitude (x3)'] ==0.055901451434921576)\n",
    "final_clean_strained_grouped_pos_cleangroup2 = final_clean_strained_grouped_pos2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup2['Group'] = final_clean_strained_grouped_pos_cleangroup2['Group'].apply(lambda x: x.strip())\n",
    "np.unique(final_clean_strained_grouped_pos_cleangroup2['Group'])\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup2.head()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2 = final_clean_strained_grouped_pos_cleangroup2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs2['Group - dB']\n",
    "# final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed']\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2 = final_clean_strained_grouped_pos_cleangroup_vs2[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)',\n",
    "       'vx (x4)', 'Strain (x5)','Group - dB (x6)', 'Group - Time Elapsed', 'Group','Synapses to IHC (y1)', 'Synapses', 'IHCs']]\n",
    "\n",
    "def split_on_number(input_string):\n",
    "    return re.findall(r\"[A-Za-z]+|\\d+\", input_string)\n",
    "\n",
    "hrs_week = 24*7\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2 = final_clean_strained_grouped_pos_cleangroup_vs2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: '0dB' if x == 'Control' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'].apply(lambda x: x[1])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'].apply(lambda x: \"wks\" if x == 'w' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Hours Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2.apply(lambda row: row['Group - Time Elapsed - Magn.']* hrs_week if row['Group - Time Elapsed - Unit'] == 'wks' else row['Group - Time Elapsed - Magn.'], axis = 1)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2\n",
    "\n",
    "freqs = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2['Freq(kHz) (x1)'])\n",
    "subs = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2['Subject'])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx = final_clean_strained_grouped_pos_cleangroup_vs_timed2.copy()\n",
    "for freq in freqs:\n",
    "    for sub in subs:\n",
    "        mask = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)] # global for updates\n",
    "        if len(mask) > 0:\n",
    "\n",
    "            mask1 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['vx (x4)'] == 'v1')]\n",
    "            mask2 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['vx (x4)'] == 'v2')]\n",
    "\n",
    "            if not mask1.empty and not mask2.empty:\n",
    "                mask1 = mask1.reset_index().iloc[0,:]\n",
    "                mask2 = mask2.reset_index().iloc[0,:]\n",
    "\n",
    "                total_syns = float(mask1['Synapses'] + mask2['Synapses'])\n",
    "                total_ihcs = float(mask1['IHCs'] + mask2['IHCs'])\n",
    "                ratio = total_syns / total_ihcs\n",
    "                # print(total_syns, total_ihcs)\n",
    "                # if total_syns == 0.0 or total_ihcs == 0.0:\n",
    "                #     print(sub, freq)\n",
    "                mask_index = mask.index\n",
    "                final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[mask_index,'Synapse to IHC Ratio per Freq (y2)'] = ratio\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Synapse to IHC Ratio per Freq (y2)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianProfile_LRcompare:\n",
    "    def __init__(self, training_data=None, prior_mean_y1=None, prior_cov_y1=None):\n",
    "        \"\"\"\n",
    "        Initialize a Bayesian profile with static scaling\n",
    "        \n",
    "        Parameters:\n",
    "        training_data: DataFrame with initial data to fit scalers\n",
    "        prior_mean: Initial guess for parameters (default: zeros)\n",
    "        prior_cov: Initial uncertainty in parameters (default: identity matrix)\n",
    "        \"\"\"\n",
    "        # For 48 parameters: intercept, x1, x3, interaction\n",
    "        self.n_params = 4\n",
    "        \n",
    "        # Initialize priors\n",
    "        if prior_mean_y1 is None:\n",
    "            self.mean_y1 = np.zeros(self.n_params)\n",
    "        else:\n",
    "            self.mean_y1 = prior_mean_y1\n",
    "            \n",
    "        if prior_cov_y1 is None:\n",
    "            self.cov_y1 = np.eye(self.n_params) * 10  # Start with high uncertainty\n",
    "        else:\n",
    "            self.cov_y1 = prior_cov_y1\n",
    "\n",
    "        # Keep track of all data points\n",
    "        self.X_history = []\n",
    "        self.y1_history = []\n",
    "        \n",
    "        # For tracking prediction performance\n",
    "        self.rmse_history_y1 = []\n",
    "        \n",
    "        # Initialize scalers\n",
    "        self.x1_scaler = StandardScaler()\n",
    "        self.x3_scaler = StandardScaler()\n",
    "        \n",
    "        # Initialize scaling flag\n",
    "        self.scaling_applied = False\n",
    "        \n",
    "        # Fit scalers if training data is provided\n",
    "        if training_data is not None:\n",
    "            self._initialize_scalers(training_data)\n",
    "\n",
    "    def _initialize_scalers(self, data):\n",
    "        \"\"\"\n",
    "        Initialize scalers with training data\n",
    "        \"\"\"\n",
    "        if 'Freq(kHz) (x1)' in data.columns and 'Amplitude (x3)' in data.columns:\n",
    "            # Fit scalers to all training data once\n",
    "            self.x1_scaler.fit(data['Freq(kHz) (x1)'].values.reshape(-1,1))\n",
    "            self.x3_scaler.fit(data['Amplitude (x3)'].values.reshape(-1, 1))\n",
    "            self.scaling_applied = True\n",
    "            print(f\"Scalers initialized with {len(data)} records\")\n",
    "        else:\n",
    "            print(\"Warning: Training data missing required columns for scaling\")\n",
    "\n",
    "    def _scale_x1(self, x1):\n",
    "        \"\"\"\n",
    "        Scale x3 using the fitted scaler\n",
    "        \"\"\"\n",
    "        if not self.scaling_applied:\n",
    "            return x1  # Return as is if scaler not fit\n",
    "        \n",
    "        x1_array = np.array([float(x1)]).reshape(-1, 1)\n",
    "        return self.x1_scaler.transform(x1_array)[0][0]\n",
    "\n",
    "    def _scale_x3(self, x3):\n",
    "        \"\"\"\n",
    "        Scale x3 using the fitted scaler\n",
    "        \"\"\"\n",
    "        if not self.scaling_applied:\n",
    "            return x3\n",
    "        \n",
    "        x3_array = np.array([float(x3)]).reshape(-1, 1)\n",
    "        return self.x3_scaler.transform(x3_array)[0][0]\n",
    "        \n",
    "    def add_observation_y1(self, x1, x3, y1, noise_var=1.0, prior_mean_y1=None, prior_cov_y1=None):\n",
    "        \"\"\"\n",
    "        Update the profile with a new observation using static scaling\n",
    "        \"\"\"\n",
    "        self.raw_x1 = float(x1)\n",
    "        self.raw_x3 = float(x3)\n",
    "        self.raw_y1 = float(y1)\n",
    "        \n",
    "        x1_scaled = self._scale_x1(x1)\n",
    "        x3_scaled = self._scale_x3(x3)\n",
    "        \n",
    "        # Reshape for processing\n",
    "        x1_scaled = np.asarray(x1_scaled).reshape(1, -1)\n",
    "        x3_scaled = np.asarray(x3_scaled).reshape(1, -1)\n",
    "        \n",
    "        arrays_to_stack = [\n",
    "            np.ones(1).reshape(1, -1),        \n",
    "            x1_scaled,\n",
    "            x3_scaled,\n",
    "            (x1_scaled.T @ x3_scaled).reshape(1,-1)                                  \n",
    "        ]\n",
    "\n",
    "        X = np.hstack(arrays_to_stack)\n",
    "        self.X_history.append(X[0])\n",
    "        self.y1_history.append(float(y1))\n",
    "        \n",
    "        # Numerical stability in matrix inversion\n",
    "        try:\n",
    "            # regularization for numerical stability\n",
    "            K = self.cov_y1 @ X.T @ np.linalg.inv(X @ self.cov_y1 @ X.T + noise_var + 1e-8 * np.eye(X.shape[0]))\n",
    "            \n",
    "            innovation = y1 - float(X @ self.mean_y1)\n",
    "            self.mean_y1 = self.mean_y1 + (K.flatten() * innovation) # our coefficients/parameters!\n",
    "            self.cov_y1 = self.cov_y1 - (K @ X @ self.cov_y1) # prep for our next set of observations\n",
    "        except np.linalg.LinAlgError as e:\n",
    "            print(f\"Matrix inversion error: {e}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            predictions = []\n",
    "            for i, x_hist in enumerate(self.X_history):\n",
    "                # Extract features from history\n",
    "                hist_x1 = x_hist[1]\n",
    "                hist_x3 = x_hist[2]\n",
    "                    \n",
    "                try:\n",
    "                    # print(f\"self.mean_y1: {self.mean_y1}, self.cov_y1 shape: {self.cov_y1.shape}\")\n",
    "                    # print(f\"Trying to predict with x1={hist_x1}, x3={hist_x3}\")\n",
    "                    pred = self.predict_y1(x1=hist_x1, x3=hist_x3)\n",
    "                    # print(pred)\n",
    "                    if pred is not None and np.isfinite(pred):\n",
    "                        predictions.append(pred)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in prediction: {e}.\")\n",
    "                    \n",
    "            if predictions:\n",
    "                rmse = np.sqrt(np.mean((np.array(self.y1_history) - np.array(predictions))**2)) # predictions likely source of errorrrrrr\n",
    "                print(rmse)\n",
    "                if np.isfinite(rmse):\n",
    "                    print(rmse)\n",
    "                    self.rmse_history_y1.append(rmse)\n",
    "                else:\n",
    "                    # If we got an invalid RMSE, append the last valid one or 0\n",
    "                    if self.rmse_history_y1:\n",
    "                        self.rmse_history_y1.append(self.rmse_history_y1[-1])\n",
    "                        print(\"Invalid RMSE!! Check here\")\n",
    "                    else:\n",
    "                        self.rmse_history_y1.append(0.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error !calculating! RMSE: {e}\")\n",
    "            # Append last RMSE or 0 if none exists\n",
    "            if self.rmse_history_y1:\n",
    "                self.rmse_history_y1.append(self.rmse_history_y1[-1])\n",
    "            else:\n",
    "                self.rmse_history_y1.append(0.0)\n",
    "\n",
    "    def predict_y1(self, x1,x3):\n",
    "        \"\"\"\n",
    "        Make a prediction for given input values with static scaling\n",
    "        \n",
    "        Parameters:\n",
    "        x1, x3: Input features (will be scaled if scaling is enabled)\n",
    "        x5: Categorical feature\n",
    "        x6, x7: Additional input features\n",
    "        \n",
    "        Returns:\n",
    "        float: Predicted value (in original scale)\n",
    "        \"\"\"\n",
    "        # Apply scaling if enabled\n",
    "        x1_scaled = self._scale_x1(x1)\n",
    "        x3_scaled = self._scale_x3(x3)\n",
    "        \n",
    "        # Reshape and encode\n",
    "        x1_scaled = np.asarray(x1_scaled).reshape(1, -1)\n",
    "        x3_scaled = np.asarray(x3_scaled).reshape(1, -1)\n",
    "        \n",
    "        # Create arrays to stack with correct shapes\n",
    "        arrays_to_stack = [\n",
    "            np.ones(1).reshape(1, -1),        \n",
    "            x1_scaled,\n",
    "            x3_scaled,\n",
    "            np.array(x1_scaled * x3_scaled).reshape(1, -1)                                  \n",
    "        ]\n",
    "\n",
    "        X = np.hstack(arrays_to_stack)\n",
    "\n",
    "        raw_pred = float(X @ self.mean_y1) # mean_y1 likely error source\n",
    "        if not np.isfinite(raw_pred):\n",
    "            print(f\"Non-finite prediction for x1={x1_scaled},x3={x3_scaled}\")\n",
    "            # print(f\"Model coefficients: {self.mean_y1}\")\n",
    "        pred = max(0, raw_pred)\n",
    "        return pred\n",
    "\n",
    "    def predict_with_uncertainty_y1(self, x1, x3):\n",
    "        \"\"\"\n",
    "        Make a prediction with uncertainty bounds\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (prediction, standard_deviation)\n",
    "        \"\"\"\n",
    "        # Apply scaling if enabled\n",
    "        x1_scaled = self._scale_x1(x1)\n",
    "        x3_scaled = self._scale_x3(x3)\n",
    "        \n",
    "        # Reshape and encode\n",
    "        x1_scaled = np.asarray(x1_scaled).reshape(1, -1)\n",
    "        x3_scaled = np.asarray(x3_scaled).reshape(1, -1)\n",
    "\n",
    "        # Create arrays to stack with correct shapes\n",
    "        arrays_to_stack = [\n",
    "            np.ones(1).reshape(1, -1),        \n",
    "            x1_scaled,\n",
    "            x3_scaled,\n",
    "            np.array(x1_scaled * x3_scaled).reshape(1, -1)                            \n",
    "        ]\n",
    "        \n",
    "        X = np.hstack(arrays_to_stack)\n",
    "        \n",
    "        # Predict\n",
    "        pred = float(X @ self.mean_y1)\n",
    "        std = float(np.sqrt(X @ self.cov_y1 @ X.T))\n",
    "        # pred_scaled = float(X @ self.mean_y1)\n",
    "        # std_scaled = float(np.sqrt(X @ self.cov_y1 @ X.T))\n",
    "\n",
    "        # if self.scaling_applied:\n",
    "        #     std = std_scaled * self.y1_scaler.scale_[0]\n",
    "        # else:\n",
    "        #     std = std_scaled\n",
    "        \n",
    "        # Check for valid values\n",
    "        if not np.isfinite(pred) or not np.isfinite(std):\n",
    "            print(f\"Warning: Non-finite prediction or std: {pred}, {std}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        # pred = self._inverse_scale_y1(pred_scaled)\n",
    "\n",
    "        pred = float(X @ self.mean_y1)\n",
    "        pred = max(0, pred)\n",
    "        std = float(np.sqrt(X @ self.cov_y1 @ X.T))\n",
    "\n",
    "        if not np.isfinite(pred) or not np.isfinite(std):\n",
    "            print(f\"Warning: Non-finite prediction or std: {pred}, {std}\")\n",
    "            return None, None\n",
    "        # Convert back to original scale if scaling was applied\n",
    "        return pred, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV - Grouped by Subject, Randomly test frequency, original target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [8.0, 22.6], 1: [11.3, 32.0], 2: [16.0, 45.2]}\n"
     ]
    }
   ],
   "source": [
    "# Split data based on ABR recording frequencies and compare!!!!!!!\n",
    "# Because it doesn't make sense to have TT by subject for this given one model is specific to a subject...\n",
    "np.random.seed(12321)\n",
    "groups = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'])\n",
    "sorted_freqs = sorted(np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)']))\n",
    "\n",
    "freq_groups = {}\n",
    "for i, freq in enumerate(sorted_freqs):\n",
    "    # Splits up the frequencies into n groups\n",
    "    group_idx = i % 3\n",
    "    if group_idx not in freq_groups:\n",
    "        freq_groups[group_idx] = []\n",
    "    freq_groups[group_idx].append(freq)\n",
    "\n",
    "print(freq_groups)\n",
    "\n",
    "records_by_group = {}\n",
    "for group in final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'].unique():\n",
    "    records_in_group = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'] == group]\n",
    "    records_by_group[group] = records_in_group\n",
    "\n",
    "train_freqs = []\n",
    "test_freqs = []\n",
    "\n",
    "for group_idx, freqs in freq_groups.items():\n",
    "    # Shuffle frequencies within this group (kinda like RF. randomly splits on which freqs to use in train/test)\n",
    "    np.random.shuffle(freqs)\n",
    "    \n",
    "    n_test = max(1, round(len(freqs) * 0.2))  # Ensuring at least 2 frequencies are used for testing\n",
    "\n",
    "    # Add to overall train/test sets\n",
    "    test_freqs.extend(freqs[:n_test])\n",
    "    train_freqs.extend(freqs[n_test:])\n",
    "\n",
    "# train_indices = []\n",
    "# test_indices = []\n",
    "\n",
    "# for group in np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group']):\n",
    "#     group_recs = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'] == group]\n",
    "#     train_group_indices = group_recs[group_recs['Freq(kHz) (x1)'].isin(train_freqs)].index.tolist()\n",
    "#     test_group_indices = group_recs[group_recs['Freq(kHz) (x1)'].isin(test_freqs)].index.tolist()\n",
    "\n",
    "    # test_indices.extend(test_group_indices)\n",
    "    # train_indices.extend(train_group_indices)\n",
    "\n",
    "train_indices = set()\n",
    "test_indices = set()\n",
    "\n",
    "for subject in np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject']):\n",
    "    subject_recs = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == subject]\n",
    "\n",
    "    train_subject_indices = subject_recs[subject_recs['Freq(kHz) (x1)'].isin(train_freqs)].index.tolist()\n",
    "    test_subject_indices = subject_recs[subject_recs['Freq(kHz) (x1)'].isin(test_freqs)].index.tolist()\n",
    "\n",
    "    # Ensure the subject has data in both splits\n",
    "    if len(train_subject_indices) == 0 or len(test_subject_indices) == 0:\n",
    "        continue  # Skip this subject to avoid leakage problems\n",
    "\n",
    "    test_indices.update(test_subject_indices)\n",
    "    train_indices.update(train_subject_indices)\n",
    "\n",
    "\n",
    "# x_inputs = ['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Group']\n",
    "# y_inputs = ['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'Synapses to IHC (y1)', 'Synapse to IHC Ratio per Freq (y2)']\n",
    "# X_train10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(train_indices), x_inputs].reset_index(drop=True)\n",
    "# X_test10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(test_indices), x_inputs].reset_index(drop=True)\n",
    "\n",
    "# y1_train10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(train_indices), y_inputs].reset_index(drop=True)\n",
    "# y1_test10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(test_indices), y_inputs].reset_index(drop=True)\n",
    "\n",
    "# y2_train10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(train_indices), y_inputs].reset_index(drop=True)\n",
    "# y2_test10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(test_indices), y_inputs].reset_index(drop=True)\n",
    "\n",
    "y1_inputs = ['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Group', 'Synapses to IHC (y1)']\n",
    "y2_inputs = ['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Group', 'Synapse to IHC Ratio per Freq (y2)']\n",
    "\n",
    "predict_per_vx_train = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(train_indices), y1_inputs].reset_index(drop=True)\n",
    "predict_per_freq_train = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(train_indices), y2_inputs].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Freq(kHz) (x1)</th>\n",
       "      <th>Level(dB) (x2)</th>\n",
       "      <th>Amplitude (x3)</th>\n",
       "      <th>vx (x4)</th>\n",
       "      <th>Strain (x5)</th>\n",
       "      <th>Group</th>\n",
       "      <th>Synapse to IHC Ratio per Freq (y2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.046317</td>\n",
       "      <td>v1</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.046317</td>\n",
       "      <td>v2</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.115609</td>\n",
       "      <td>v1</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.115609</td>\n",
       "      <td>v2</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.323602</td>\n",
       "      <td>v1</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.323602</td>\n",
       "      <td>v2</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.523731</td>\n",
       "      <td>v1</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.523731</td>\n",
       "      <td>v2</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.637744</td>\n",
       "      <td>v1</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.637744</td>\n",
       "      <td>v2</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.859953</td>\n",
       "      <td>v1</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.859953</td>\n",
       "      <td>v2</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.295117</td>\n",
       "      <td>v1</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WPZ145</td>\n",
       "      <td>22.6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.295117</td>\n",
       "      <td>v2</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>98dB 8wks post</td>\n",
       "      <td>11.534653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject  Freq(kHz) (x1)  Level(dB) (x2)  Amplitude (x3) vx (x4)  \\\n",
       "0   WPZ145            22.6            30.0        0.046317      v1   \n",
       "1   WPZ145            22.6            30.0        0.046317      v2   \n",
       "2   WPZ145            22.6            35.0        0.115609      v1   \n",
       "3   WPZ145            22.6            35.0        0.115609      v2   \n",
       "4   WPZ145            22.6            40.0        0.323602      v1   \n",
       "5   WPZ145            22.6            40.0        0.323602      v2   \n",
       "6   WPZ145            22.6            50.0        0.523731      v1   \n",
       "7   WPZ145            22.6            50.0        0.523731      v2   \n",
       "8   WPZ145            22.6            60.0        0.637744      v1   \n",
       "9   WPZ145            22.6            60.0        0.637744      v2   \n",
       "10  WPZ145            22.6            70.0        0.859953      v1   \n",
       "11  WPZ145            22.6            70.0        0.859953      v2   \n",
       "12  WPZ145            22.6            80.0        1.295117      v1   \n",
       "13  WPZ145            22.6            80.0        1.295117      v2   \n",
       "\n",
       "   Strain (x5)           Group  Synapse to IHC Ratio per Freq (y2)  \n",
       "0        C57B6  98dB 8wks post                           11.534653  \n",
       "1        C57B6  98dB 8wks post                           11.534653  \n",
       "2        C57B6  98dB 8wks post                           11.534653  \n",
       "3        C57B6  98dB 8wks post                           11.534653  \n",
       "4        C57B6  98dB 8wks post                           11.534653  \n",
       "5        C57B6  98dB 8wks post                           11.534653  \n",
       "6        C57B6  98dB 8wks post                           11.534653  \n",
       "7        C57B6  98dB 8wks post                           11.534653  \n",
       "8        C57B6  98dB 8wks post                           11.534653  \n",
       "9        C57B6  98dB 8wks post                           11.534653  \n",
       "10       C57B6  98dB 8wks post                           11.534653  \n",
       "11       C57B6  98dB 8wks post                           11.534653  \n",
       "12       C57B6  98dB 8wks post                           11.534653  \n",
       "13       C57B6  98dB 8wks post                           11.534653  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_per_freq_train[(predict_per_freq_train['Subject'] == 'WPZ145')\\\n",
    "            & (predict_per_freq_train['Freq(kHz) (x1)'] == 22.6)]\n",
    "\n",
    "# type(y2_train10['Freq(kHz) (x1)'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y1_train10[(y1_train10['Subject'] == 'WPZ100') & (y1_train10['Freq(kHz) (x1)'] == 16.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fixed to have subjects stay within one fold\n",
    "\n",
    "# thresh = 0\n",
    "# train_rmse_scores = []\n",
    "# val_rmse_scores = []\n",
    "# subjects_profiles_CVsplit_LR = {}\n",
    "# all_train_data = {}\n",
    "# all_val_data = {}\n",
    "\n",
    "# groups = [str(group) for group in np.unique(X_train10['Group'])]\n",
    "# group_train_rmse = {group: [] for group in groups}\n",
    "# group_val_rmse = {group: [] for group in groups}\n",
    "\n",
    "# subject_group_df = X_train10.groupby('Subject').first().reset_index()\n",
    "# subject_labels = subject_group_df['Group'].values\n",
    "# subject_ids = subject_group_df['Subject'].values\n",
    "\n",
    "# group_k_fold = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# for fold_idx, (train_idx, val_idx) in enumerate(group_k_fold.split(X_train10, groups=X_train10['Subject'])):\n",
    "#     X_fold_train, X_fold_val = X_train10.iloc[train_idx], X_train10.iloc[val_idx]\n",
    "#     y_fold_train, y_fold_val = y1_train10.iloc[train_idx], y1_train10.iloc[val_idx]\n",
    "#     print(f\"\\nProcessing Fold {fold_idx+1}\")\n",
    "\n",
    "#     for subject in np.unique(X_fold_train['Subject']):\n",
    "#         subjects_profiles_CVsplit_LR[subject] = BayesianProfile_LRcompare()\n",
    "\n",
    "#         subject_data = X_fold_train[X_fold_train['Subject'] == subject]\n",
    "\n",
    "#         grouped = subject_data.groupby([\n",
    "#             'Freq(kHz) (x1)', 'Amplitude (x3)', 'Strain (x5)',   \n",
    "#             'Group'\n",
    "#         ])\n",
    "\n",
    "#         for group_key, group_data in grouped:\n",
    "#             if group_data['Level(dB) (x2)'].min() < thresh:\n",
    "#                 continue\n",
    "\n",
    "#             for idx, row in group_data.iterrows():\n",
    "#                 x1_val = row['Freq(kHz) (x1)']\n",
    "#                 x3_val = row['Amplitude (x3)']\n",
    "#                 y1_val = y_fold_train.loc[idx]['Synapses to IHC (y1)']\n",
    "\n",
    "#                 # print(type(x1_val))\n",
    "#                 # print(type(x3_val))\n",
    "#                 # print(y1_val)\n",
    "\n",
    "#                 subjects_profiles_CVsplit_LR[subject].add_observation_y1(\n",
    "#                     x1=x1_val,\n",
    "#                     x3=x3_val,\n",
    "#                     y1=float(y1_val)\n",
    "#                 )\n",
    "\n",
    "#     train_predictions = {}\n",
    "\n",
    "#     for idx, row in X_fold_train.iterrows():\n",
    "#         if row['Level(dB) (x2)'] >= thresh:\n",
    "#             subject = row['Subject']\n",
    "#             x1_train = row['Freq(kHz) (x1)']\n",
    "#             x3_train = row['Amplitude (x3)']\n",
    "#         if subject in subjects_profiles_CVsplit_LR:\n",
    "#             pred = subjects_profiles_CVsplit_LR[subject].predict_y1(x1= x1_train, x3=x3_train)\n",
    "#             train_predictions[idx] = float(pred) \n",
    "\n",
    "#     successful_train_indices = list(train_predictions.keys())\n",
    "\n",
    "#     if successful_train_indices:\n",
    "#         y_train_true = y_fold_train.loc[successful_train_indices]\n",
    "#         y_train_pred = pd.Series([train_predictions[idx] for idx in successful_train_indices], \n",
    "#                                 index=successful_train_indices)\n",
    "\n",
    "#     val_predictions = {}\n",
    "\n",
    "#     for idx, row in X_fold_val.iterrows():\n",
    "#         if row['Level(dB) (x2)'] >= thresh:\n",
    "#             subject = row['Subject']\n",
    "#             x1_val = row['Freq(kHz) (x1)']\n",
    "#             x3_val = row['Amplitude (x3)']\n",
    "#         if subject in subjects_profiles_CVsplit_LR:\n",
    "#             pred = subjects_profiles_CVsplit_LR[subject].predict_y1(x1= x1_val, x3=x3_val)\n",
    "#             val_predictions[idx] = float(pred)\n",
    "\n",
    "#     successful_val_indices = list(val_predictions.keys())\n",
    "\n",
    "#     if successful_val_indices:\n",
    "#         y_val_true = y_fold_val.loc[successful_val_indices]\n",
    "#         y_val_pred = pd.Series([val_predictions[idx] for idx in successful_val_indices], \n",
    "#                             index=successful_val_indices)\n",
    "\n",
    "#         # Print basic information\n",
    "#         print(f\"Train: {len(successful_train_indices)} successful predictions out of {len(X_fold_train)}\")\n",
    "#         print(f\"Validation: {len(successful_val_indices)} successful predictions out of {len(X_fold_val)}\")\n",
    "\n",
    "#         # Calculate train RMSE\n",
    "#         true_name = f'{fold_idx} - train - true'\n",
    "#         pred_name = f'{fold_idx} - train - pred'\n",
    "#         all_train_data[true_name] = y_train_true\n",
    "#         all_train_data[pred_name] = y_train_pred\n",
    "\n",
    "#         fold_train_rmse = np.sqrt(np.mean((y_train_true - y_train_pred)**2))\n",
    "#         train_rmse_scores.append(fold_train_rmse)\n",
    "\n",
    "#         # Calculate val RMSE\n",
    "#         true_name = f'{fold_idx} - val - true'\n",
    "#         pred_name = f'{fold_idx} - val - pred'\n",
    "#         all_val_data[true_name] = y_val_true\n",
    "#         all_val_data[pred_name] = y_val_pred\n",
    "\n",
    "#         fold_val_rmse = np.sqrt(np.mean((y_val_true - y_val_pred)**2))\n",
    "#         val_rmse_scores.append(fold_val_rmse)\n",
    "\n",
    "#         # Calculate group-specific RMSE\n",
    "#         for group in groups:\n",
    "#             # Get the subject groups\n",
    "#             # train_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[successful_train_indices]\n",
    "#             # val_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed.loc[successful_val_indices]\n",
    "#             train_subjects_df = X_train10.loc[successful_train_indices]\n",
    "#             val_subjects_df = X_train10.loc[successful_val_indices]\n",
    "\n",
    "#             # Filter for successful predictions for this group\n",
    "#             group_train_mask = train_subjects_df['Group'] == group\n",
    "#             if group_train_mask.any():\n",
    "#                 group_indices = group_train_mask.index[group_train_mask]\n",
    "#                 group_rmse = np.sqrt(np.mean((y_train_true.loc[group_indices] - y_train_pred.loc[group_indices])**2))\n",
    "#                 group_train_rmse[group].append(group_rmse)\n",
    "\n",
    "#             group_val_mask = val_subjects_df['Group'] == group\n",
    "#             if group_val_mask.any():\n",
    "#                 group_indices = group_val_mask.index[group_val_mask]\n",
    "#                 group_rmse = np.sqrt(np.mean((y_val_true.loc[group_indices] - y_val_pred.loc[group_indices])**2))\n",
    "#                 group_val_rmse[group].append(group_rmse)\n",
    "\n",
    "#         print(f\"Fold {fold_idx+1}: Train RMSE = {fold_train_rmse:.4f}, Validation RMSE = {fold_val_rmse:.4f}\")\n",
    "#     else:\n",
    "#         print(f\"Fold {fold_idx+1}: No successful training predictions\")\n",
    "\n",
    "# # Calculate average RMSE across all folds\n",
    "# if train_rmse_scores:\n",
    "#     avg_train_rmse = np.mean(train_rmse_scores)\n",
    "#     std_train_rmse = np.std(train_rmse_scores)\n",
    "#     print(f\"\\nAverage Train RMSE across folds: {avg_train_rmse:.4f} ± {std_train_rmse:.4f}\")\n",
    "\n",
    "# if val_rmse_scores:\n",
    "#     avg_val_rmse = np.mean(val_rmse_scores)\n",
    "#     std_val_rmse = np.std(val_rmse_scores)\n",
    "#     print(f\"Average Validation RMSE across folds: {avg_val_rmse:.4f} ± {std_val_rmse:.4f}\")\n",
    "\n",
    "# # Visualization of group-specific RMSE\n",
    "# if any(group_train_rmse[group] for group in groups) and any(group_val_rmse[group] for group in groups):\n",
    "#     # Prepare data for plotting\n",
    "#     plot_groups = []\n",
    "#     plot_train_rmse = []\n",
    "#     plot_val_rmse = []\n",
    "#     plot_train_std = []\n",
    "#     plot_val_std = []\n",
    "    \n",
    "#     for group in groups:\n",
    "#         if group_train_rmse[group] and group_val_rmse[group]:\n",
    "#             plot_groups.append(group)\n",
    "#             plot_train_rmse.append(np.mean(group_train_rmse[group]))\n",
    "#             plot_val_rmse.append(np.mean(group_val_rmse[group]))\n",
    "#             plot_train_std.append(np.std(group_train_rmse[group]))\n",
    "#             plot_val_std.append(np.std(group_val_rmse[group]))\n",
    "    \n",
    "#     # Create plot if we have data\n",
    "#     if plot_groups:\n",
    "#         plt.figure(figsize=(14, 7))\n",
    "#         x = np.arange(len(plot_groups))\n",
    "#         width = 0.35\n",
    "        \n",
    "#         plt.bar(x - width/2, plot_train_rmse, width, yerr=plot_train_std, \n",
    "#                 label='Train RMSE', color='blue', alpha=0.7, capsize=5)\n",
    "#         plt.bar(x + width/2, plot_val_rmse, width, yerr=plot_val_std,\n",
    "#                 label='Validation RMSE', color='red', alpha=0.7, capsize=5)\n",
    "        \n",
    "#         plt.xlabel('Group')\n",
    "#         plt.ylabel('RMSE')\n",
    "#         plt.ylim((0,5))\n",
    "#         plt.title('Bayesian Model Performance by Group (Mean ± Std)')\n",
    "#         plt.xticks(x, plot_groups, rotation=90)\n",
    "#         plt.legend()\n",
    "#         plt.grid(True, axis='y', alpha=0.3)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "# # Create lists to store the data\n",
    "# groups_list = []\n",
    "# train_mean_list = []\n",
    "# train_std_list = []\n",
    "# val_mean_list = []\n",
    "# val_std_list = []\n",
    "\n",
    "# # Extract values for each group\n",
    "# for group in sorted(group_train_rmse.keys()):\n",
    "#     train_values = group_train_rmse[group]\n",
    "#     val_values = group_val_rmse[group]\n",
    "    \n",
    "#     groups_list.append(group)\n",
    "    \n",
    "#     if train_values:\n",
    "#         train_mean_list.append(np.mean(train_values))\n",
    "#         train_std_list.append(np.std(train_values))\n",
    "#     else:\n",
    "#         train_mean_list.append(np.nan)\n",
    "#         train_std_list.append(np.nan)\n",
    "        \n",
    "#     if val_values:\n",
    "#         val_mean_list.append(np.mean(val_values))\n",
    "#         val_std_list.append(np.std(val_values))\n",
    "#     else:\n",
    "#         val_mean_list.append(np.nan)\n",
    "#         val_std_list.append(np.nan)\n",
    "\n",
    "# group_metrics_df = pd.DataFrame({\n",
    "#     'Group': groups_list,\n",
    "#     'Train_RMSE_Mean': train_mean_list,\n",
    "#     'Val_RMSE_Mean': val_mean_list,\n",
    "#     'Train_RMSE_Std': train_std_list,\n",
    "#     'Val_RMSE_Std': val_std_list\n",
    "# })\n",
    "\n",
    "# group_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12187"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV - Predicting for total ratio across viewing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [8.0, 22.6], 1: [11.3, 32.0], 2: [16.0, 45.2]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Freq(kHz) (x1)</th>\n",
       "      <th>Level(dB) (x2)</th>\n",
       "      <th>Amplitude (x3)</th>\n",
       "      <th>Strain (x5)</th>\n",
       "      <th>Group</th>\n",
       "      <th>Synapse to IHC Ratio per Freq (y2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WPZ100</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.077363</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>8wks ctrl</td>\n",
       "      <td>17.487437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WPZ100</td>\n",
       "      <td>22.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.085015</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>8wks ctrl</td>\n",
       "      <td>17.378641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WPZ100</td>\n",
       "      <td>32.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.173144</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>8wks ctrl</td>\n",
       "      <td>18.282828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WPZ101</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.128507</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>8wks ctrl</td>\n",
       "      <td>16.170213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WPZ102</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.091656</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>94dB 2w post</td>\n",
       "      <td>16.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>WPZ98</td>\n",
       "      <td>22.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.056974</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>90dB 2w post</td>\n",
       "      <td>18.535354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>WPZ98</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>90dB 2w post</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>WPZ99</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>8wks ctrl</td>\n",
       "      <td>17.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>WPZ99</td>\n",
       "      <td>22.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.079862</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>8wks ctrl</td>\n",
       "      <td>17.524272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>WPZ99</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.073263</td>\n",
       "      <td>C57B6</td>\n",
       "      <td>8wks ctrl</td>\n",
       "      <td>18.835979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject  Freq(kHz) (x1)  Level(dB) (x2)  Amplitude (x3) Strain (x5)  \\\n",
       "0    WPZ100            16.0            10.0        0.077363       C57B6   \n",
       "1    WPZ100            22.6            15.0        0.085015       C57B6   \n",
       "2    WPZ100            32.0            15.0        0.173144       C57B6   \n",
       "3    WPZ101            32.0            10.0        0.128507       C57B6   \n",
       "4    WPZ102            16.0            10.0        0.091656       C57B6   \n",
       "..      ...             ...             ...             ...         ...   \n",
       "306   WPZ98            22.6            10.0        0.056974       C57B6   \n",
       "307   WPZ98            32.0            10.0        0.017715       C57B6   \n",
       "308   WPZ99            16.0            10.0        0.012406       C57B6   \n",
       "309   WPZ99            22.6            10.0        0.079862       C57B6   \n",
       "310   WPZ99            32.0            10.0        0.073263       C57B6   \n",
       "\n",
       "            Group  Synapse to IHC Ratio per Freq (y2)  \n",
       "0       8wks ctrl                           17.487437  \n",
       "1       8wks ctrl                           17.378641  \n",
       "2       8wks ctrl                           18.282828  \n",
       "3       8wks ctrl                           16.170213  \n",
       "4    94dB 2w post                           16.333333  \n",
       "..            ...                                 ...  \n",
       "306  90dB 2w post                           18.535354  \n",
       "307  90dB 2w post                           17.500000  \n",
       "308     8wks ctrl                           17.947368  \n",
       "309     8wks ctrl                           17.524272  \n",
       "310     8wks ctrl                           18.835979  \n",
       "\n",
       "[311 rows x 7 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data based on ABR recording frequencies and compare!!!!!!!\n",
    "# Because it doesn't make sense to have TT by subject for this given one model is specific to a subject...\n",
    "np.random.seed(12321)\n",
    "groups = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'])\n",
    "sorted_freqs = sorted(np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)']))\n",
    "\n",
    "freq_groups = {}\n",
    "for i, freq in enumerate(sorted_freqs):\n",
    "    # Splits up the frequencies into n groups\n",
    "    group_idx = i % 3\n",
    "    if group_idx not in freq_groups:\n",
    "        freq_groups[group_idx] = []\n",
    "    freq_groups[group_idx].append(freq)\n",
    "\n",
    "print(freq_groups)\n",
    "\n",
    "records_by_group = {}\n",
    "for group in final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'].unique():\n",
    "    records_in_group = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'] == group]\n",
    "    records_by_group[group] = records_in_group\n",
    "\n",
    "train_freqs = []\n",
    "test_freqs = []\n",
    "\n",
    "for group_idx, freqs in freq_groups.items():\n",
    "    # Shuffle frequencies within this group (kinda like RF. randomly splits on which freqs to use in train/test)\n",
    "    np.random.shuffle(freqs)\n",
    "    \n",
    "    n_test = max(1, round(len(freqs) * 0.2))  # Ensuring at least 2 frequencies are used for testing\n",
    "\n",
    "    # Add to overall train/test sets\n",
    "    test_freqs.extend(freqs[:n_test])\n",
    "    train_freqs.extend(freqs[n_test:])\n",
    "\n",
    "train_indices = set()\n",
    "test_indices = set()\n",
    "\n",
    "for subject in np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject']):\n",
    "    subject_recs = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == subject]\n",
    "\n",
    "    train_subject_indices = subject_recs[subject_recs['Freq(kHz) (x1)'].isin(train_freqs)].index.tolist()\n",
    "    test_subject_indices = subject_recs[subject_recs['Freq(kHz) (x1)'].isin(test_freqs)].index.tolist()\n",
    "\n",
    "    # Ensure the subject has data in both splits\n",
    "    if len(train_subject_indices) == 0 or len(test_subject_indices) == 0:\n",
    "        continue  # Skip this subject to avoid leakage problems\n",
    "\n",
    "    test_indices.update(test_subject_indices)\n",
    "    train_indices.update(train_subject_indices)\n",
    "\n",
    "y1_inputs = ['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'Strain (x5)', 'Group', 'Synapses to IHC (y1)']\n",
    "y2_inputs = ['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'Strain (x5)', 'Group', 'Synapse to IHC Ratio per Freq (y2)']\n",
    "\n",
    "predict_per_vx_train = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(train_indices), y1_inputs].reset_index(drop=True)\n",
    "predict_per_freq_train = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[list(train_indices), y2_inputs].reset_index(drop=True)\n",
    "\n",
    "predict_per_freq_train_slim = predict_per_freq_train.groupby(['Subject', 'Freq(kHz) (x1)']).first().reset_index()\n",
    "predict_per_freq_train_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Fold 1\n",
      "[]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 42\u001b[0m\n\u001b[1;32m     37\u001b[0m             y1_val \u001b[38;5;241m=\u001b[39m y_fold_train\u001b[38;5;241m.\u001b[39mloc[y1_val_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSynapse to IHC Ratio per Freq (y2)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;28mprint\u001b[39m(y1_val)\n\u001b[1;32m     39\u001b[0m             subjects_profiles_CVsplit_LR[subject]\u001b[38;5;241m.\u001b[39madd_observation_y1(\n\u001b[1;32m     40\u001b[0m                 x1\u001b[38;5;241m=\u001b[39mfreq,\n\u001b[1;32m     41\u001b[0m                 x3\u001b[38;5;241m=\u001b[39mamp,\n\u001b[0;32m---> 42\u001b[0m                 y1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my1_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m             )\n\u001b[1;32m     45\u001b[0m train_predictions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m X_fold_train\u001b[38;5;241m.\u001b[39miterrows():\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# fixed to have subjects stay within one fold\n",
    "\n",
    "thresh = 0\n",
    "train_rmse_scores = []\n",
    "val_rmse_scores = []\n",
    "subjects_profiles_CVsplit_LR = {}\n",
    "all_train_data = {}\n",
    "all_val_data = {}\n",
    "\n",
    "groups = [str(group) for group in np.unique(predict_per_freq_train['Group'])]\n",
    "group_train_rmse = {group: [] for group in groups}\n",
    "group_val_rmse = {group: [] for group in groups}\n",
    "\n",
    "subject_group_df = predict_per_freq_train.groupby('Subject').first().reset_index()\n",
    "subject_labels = subject_group_df['Group'].values\n",
    "subject_ids = subject_group_df['Subject'].values\n",
    "\n",
    "group_k_fold = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X_cols = ['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'Strain (x5)', 'Group']\n",
    "y2_cols = ['Synapse to IHC Ratio per Freq (y2)']\n",
    "\n",
    "X_train = predict_per_freq_train[X_cols]\n",
    "y2_train = predict_per_freq_train[y2_cols]\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(group_k_fold.split(X_train, groups=X_train['Subject'])):\n",
    "    \n",
    "    X_fold_train, X_fold_val = predict_per_freq_train.loc[train_idx, X_cols], predict_per_freq_train.loc[val_idx, X_cols]\n",
    "    y_fold_train, y_fold_val = predict_per_freq_train.loc[train_idx, y2_cols], predict_per_freq_train.loc[val_idx, y2_cols]\n",
    "    print(f\"\\nProcessing Fold {fold_idx+1}\")\n",
    "\n",
    "    for subject in np.unique(X_fold_train['Subject']):\n",
    "        for freq in np.unique(X_fold_train['Freq(kHz) (x1)']):\n",
    "            subjects_profiles_CVsplit_LR[subject] = BayesianProfile_LRcompare()\n",
    "            subject_freq_data = X_fold_train[(X_fold_train['Subject'] == subject) & (X_fold_train['Freq(kHz) (x1)'] == freq) & (X_fold_train['Level(dB) (x2)'] >= thresh)]\n",
    "\n",
    "            for amp in np.unique(X_fold_train['Amplitude (x3)']):\n",
    "                y1_val_index = subject_freq_data[subject_freq_data['Amplitude (x3)'] == amp].index\n",
    "                y1_val = y_fold_train.loc[y1_val_index, 'Synapse to IHC Ratio per Freq (y2)'].values\n",
    "                print(y1_val)\n",
    "                subjects_profiles_CVsplit_LR[subject].add_observation_y1(\n",
    "                    x1=freq,\n",
    "                    x3=amp,\n",
    "                    y1=float(y1_val)\n",
    "                )\n",
    "\n",
    "    train_predictions = {}\n",
    "\n",
    "    for idx, row in X_fold_train.iterrows():\n",
    "        if row['Level(dB) (x2)'] >= thresh:\n",
    "            subject = row['Subject']\n",
    "            x1_train = row['Freq(kHz) (x1)']\n",
    "            x3_train = row['Amplitude (x3)']\n",
    "        if subject in subjects_profiles_CVsplit_LR:\n",
    "            pred = subjects_profiles_CVsplit_LR[subject].predict_y1(x1= x1_train, x3=x3_train)\n",
    "            train_predictions[idx] = float(pred) \n",
    "\n",
    "    successful_train_indices = list(train_predictions.keys())\n",
    "\n",
    "    if successful_train_indices:\n",
    "        y_train_true = y_fold_train.loc[successful_train_indices]\n",
    "        y_train_pred = pd.Series([train_predictions[idx] for idx in successful_train_indices], \n",
    "                                index=successful_train_indices)\n",
    "\n",
    "    val_predictions = {}\n",
    "\n",
    "    for idx, row in X_fold_val.iterrows():\n",
    "        if row['Level(dB) (x2)'] >= thresh:\n",
    "            subject = row['Subject']\n",
    "            x1_val = row['Freq(kHz) (x1)']\n",
    "            x3_val = row['Amplitude (x3)']\n",
    "        if subject in subjects_profiles_CVsplit_LR:\n",
    "            pred = subjects_profiles_CVsplit_LR[subject].predict_y1(x1= x1_val, x3=x3_val)\n",
    "            val_predictions[idx] = float(pred)\n",
    "\n",
    "    successful_val_indices = list(val_predictions.keys())\n",
    "\n",
    "    if successful_val_indices:\n",
    "        y_val_true = y_fold_val.loc[successful_val_indices]\n",
    "        y_val_pred = pd.Series([val_predictions[idx] for idx in successful_val_indices], \n",
    "                            index=successful_val_indices)\n",
    "\n",
    "        # Print basic information\n",
    "        print(f\"Train: {len(successful_train_indices)} successful predictions out of {len(X_fold_train)}\")\n",
    "        print(f\"Validation: {len(successful_val_indices)} successful predictions out of {len(X_fold_val)}\")\n",
    "\n",
    "        # Calculate train RMSE\n",
    "        true_name = f'{fold_idx} - train - true'\n",
    "        pred_name = f'{fold_idx} - train - pred'\n",
    "        all_train_data[true_name] = y_train_true\n",
    "        all_train_data[pred_name] = y_train_pred\n",
    "\n",
    "        fold_train_rmse = np.sqrt(np.mean((y_train_true - y_train_pred)**2))\n",
    "        train_rmse_scores.append(fold_train_rmse)\n",
    "\n",
    "        # Calculate val RMSE\n",
    "        true_name = f'{fold_idx} - val - true'\n",
    "        pred_name = f'{fold_idx} - val - pred'\n",
    "        all_val_data[true_name] = y_val_true\n",
    "        all_val_data[pred_name] = y_val_pred\n",
    "\n",
    "        fold_val_rmse = np.sqrt(np.mean((y_val_true - y_val_pred)**2))\n",
    "        val_rmse_scores.append(fold_val_rmse)\n",
    "\n",
    "        # Calculate group-specific RMSE\n",
    "        for group in groups:\n",
    "            # Get the subject groups\n",
    "            # train_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[successful_train_indices]\n",
    "            # val_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed.loc[successful_val_indices]\n",
    "            train_subjects_df = predict_per_freq_train_slim.loc[successful_train_indices]\n",
    "            val_subjects_df = predict_per_freq_train_slim.loc[successful_val_indices]\n",
    "\n",
    "            # Filter for successful predictions for this group\n",
    "            group_train_mask = train_subjects_df['Group'] == group\n",
    "            if group_train_mask.any():\n",
    "                group_indices = group_train_mask.index[group_train_mask]\n",
    "                group_rmse = np.sqrt(np.mean((y_train_true.loc[group_indices] - y_train_pred.loc[group_indices])**2))\n",
    "                group_train_rmse[group].append(group_rmse)\n",
    "\n",
    "            group_val_mask = val_subjects_df['Group'] == group\n",
    "            if group_val_mask.any():\n",
    "                group_indices = group_val_mask.index[group_val_mask]\n",
    "                group_rmse = np.sqrt(np.mean((y_val_true.loc[group_indices] - y_val_pred.loc[group_indices])**2))\n",
    "                group_val_rmse[group].append(group_rmse)\n",
    "\n",
    "        print(f\"Fold {fold_idx+1}: Train RMSE = {fold_train_rmse:.4f}, Validation RMSE = {fold_val_rmse:.4f}\")\n",
    "    else:\n",
    "        print(f\"Fold {fold_idx+1}: No successful training predictions\")\n",
    "\n",
    "# Calculate average RMSE across all folds\n",
    "if train_rmse_scores:\n",
    "    avg_train_rmse = np.mean(train_rmse_scores)\n",
    "    std_train_rmse = np.std(train_rmse_scores)\n",
    "    print(f\"\\nAverage Train RMSE across folds: {avg_train_rmse:.4f} ± {std_train_rmse:.4f}\")\n",
    "\n",
    "if val_rmse_scores:\n",
    "    avg_val_rmse = np.mean(val_rmse_scores)\n",
    "    std_val_rmse = np.std(val_rmse_scores)\n",
    "    print(f\"Average Validation RMSE across folds: {avg_val_rmse:.4f} ± {std_val_rmse:.4f}\")\n",
    "\n",
    "# Visualization of group-specific RMSE\n",
    "if any(group_train_rmse[group] for group in groups) and any(group_val_rmse[group] for group in groups):\n",
    "    # Prepare data for plotting\n",
    "    plot_groups = []\n",
    "    plot_train_rmse = []\n",
    "    plot_val_rmse = []\n",
    "    plot_train_std = []\n",
    "    plot_val_std = []\n",
    "    \n",
    "    for group in groups:\n",
    "        if group_train_rmse[group] and group_val_rmse[group]:\n",
    "            plot_groups.append(group)\n",
    "            plot_train_rmse.append(np.mean(group_train_rmse[group]))\n",
    "            plot_val_rmse.append(np.mean(group_val_rmse[group]))\n",
    "            plot_train_std.append(np.std(group_train_rmse[group]))\n",
    "            plot_val_std.append(np.std(group_val_rmse[group]))\n",
    "    \n",
    "    # Create plot if we have data\n",
    "    if plot_groups:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        x = np.arange(len(plot_groups))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, plot_train_rmse, width, yerr=plot_train_std, \n",
    "                label='Train RMSE', color='blue', alpha=0.7, capsize=5)\n",
    "        plt.bar(x + width/2, plot_val_rmse, width, yerr=plot_val_std,\n",
    "                label='Validation RMSE', color='red', alpha=0.7, capsize=5)\n",
    "        \n",
    "        plt.xlabel('Group')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.ylim((0,5))\n",
    "        plt.title('Bayesian Model Performance by Group (Mean ± Std)')\n",
    "        plt.xticks(x, plot_groups, rotation=90)\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create lists to store the data\n",
    "groups_list = []\n",
    "train_mean_list = []\n",
    "train_std_list = []\n",
    "val_mean_list = []\n",
    "val_std_list = []\n",
    "\n",
    "# Extract values for each group\n",
    "for group in sorted(group_train_rmse.keys()):\n",
    "    train_values = group_train_rmse[group]\n",
    "    val_values = group_val_rmse[group]\n",
    "    \n",
    "    groups_list.append(group)\n",
    "    \n",
    "    if train_values:\n",
    "        train_mean_list.append(np.mean(train_values))\n",
    "        train_std_list.append(np.std(train_values))\n",
    "    else:\n",
    "        train_mean_list.append(np.nan)\n",
    "        train_std_list.append(np.nan)\n",
    "        \n",
    "    if val_values:\n",
    "        val_mean_list.append(np.mean(val_values))\n",
    "        val_std_list.append(np.std(val_values))\n",
    "    else:\n",
    "        val_mean_list.append(np.nan)\n",
    "        val_std_list.append(np.nan)\n",
    "\n",
    "group_metrics_df = pd.DataFrame({\n",
    "    'Group': groups_list,\n",
    "    'Train_RMSE_Mean': train_mean_list,\n",
    "    'Val_RMSE_Mean': val_mean_list,\n",
    "    'Train_RMSE_Std': train_std_list,\n",
    "    'Val_RMSE_Std': val_std_list\n",
    "})\n",
    "\n",
    "group_metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_manorimagepred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
