{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fixed to have subjects stay within one fold\n",
    "\n",
    "# thresh = 0\n",
    "# k_folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# train_rmse_scores = []\n",
    "# val_rmse_scores = []\n",
    "# subjects_profiles_CVsplit_LR = {}\n",
    "# all_train_data = {}\n",
    "# all_val_data = {}\n",
    "\n",
    "# # Groups setup\n",
    "# groups = [str(group) for group in np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed['Group'])]\n",
    "# group_train_rmse = {group: [] for group in groups}\n",
    "# group_val_rmse = {group: [] for group in groups}\n",
    "\n",
    "# group_k_fold = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# for fold_idx, (train_idx, val_idx) in enumerate(group_k_fold.split(X_train10, groups=X_train10['Subject'])):\n",
    "#     X_fold_train, X_fold_val = X_train10.iloc[train_idx], X_train10.iloc[val_idx]\n",
    "#     y_fold_train, y_fold_val = y1_train10.iloc[train_idx], y1_train10.iloc[val_idx]\n",
    "    \n",
    "#     print(f\"\\nProcessing Fold {fold_idx+1}\")\n",
    "    \n",
    "#     # First, build the Bayesian profiles for each subject\n",
    "#     for subject in np.unique(X_fold_train['Subject']):\n",
    "#         profile_key = f\"{subject}\"\n",
    "#         subjects_profiles_CVsplit_LR[profile_key] = BayesianProfile_LRcompare()\n",
    "        \n",
    "#         subject_data = X_fold_train[X_fold_train['Subject'] == subject]\n",
    "        \n",
    "#         # Group by the combination of features that define unique conditions\n",
    "#         grouped = subject_data.groupby([\n",
    "#             'Freq(kHz) (x1)', 'Amplitude (x3)', 'Strain (x5)',   \n",
    "#             'Group'\n",
    "#         ])\n",
    "        \n",
    "#         # Process each group (condition)\n",
    "#         for group_key, group_data in grouped:\n",
    "#             if group_data['Level(dB) (x2)'].min() < thresh:\n",
    "#                 continue\n",
    "                \n",
    "#             # Get all y1 values for this condition\n",
    "#             indices = group_data.index\n",
    "#             y_values = y_fold_train.loc[indices]\n",
    "            \n",
    "#             # Extract feature values (same for all rows in group)\n",
    "#             x1_val = group_data['Freq(kHz) (x1)'].iloc[0]\n",
    "#             x2_val = group_data['Level(dB) (x2)'].iloc[0]\n",
    "#             x3_val = group_data['Amplitude (x3)'].iloc[0]\n",
    "            \n",
    "#             # Add each observation with same features but different y values\n",
    "#             for idx, y1_val in zip(indices, y_values):\n",
    "#                 subjects_profiles_CVsplit_LR[profile_key].add_observation_y1(\n",
    "#                     x1=x1_val, x2=x2_val, x3=x3_val,\n",
    "#                     y1=float(y1_val),\n",
    "#                 )\n",
    "    \n",
    "#     # Now calculate RMSE for this fold (both training and validation)\n",
    "#     # Store predictions in a dictionary keyed by index\n",
    "#         train_predictions = {}\n",
    "        \n",
    "#         for idx, row in X_fold_train.iterrows():\n",
    "#             if row['Level(dB) (x2)'] >= thresh:\n",
    "#                 subject = row['Subject']\n",
    "#                 profile_key = f\"{subject}\"\n",
    "#                 x1_train = row['Freq(kHz) (x1)']\n",
    "#                 x2_train = row['Level(dB) (x2)']\n",
    "#                 x3_train = row['Amplitude (x3)']\n",
    "#             if profile_key in subjects_profiles_CVsplit_LR:\n",
    "#                 pred = subjects_profiles_CVsplit_LR[profile_key].predict_y1(x1= x1_train, x2=x2_train, x3=x3_train)\n",
    "#                 train_predictions[idx] = float(pred)  # Ensure it's a scalar\n",
    "\n",
    "#         # Create pandas Series with predictions matched to proper indices\n",
    "#         successful_train_indices = list(train_predictions.keys())\n",
    "        \n",
    "#         if successful_train_indices:\n",
    "#             y_train_true = y_fold_train.loc[successful_train_indices]\n",
    "#             y_train_pred = pd.Series([train_predictions[idx] for idx in successful_train_indices], \n",
    "#                                     index=successful_train_indices)\n",
    "            \n",
    "#             # Do the same for validation\n",
    "#         val_predictions = {}\n",
    "        \n",
    "#         for idx, row in X_fold_val.iterrows():\n",
    "#             if row['Level(dB) (x2)'] >= thresh:\n",
    "#                 subject = row['Subject']\n",
    "#                 profile_key = f\"{subject}\"\n",
    "#                 x1_val = row['Freq(kHz) (x1)']\n",
    "#                 x2_val = row['Level(dB) (x2)']\n",
    "#                 x3_val = row['Amplitude (x3)']\n",
    "#             if profile_key in subjects_profiles_CVsplit_LR:\n",
    "#                 pred = subjects_profiles_CVsplit_LR[profile_key].predict_y1(x1= x1_val, x2=x2_val, x3=x3_val)\n",
    "#                 val_predictions[idx] = float(pred)  # Ensure it's a scalar\n",
    "\n",
    "        \n",
    "#         # Create properly indexed validation predictions\n",
    "#         successful_val_indices = list(val_predictions.keys())\n",
    "        \n",
    "#         if successful_val_indices:\n",
    "#             y_val_true = y_fold_val.loc[successful_val_indices]\n",
    "#             y_val_pred = pd.Series([val_predictions[idx] for idx in successful_val_indices], \n",
    "#                                 index=successful_val_indices)\n",
    "            \n",
    "#             # Print basic information\n",
    "#             print(f\"Train: {len(successful_train_indices)} successful predictions out of {len(X_fold_train)}\")\n",
    "#             print(f\"Validation: {len(successful_val_indices)} successful predictions out of {len(X_fold_val)}\")\n",
    "            \n",
    "#             # Calculate RMSE\n",
    "#             true_name = f'{fold_idx} - train - true'\n",
    "#             pred_name = f'{fold_idx} - train - pred'\n",
    "#             all_train_data[true_name] = y_train_true\n",
    "#             all_train_data[pred_name] = y_train_pred\n",
    "\n",
    "#             fold_train_rmse = np.sqrt(np.mean((y_train_true - y_train_pred)**2))\n",
    "#             train_rmse_scores.append(fold_train_rmse)\n",
    "            \n",
    "#             true_name = f'{fold_idx} - val - true'\n",
    "#             pred_name = f'{fold_idx} - val - pred'\n",
    "#             all_val_data[true_name] = y_val_true\n",
    "#             all_val_data[pred_name] = y_val_pred\n",
    "\n",
    "#             fold_val_rmse = np.sqrt(np.mean((y_val_true - y_val_pred)**2))\n",
    "#             val_rmse_scores.append(fold_val_rmse)\n",
    "            \n",
    "#             print(f\"Fold {fold_idx+1}: Train RMSE = {fold_train_rmse:.4f}, Validation RMSE = {fold_val_rmse:.4f}\")\n",
    "                \n",
    "#                 # Calculate group-specific RMSE\n",
    "#     for group in groups:\n",
    "#         # Get the subject groups\n",
    "#         train_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed.loc[successful_train_indices]\n",
    "#         val_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed.loc[successful_val_indices]\n",
    "        \n",
    "#         # Filter for successful predictions for this group\n",
    "#         group_train_mask = train_subjects_df['Group'] == group\n",
    "#         if group_train_mask.any():\n",
    "#             group_indices = group_train_mask.index[group_train_mask]\n",
    "#             group_rmse = np.sqrt(np.mean((y_train_true.loc[group_indices] - y_train_pred.loc[group_indices])**2))\n",
    "#             group_train_rmse[group].append(group_rmse)\n",
    "        \n",
    "#         group_val_mask = val_subjects_df['Group'] == group\n",
    "#         if group_val_mask.any():\n",
    "#             group_indices = group_val_mask.index[group_val_mask]\n",
    "#             group_rmse = np.sqrt(np.mean((y_val_true.loc[group_indices] - y_val_pred.loc[group_indices])**2))\n",
    "#             group_val_rmse[group].append(group_rmse)\n",
    "#     else:\n",
    "#         print(f\"Fold {fold_idx+1}: No successful training predictions\")\n",
    "\n",
    "# # Calculate average RMSE across all folds\n",
    "# if train_rmse_scores:\n",
    "#     avg_train_rmse = np.mean(train_rmse_scores)\n",
    "#     std_train_rmse = np.std(train_rmse_scores)\n",
    "#     print(f\"\\nAverage Train RMSE across folds: {avg_train_rmse:.4f} ± {std_train_rmse:.4f}\")\n",
    "\n",
    "# if val_rmse_scores:\n",
    "#     avg_val_rmse = np.mean(val_rmse_scores)\n",
    "#     std_val_rmse = np.std(val_rmse_scores)\n",
    "#     print(f\"Average Validation RMSE across folds: {avg_val_rmse:.4f} ± {std_val_rmse:.4f}\")\n",
    "\n",
    "# # Calculate group-specific RMSE \n",
    "# print(\"\\nGroup-specific RMSE:\")\n",
    "# print(\"Group | Train RMSE (Mean ± Std) | Validation RMSE (Mean ± Std)\")\n",
    "# print(\"----- | ----------------------- | -----------------------------\")\n",
    "# for group in groups:\n",
    "#     train_values = group_train_rmse[group]\n",
    "#     val_values = group_val_rmse[group]\n",
    "    \n",
    "#     if train_values:\n",
    "#         train_mean = np.mean(train_values)\n",
    "#         train_std = np.std(train_values)\n",
    "#         train_str = f\"{train_mean:.6f} ± {train_std:.6f}\"\n",
    "#     else:\n",
    "#         train_str = \"N/A\"\n",
    "        \n",
    "#     if val_values:\n",
    "#         val_mean = np.mean(val_values)\n",
    "#         val_std = np.std(val_values)\n",
    "#         val_str = f\"{val_mean:.6f} ± {val_std:.6f}\"\n",
    "#     else:\n",
    "#         val_str = \"N/A\"\n",
    "        \n",
    "#     print(f\"{group:15s} | {train_str:25s} | {val_str:25s}\")\n",
    "\n",
    "# # Visualization of group-specific RMSE\n",
    "# if any(group_train_rmse[group] for group in groups) and any(group_val_rmse[group] for group in groups):\n",
    "#     # Prepare data for plotting\n",
    "#     plot_groups = []\n",
    "#     plot_train_rmse = []\n",
    "#     plot_val_rmse = []\n",
    "#     plot_train_std = []\n",
    "#     plot_val_std = []\n",
    "    \n",
    "#     for group in groups:\n",
    "#         if group_train_rmse[group] and group_val_rmse[group]:\n",
    "#             plot_groups.append(group)\n",
    "#             plot_train_rmse.append(np.mean(group_train_rmse[group]))\n",
    "#             plot_val_rmse.append(np.mean(group_val_rmse[group]))\n",
    "#             plot_train_std.append(np.std(group_train_rmse[group]))\n",
    "#             plot_val_std.append(np.std(group_val_rmse[group]))\n",
    "    \n",
    "#     # Create plot if we have data\n",
    "#     if plot_groups:\n",
    "#         plt.figure(figsize=(14, 7))\n",
    "#         x = np.arange(len(plot_groups))\n",
    "#         width = 0.35\n",
    "        \n",
    "#         plt.bar(x - width/2, plot_train_rmse, width, yerr=plot_train_std, \n",
    "#                 label='Train RMSE', color='blue', alpha=0.7, capsize=5)\n",
    "#         plt.bar(x + width/2, plot_val_rmse, width, yerr=plot_val_std,\n",
    "#                 label='Validation RMSE', color='red', alpha=0.7, capsize=5)\n",
    "        \n",
    "#         plt.xlabel('Group')\n",
    "#         plt.ylabel('RMSE')\n",
    "#         plt.ylim((0,5))\n",
    "#         plt.title('Bayesian Model Performance by Group (Mean ± Std)')\n",
    "#         plt.xticks(x, plot_groups, rotation=90)\n",
    "#         plt.legend()\n",
    "#         plt.grid(True, axis='y', alpha=0.3)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "# groups_list = []\n",
    "# train_mean_list = []\n",
    "# train_std_list = []\n",
    "# val_mean_list = []\n",
    "# val_std_list = []\n",
    "\n",
    "# # Extract values for each group\n",
    "# for group in sorted(group_train_rmse.keys()):\n",
    "#     train_values = group_train_rmse[group]\n",
    "#     val_values = group_val_rmse[group]\n",
    "    \n",
    "#     groups_list.append(group)\n",
    "    \n",
    "#     if train_values:\n",
    "#         train_mean_list.append(np.mean(train_values))\n",
    "#         train_std_list.append(np.std(train_values))\n",
    "#     else:\n",
    "#         train_mean_list.append(np.nan)\n",
    "#         train_std_list.append(np.nan)\n",
    "        \n",
    "#     if val_values:\n",
    "#         val_mean_list.append(np.mean(val_values))\n",
    "#         val_std_list.append(np.std(val_values))\n",
    "#     else:\n",
    "#         val_mean_list.append(np.nan)\n",
    "#         val_std_list.append(np.nan)\n",
    "\n",
    "\n",
    "# # Create lists to store the data\n",
    "# groups_list = []\n",
    "# train_mean_list = []\n",
    "# train_std_list = []\n",
    "# val_mean_list = []\n",
    "# val_std_list = []\n",
    "\n",
    "# # Extract values for each group\n",
    "# for group in sorted(group_train_rmse.keys()):\n",
    "#     train_values = group_train_rmse[group]\n",
    "#     val_values = group_val_rmse[group]\n",
    "    \n",
    "#     groups_list.append(group)\n",
    "    \n",
    "#     if train_values:\n",
    "#         train_mean_list.append(np.mean(train_values))\n",
    "#         train_std_list.append(np.std(train_values))\n",
    "#     else:\n",
    "#         train_mean_list.append(np.nan)\n",
    "#         train_std_list.append(np.nan)\n",
    "        \n",
    "#     if val_values:\n",
    "#         val_mean_list.append(np.mean(val_values))\n",
    "#         val_std_list.append(np.std(val_values))\n",
    "#     else:\n",
    "#         val_mean_list.append(np.nan)\n",
    "#         val_std_list.append(np.nan)\n",
    "\n",
    "# group_metrics_df = pd.DataFrame({\n",
    "#     'Group': groups_list,\n",
    "#     'Train_RMSE_Mean': train_mean_list,\n",
    "#     'Val_RMSE_Mean': val_mean_list,\n",
    "#     'Train_RMSE_Std': train_std_list,\n",
    "#     'Val_RMSE_Std': val_std_list\n",
    "# })\n",
    "\n",
    "# group_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.distributed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ABRA_35 import interpolate_and_smooth, CNN, plot_wave, calculate_and_plot_wave, plot_waves_single_frequency, arfread, get_str, calculate_hearing_threshold, all_thresholds, peak_finding\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import os\n",
    "import struct\n",
    "import datetime\n",
    "# from skfda import FDataGrid\n",
    "# from skfda.preprocessing.dim_reduction import FPCA\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import torch.nn as nn\n",
    "import splitfolders\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.signal import find_peaks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np4\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1 = 128\n",
    "filter2 = 32\n",
    "dropout1 = 0.5\n",
    "dropout2 = 0.3\n",
    "dropout_fc = 0.1\n",
    "\n",
    "# Model initialization\n",
    "peak_finding_model = CNN(filter1, filter2, dropout1, dropout2, dropout_fc)\n",
    "model_loader = torch.load('./models/waveI_cnn.pth')\n",
    "peak_finding_model.load_state_dict(model_loader)\n",
    "peak_finding_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finding(wave):\n",
    "    # Prepare waveform\n",
    "    waveform=interpolate_and_smooth(wave) # Added indexing per calculate and plot wave function\n",
    "    # waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0) archived ABRA\n",
    "    waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).unsqueeze(0) #newer ABRA\n",
    "    # print(waveform_torch)\n",
    "    # Get prediction from model\n",
    "    outputs = peak_finding_model(waveform_torch)\n",
    "    prediction = int(round(outputs.detach().numpy()[0][0], 0))\n",
    "    # prediction_test = int(round(outputs.detach().numpy()[0], 0))\n",
    "    # print(\"Model output:\", outputs, \"Prediction true start:\", prediction)\n",
    "\n",
    "    # Apply Gaussian smoothing\n",
    "    smoothed_waveform = gaussian_filter1d(waveform, sigma=1)\n",
    "\n",
    "    # Find peaks and troughs\n",
    "    n = 18\n",
    "    t = 14\n",
    "    # start_point = prediction - 9 archived ABRA\n",
    "    start_point = prediction - 6 #newer ABRA\n",
    "    smoothed_peaks, _ = find_peaks(smoothed_waveform[start_point:], distance=n)\n",
    "    smoothed_troughs, _ = find_peaks(-smoothed_waveform, distance=t)\n",
    "    sorted_indices = np.argsort(smoothed_waveform[smoothed_peaks+start_point])\n",
    "    highest_smoothed_peaks = np.sort(smoothed_peaks[sorted_indices[-5:]] + start_point)\n",
    "    relevant_troughs = np.array([])\n",
    "    for p in range(len(highest_smoothed_peaks)):\n",
    "        c = 0\n",
    "        for t in smoothed_troughs:\n",
    "            if t > highest_smoothed_peaks[p]:\n",
    "                if p != 4:\n",
    "                    try:\n",
    "                        if t < highest_smoothed_peaks[p+1]:\n",
    "                            relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                else:\n",
    "                    relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                    break\n",
    "    relevant_troughs = relevant_troughs.astype('i')\n",
    "    return highest_smoothed_peaks, relevant_troughs\n",
    "\n",
    "def extract_metadata(metadata_lines):\n",
    "    # Dictionary to store extracted metadata\n",
    "    metadata = {}\n",
    "    \n",
    "    for line in metadata_lines:\n",
    "        # Extract SW FREQ\n",
    "        freq_match = re.search(r'SW FREQ:\\s*(\\d+\\.?\\d*)', line)\n",
    "        if freq_match:\n",
    "            metadata['SW_FREQ'] = float(freq_match.group(1))\n",
    "        \n",
    "        # Extract LEVELS\n",
    "        levels_match = re.search(r':LEVELS:\\s*([^:]+)', line)\n",
    "        if levels_match:\n",
    "            # Split levels and convert to list of floats\n",
    "            metadata['LEVELS'] = [float(level) for level in levels_match.group(1).split(';') if level]\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def read_custom_tsv(file_path):\n",
    "    # Read the entire file\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split the content into metadata and data sections\n",
    "    metadata_lines = []\n",
    "    data_section = None\n",
    "    \n",
    "    # Find the ':DATA' marker\n",
    "    data_start = content.find(':DATA')\n",
    "    \n",
    "    if data_start != -1:\n",
    "        # Extract metadata (lines before ':DATA')\n",
    "        metadata_lines = content[:data_start].split('\\n')\n",
    "        \n",
    "        # Extract data section\n",
    "        data_section = content[data_start:].split(':DATA')[1].strip()\n",
    "    \n",
    "    # Extract specific metadata\n",
    "    metadata = extract_metadata(metadata_lines)\n",
    "    \n",
    "    # Read the data section directly\n",
    "    try:\n",
    "        # Use StringIO to create a file-like object from the data section\n",
    "        raw_data = pd.read_csv(\n",
    "            io.StringIO(data_section), \n",
    "            sep='\\s+',  # Use whitespace as separator\n",
    "            header=None\n",
    "        )\n",
    "        raw_data = raw_data.T\n",
    "        # Add metadata columns to the DataFrame\n",
    "        if 'SW_FREQ' in metadata:\n",
    "            raw_data['Freq(kHz)'] = metadata['SW_FREQ']\n",
    "            # raw_data['Freq(Hz)'] = raw_data['Freq(Hz)'].apply(lambda x: x*1000)\n",
    "        \n",
    "        if 'LEVELS' in metadata:\n",
    "            # Repeat levels to match the number of rows\n",
    "            levels_repeated = metadata['LEVELS'] * (len(raw_data) // len(metadata['LEVELS']) + 1)\n",
    "            raw_data['Level(dB)'] = levels_repeated[:len(raw_data)]\n",
    "        \n",
    "        filtered_data = raw_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "        filtered_data.columns = filtered_data.columns.map(str)\n",
    "\n",
    "        columns = ['Freq(kHz)'] + ['Level(dB)'] + [col for col in filtered_data.columns if col.isnumeric() == True]\n",
    "        filtered_data = filtered_data[columns]\n",
    "        return filtered_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        return None, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks_troughs_amp_final(df, freq, db, time_scale=10, multiply_y_factor=1.0, units='Microvolts'):\n",
    "    db_column = 'Level(dB)'\n",
    "    \n",
    "    khz = df[(df['Freq(kHz)'] == freq) & (df[db_column] == db)]\n",
    "    if not khz.empty:\n",
    "        index = khz.index.values[0]\n",
    "        final = df.loc[index, '0':].dropna()\n",
    "        final = pd.to_numeric(final, errors='coerce').dropna()\n",
    "\n",
    "        target = int(244 * (time_scale / 10))\n",
    "        \n",
    "        # Process the wave as in calculate_and_plot_wave\n",
    "        y_values = interpolate_and_smooth(final, target)\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        y_values *= multiply_y_factor\n",
    "        \n",
    "        # Handle units conversion if needed\n",
    "        if units == 'Nanovolts':\n",
    "            y_values /= 1000\n",
    "            \n",
    "        # Generate normalized version for peak finding\n",
    "        y_values_fpf = interpolate_and_smooth(y_values[:244])\n",
    "        \n",
    "        # Standardize and normalize for peak finding, exactly as in the original\n",
    "        flattened_data = y_values_fpf.flatten().reshape(-1, 1)\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(flattened_data)\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = min_max_scaler.fit_transform(standardized_data).reshape(y_values_fpf.shape)\n",
    "        y_values_fpf = interpolate_and_smooth(scaled_data[:244])\n",
    "        \n",
    "        # Find peaks using the normalized data\n",
    "        highest_peaks, relevant_troughs = peak_finding(y_values_fpf)\n",
    "        \n",
    "        # Calculate amplitude on the processed but non-normalized data\n",
    "        if highest_peaks.size > 0 and relevant_troughs.size > 0:\n",
    "            # Following the same approach as in the display_metrics_table function\n",
    "            first_peak_amplitude = y_values[highest_peaks[0]] - y_values[relevant_troughs[0]]\n",
    "            return highest_peaks, relevant_troughs, first_peak_amplitude\n",
    "    \n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del int\n",
    "time_scale = 18\n",
    "amp_per_freq = {'Subject': [], 'Freq(kHz) (x1)': [], 'Level(dB) (x2)': [], 'Amplitude (x3)':[]}\n",
    "start_path = '/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/WPZ Electrophysiology'\n",
    "for subject in os.listdir(start_path):\n",
    "    # print(\"Subject:\",subject)\n",
    "    for fq in os.listdir(os.path.join(start_path,subject)):\n",
    "        # print(fq)\n",
    "        if fq.startswith('ABR') and fq.endswith('.tsv'):\n",
    "            path = os.path.join(start_path,subject,fq)\n",
    "            data_df = read_custom_tsv(path)\n",
    "            # print(data_df)\n",
    "            freqs = data_df['Freq(kHz)'].unique().tolist()\n",
    "            levels = data_df['Level(dB)'].unique().tolist()\n",
    "            for freq in freqs:\n",
    "                for lvl in levels:\n",
    "                    # print(\"Frequency=\",freq, \"Level=\", lvl)\n",
    "                    _, _, amp = peaks_troughs_amp_final(df=data_df, freq=freq, db=lvl, time_scale=time_scale)\n",
    "                    # print(f'Amplitude: {amp}\\n')\n",
    "                    amp_per_freq['Subject'].append(subject)\n",
    "                    amp_per_freq['Freq(kHz) (x1)'].append(freq)\n",
    "                    amp_per_freq['Level(dB) (x2)'].append(lvl)\n",
    "                    amp_per_freq['Amplitude (x3)'].append(amp)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "amp_df_full = pd.DataFrame(data=amp_per_freq)\n",
    "\n",
    "raw_synapse_counts = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Ribbon and Synapse Counts.xlsx')\n",
    "raw_synapse_counts = raw_synapse_counts.mask(lambda x: x.isnull()).dropna()\n",
    "raw_synapse_counts['Synapses to IHC (y1)'] = raw_synapse_counts.iloc[:,6]\n",
    "raw_synapse_counts['vx (x4)'] = raw_synapse_counts['vx']\n",
    "raw_synapse_counts.drop(columns=['vx'], inplace=True)\n",
    "raw_synapse_counts.rename(columns={'Freq':'Freq(kHz) (x1)'}, inplace=True)\n",
    "# raw_synapse_counts['Freq(Hz) (x1)'] = raw_synapse_counts['Freq(Hz) (x1)'].apply(lambda x: x*1000) # PUTTING BACK\n",
    "raw_synapse_counts.rename(columns={'Case':'Subject'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 - values per vx\n",
    "\n",
    "paired = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(kHz) (x1)']), on=['Subject', 'Freq(kHz) (x1)'])\n",
    "# slice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs (y2)']]\n",
    "final = paired[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs']]\n",
    "final_clean = final.dropna()\n",
    "\n",
    "# adding in the strain feature\n",
    "strains = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Mouse groups.xlsx')\n",
    "final_clean_strained = final_clean.join(strains.set_index('ID#'), on='Subject')\n",
    "final_clean_strained['Strain'] = final_clean_strained['Strain'].str.strip()\n",
    "final_clean_strained = final_clean_strained.rename(columns={'Strain': 'Strain (x5)'})\n",
    "final_clean_strained = final_clean_strained.dropna()\n",
    "final_clean_strained = final_clean_strained[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Synapses to IHC (y1)', 'Group']]\n",
    "\n",
    "final_clean_strained_grouped = final_clean_strained.copy()\n",
    "final_clean_strained_grouped['Group - dB'] = final_clean_strained_grouped['Group'].apply(lambda x: x.split(' ')[0] if x.split(' ')[0].endswith('dB') else 'Control')\n",
    "final_clean_strained_grouped['Group - Time Elapsed'] = final_clean_strained_grouped['Group'].apply(lambda x: x.split(' ')[1] if x.split(' ')[1].endswith(('h', 'wks', 'w')) else x.split(' ')[0])\n",
    "final_clean_strained_grouped.head()\n",
    "\n",
    "final_clean_strained_grouped_pos = final_clean_strained_grouped.copy()\n",
    "final_clean_strained_grouped_pos['Amplitude (x3)'] = final_clean_strained_grouped['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup = final_clean_strained_grouped_pos.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup['Group'] = final_clean_strained_grouped_pos_cleangroup['Group'].apply(lambda x: x.strip())\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup.head()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs = final_clean_strained_grouped_pos_cleangroup.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - dB']\n",
    "# final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed']\n",
    "final_clean_strained_grouped_pos_cleangroup_vs = final_clean_strained_grouped_pos_cleangroup_vs[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)',\n",
    "       'vx (x4)', 'Strain (x5)','Group - dB (x6)', 'Group - Time Elapsed', 'Group','Synapses to IHC (y1)']]\n",
    "\n",
    "def split_on_number(input_string):\n",
    "    return re.findall(r\"[A-Za-z]+|\\d+\", input_string)\n",
    "\n",
    "hrs_week = 24*7\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed = final_clean_strained_grouped_pos_cleangroup_vs.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: '0dB' if x == 'Control' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'].apply(lambda x: x[1])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'].apply(lambda x: \"wks\" if x == 'w' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Hours Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed.apply(lambda row: row['Group - Time Elapsed - Magn.']* hrs_week if row['Group - Time Elapsed - Unit'] == 'wks' else row['Group - Time Elapsed - Magn.'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 - Averaged per Vx\n",
    "\n",
    "paired2 = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(kHz) (x1)']), on=['Subject', 'Freq(kHz) (x1)'])\n",
    "# lilslice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs']]\n",
    "final2 = paired2[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'Synapses', 'IHCs']]\n",
    "final_clean2 = final2.dropna()\n",
    "\n",
    "# adding in the strain feature\n",
    "strains = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Mouse groups.xlsx')\n",
    "final_clean_strained2 = final_clean2.join(strains.set_index('ID#'), on='Subject')\n",
    "final_clean_strained2['Strain'] = final_clean_strained2['Strain'].str.strip()\n",
    "final_clean_strained2 = final_clean_strained2.rename(columns={'Strain': 'Strain (x5)'})\n",
    "final_clean_strained2 = final_clean_strained2.dropna()\n",
    "final_clean_strained2 = final_clean_strained2[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Synapses to IHC (y1)', 'Group', 'Synapses', 'IHCs']]\n",
    "# np.unique(final_clean_strained2['Group'])\n",
    "\n",
    "# final_clean_70 = final_clean[final_clean['Level(dB) (x2)'] >= 70.0]\n",
    "# final_clean_strained_70 = final_clean_strained[final_clean_strained['Level(dB) (x2)'] >= 70.0]\n",
    "# # np.unique(final_clean['Level(dB) (x2)']) max level is 80 db\n",
    "# len(final_clean), len(final_clean_70) # 10000 less data points!!!\n",
    "\n",
    "final_clean_strained_grouped2 = final_clean_strained2.copy()\n",
    "final_clean_strained_grouped2['Group - dB'] = final_clean_strained_grouped2['Group'].apply(lambda x: x.split(' ')[0] if x.split(' ')[0].endswith('dB') else 'Control')\n",
    "final_clean_strained_grouped2['Group - Time Elapsed'] = final_clean_strained_grouped2['Group'].apply(lambda x: x.split(' ')[1] if x.split(' ')[1].endswith(('h', 'wks', 'w')) else x.split(' ')[0])\n",
    "final_clean_strained_grouped2.head()\n",
    "\n",
    "final_clean_strained_grouped_pos2 = final_clean_strained_grouped2.copy()\n",
    "final_clean_strained_grouped_pos2['Amplitude (x3)'] = final_clean_strained_grouped2['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "len(final_clean_strained_grouped_pos2[final_clean_strained_grouped_pos2['Amplitude (x3)'] < 0])\n",
    "\n",
    "final_clean_strained_grouped_pos2['Amplitude (x3)'] = final_clean_strained_grouped2['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "# final_clean_strained_grouped_pos[(final_clean_strained_grouped_pos['Subject'] == 'WPZ66') & (final_clean_strained_grouped_pos['Amplitude (x3)'] ==0.055901451434921576)\n",
    "final_clean_strained_grouped_pos_cleangroup2 = final_clean_strained_grouped_pos2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup2['Group'] = final_clean_strained_grouped_pos_cleangroup2['Group'].apply(lambda x: x.strip())\n",
    "np.unique(final_clean_strained_grouped_pos_cleangroup2['Group'])\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup2.head()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2 = final_clean_strained_grouped_pos_cleangroup2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs2['Group - dB']\n",
    "# final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed']\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2 = final_clean_strained_grouped_pos_cleangroup_vs2[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)',\n",
    "       'vx (x4)', 'Strain (x5)','Group - dB (x6)', 'Group - Time Elapsed', 'Group','Synapses to IHC (y1)', 'Synapses', 'IHCs']]\n",
    "\n",
    "def split_on_number(input_string):\n",
    "    return re.findall(r\"[A-Za-z]+|\\d+\", input_string)\n",
    "\n",
    "hrs_week = 24*7\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2 = final_clean_strained_grouped_pos_cleangroup_vs2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: '0dB' if x == 'Control' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'].apply(lambda x: x[1])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'].apply(lambda x: \"wks\" if x == 'w' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Hours Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2.apply(lambda row: row['Group - Time Elapsed - Magn.']* hrs_week if row['Group - Time Elapsed - Unit'] == 'wks' else row['Group - Time Elapsed - Magn.'], axis = 1)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2\n",
    "\n",
    "freqs = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2['Freq(kHz) (x1)'])\n",
    "subs = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2['Subject'])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx = final_clean_strained_grouped_pos_cleangroup_vs_timed2.copy()\n",
    "for freq in freqs:\n",
    "    for sub in subs:\n",
    "        mask = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)] # global for updates\n",
    "        if len(mask) > 0:\n",
    "\n",
    "            mask1 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['vx (x4)'] == 'v1')]\n",
    "            mask2 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['vx (x4)'] == 'v2')]\n",
    "\n",
    "            if not mask1.empty and not mask2.empty:\n",
    "                mask1 = mask1.reset_index().iloc[0,:]\n",
    "                mask2 = mask2.reset_index().iloc[0,:]\n",
    "\n",
    "                total_syns = float(mask1['Synapses'] + mask2['Synapses'])\n",
    "                total_ihcs = float(mask1['IHCs'] + mask2['IHCs'])\n",
    "                ratio = total_syns / total_ihcs\n",
    "                # print(total_syns, total_ihcs)\n",
    "                # if total_syns == 0.0 or total_ihcs == 0.0:\n",
    "                #     print(sub, freq)\n",
    "                mask_index = mask.index\n",
    "                final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[mask_index,'Synapse to IHC Ratio per Freq (y2)'] = ratio\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Synapse to IHC Ratio per Freq (y2)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class BayesianProfile_LRcompare:\n",
    "    def __init__(self, training_data=None, prior_mean_y1=None, prior_cov_y1=None):\n",
    "        \"\"\"\n",
    "        Initialize a Bayesian profile with static scaling\n",
    "        \n",
    "        Parameters:\n",
    "        training_data: DataFrame with initial data to fit scalers\n",
    "        prior_mean: Initial guess for parameters (default: zeros)\n",
    "        prior_cov: Initial uncertainty in parameters (default: identity matrix)\n",
    "        \"\"\"\n",
    "        # For 48 parameters: intercept, x1, x3, x5 (one-hot), x6, x7, interactions\n",
    "        self.n_params = 5\n",
    "        \n",
    "        # Initialize priors\n",
    "        if prior_mean_y1 is None:\n",
    "            self.mean_y1 = np.zeros(self.n_params)\n",
    "        else:\n",
    "            self.mean_y1 = prior_mean_y1\n",
    "            \n",
    "        if prior_cov_y1 is None:\n",
    "            self.cov_y1 = np.eye(self.n_params) * 10  # Start with high uncertainty\n",
    "        else:\n",
    "            self.cov_y1 = prior_cov_y1\n",
    "\n",
    "        # Keep track of all data points\n",
    "        self.X_history = []\n",
    "        self.y1_history = []\n",
    "        \n",
    "        # For tracking prediction performance\n",
    "        self.rmse_history_y1 = []\n",
    "        \n",
    "        # Initialize scalers\n",
    "        self.x1_scaler = StandardScaler()\n",
    "        self.x2_scaler = StandardScaler()\n",
    "        self.x3_scaler = StandardScaler()\n",
    "        \n",
    "        # Initialize scaling flag\n",
    "        self.scaling_applied = False\n",
    "        \n",
    "        # Fit scalers if training data is provided\n",
    "        if training_data is not None:\n",
    "            self._initialize_scalers(training_data)\n",
    "\n",
    "    def _initialize_scalers(self, data):\n",
    "        \"\"\"\n",
    "        Initialize scalers with training data\n",
    "        \"\"\"\n",
    "        if 'Freq(kHz) (x1)' in data.columns and 'Level(dB) (x2)' in data.columns and 'Amplitude (x3)' in data.columns:\n",
    "            # Fit scalers to all training data once\n",
    "            self.x1_scaler.fit(data['Freq(kHz) (x1)'].values.reshape(-1,1))\n",
    "            self.x2_scaler.fit(data['Level(dB) (x2)'].values.reshape(-1,1))\n",
    "            self.x3_scaler.fit(data['Amplitude (x3)'].values.reshape(-1, 1))\n",
    "            self.scaling_applied = True\n",
    "            print(f\"Scalers initialized with {len(data)} records\")\n",
    "        else:\n",
    "            print(\"Warning: Training data missing required columns for scaling\")\n",
    "\n",
    "    def _scale_x1(self, x1):\n",
    "        \"\"\"\n",
    "        Scale x3 using the fitted scaler\n",
    "        \"\"\"\n",
    "        if not self.scaling_applied:\n",
    "            return x1  # Return as is if scaler not fit\n",
    "        \n",
    "        x1_array = np.array([float(x1)]).reshape(-1, 1)\n",
    "        return self.x1_scaler.transform(x1_array)[0][0]\n",
    "\n",
    "    def _scale_x2(self, x2):\n",
    "        \"\"\"\n",
    "        Scale x3 using the fitted scaler\n",
    "        \"\"\"\n",
    "        if not self.scaling_applied:\n",
    "            return x2  # Return as is if scaler not fit\n",
    "        \n",
    "        x2_array = np.array([float(x2)]).reshape(-1, 1)\n",
    "        return self.x2_scaler.transform(x2_array)[0][0]\n",
    "\n",
    "    def _scale_x3(self, x3):\n",
    "        \"\"\"\n",
    "        Scale x3 using the fitted scaler\n",
    "        \"\"\"\n",
    "        if not self.scaling_applied:\n",
    "            return x3  # Return as is if scaler not fit\n",
    "        \n",
    "        x3_array = np.array([float(x3)]).reshape(-1, 1)\n",
    "        return self.x3_scaler.transform(x3_array)[0][0]\n",
    "        \n",
    "    def add_observation_y1(self, x1, x2, x3, y1, noise_var=1.0, prior_mean_y1=None, prior_cov_y1=None):\n",
    "        \"\"\"\n",
    "        Update the profile with a new observation using static scaling\n",
    "        \"\"\"\n",
    "        self.raw_x1 = float(x1)\n",
    "        self.raw_x2 = float(x2)\n",
    "        self.raw_x3 = float(x3)\n",
    "        self.raw_y1 = float(y1)\n",
    "        \n",
    "        x1_scaled = self._scale_x1(x1)\n",
    "        x2_scaled = self._scale_x2(x2)\n",
    "        x3_scaled = self._scale_x3(x3)\n",
    "        \n",
    "        # Reshape for processing\n",
    "        x1_scaled = np.asarray(x1_scaled).reshape(1, -1)\n",
    "        x2_scaled = np.asarray(x2_scaled).reshape(1, -1)\n",
    "        x3_scaled = np.asarray(x3_scaled).reshape(1, -1)\n",
    "        \n",
    "        # Create arrays to stack with correct shapes\n",
    "        arrays_to_stack = [\n",
    "            np.ones(1).reshape(1, -1),        \n",
    "            x1_scaled,\n",
    "            x2_scaled,\n",
    "            x3_scaled,\n",
    "            x1_scaled.T @ x3_scaled                                  \n",
    "        ]\n",
    "\n",
    "        X = np.hstack(arrays_to_stack)\n",
    "        self.X_history.append(X[0])\n",
    "        # self.y1_history.append(float(y1_scaled))\n",
    "        self.y1_history.append(float(y1))\n",
    "        \n",
    "        # Numerical stability in matrix inversion\n",
    "        try:\n",
    "            # regularization for numerical stability\n",
    "            K = self.cov_y1 @ X.T @ np.linalg.inv(X @ self.cov_y1 @ X.T + noise_var + 1e-8 * np.eye(X.shape[0]))\n",
    "            \n",
    "            innovation = y1 - float(X @ self.mean_y1)\n",
    "            self.mean_y1 = self.mean_y1 + (K.flatten() * innovation) # our coefficients/parameters!\n",
    "            self.cov_y1 = self.cov_y1 - (K @ X @ self.cov_y1) # prep for our next set of observations\n",
    "        except np.linalg.LinAlgError as e:\n",
    "            print(f\"Matrix inversion error: {e}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            predictions = []\n",
    "            for i, x_hist in enumerate(self.X_history):\n",
    "                # Extract features from history\n",
    "                hist_x1 = x_hist[1]\n",
    "                hist_x2 = x_hist[2]\n",
    "                hist_x3 = x_hist[3]\n",
    "                    \n",
    "                # Try to predict in scaled space\n",
    "                try:\n",
    "                    pred = self.predict_y1(x1=hist_x1, x2=hist_x2, x3=hist_x3)\n",
    "                    if pred is not None and np.isfinite(pred):\n",
    "                        # Convert back to scaled space for RMSE calculation\n",
    "                        # scaled_pred = self._scale_y1(pred)\n",
    "                        predictions.append(pred)\n",
    "                    else:\n",
    "                        predictions.append(self.y1_history[i])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in !prediction! for RMSE: {e}\")\n",
    "                    predictions.append(self.y1_history[i])\n",
    "                    \n",
    "            if predictions:\n",
    "                rmse = np.sqrt(np.mean((np.array(self.y1_history) - np.array(predictions))**2))\n",
    "                if np.isfinite(rmse):\n",
    "                    self.rmse_history_y1.append(rmse)\n",
    "                else:\n",
    "                    # If we got an invalid RMSE, append the last valid one or 0\n",
    "                    if self.rmse_history_y1:\n",
    "                        self.rmse_history_y1.append(self.rmse_history_y1[-1])\n",
    "                        print(\"Invalid RMSE!! Check here\")\n",
    "                    else:\n",
    "                        self.rmse_history_y1.append(0.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error !calculating! RMSE: {e}\")\n",
    "            # Append last RMSE or 0 if none exists\n",
    "            if self.rmse_history_y1:\n",
    "                self.rmse_history_y1.append(self.rmse_history_y1[-1])\n",
    "            else:\n",
    "                self.rmse_history_y1.append(0.0)\n",
    "\n",
    "    def predict_y1(self, x1, x2, x3):\n",
    "        \"\"\"\n",
    "        Make a prediction for given input values with static scaling\n",
    "        \n",
    "        Parameters:\n",
    "        x1, x3: Input features (will be scaled if scaling is enabled)\n",
    "        x5: Categorical feature\n",
    "        x6, x7: Additional input features\n",
    "        \n",
    "        Returns:\n",
    "        float: Predicted value (in original scale)\n",
    "        \"\"\"\n",
    "        # Apply scaling if enabled\n",
    "        x1_scaled = self._scale_x1(x1)\n",
    "        x2_scaled = self._scale_x2(x2)\n",
    "        x3_scaled = self._scale_x3(x3)\n",
    "        \n",
    "        # Reshape and encode\n",
    "        x1_scaled = np.asarray(x1_scaled).reshape(1, -1)\n",
    "        x2_scaled = np.asarray(x2_scaled).reshape(1, -1)\n",
    "        x3_scaled = np.asarray(x3_scaled).reshape(1, -1)\n",
    "        \n",
    "        # Create arrays to stack with correct shapes\n",
    "        arrays_to_stack = [\n",
    "            np.ones(1).reshape(1, -1),        \n",
    "            x1_scaled,\n",
    "            x2_scaled,\n",
    "            x3_scaled,\n",
    "            x1_scaled.T @ x3_scaled                                  \n",
    "        ]\n",
    "\n",
    "        X = np.hstack(arrays_to_stack)\n",
    "        \n",
    "        # Get prediction\n",
    "        # # y1_scaled_pred = float(X @ self.mean_y1)\n",
    "        \n",
    "        # # Check if prediction is valid\n",
    "        # if not np.isfinite(y1_scaled_pred):\n",
    "        #     print(f\"Warning: Non-finite prediction: {y1_scaled_pred}\")\n",
    "        #     return None\n",
    "        \n",
    "        # pred = self._inverse_scale_y1(y1_scaled_pred)\n",
    "\n",
    "        raw_pred = float(X @ self.mean_y1)\n",
    "        if not np.isfinite(raw_pred):\n",
    "            print(f\"Non-finite prediction for x1={x1}, x2={x2}, x3={x3}\")\n",
    "            print(f\"Scaled values: x1={x1_scaled}, x2={x2_scaled}, x3={x3_scaled}\")\n",
    "            print(f\"Model coefficients: {self.mean_y1}\")\n",
    "        pred = max(0, pred)\n",
    "        return pred\n",
    "\n",
    "    def predict_with_uncertainty_y1(self, x1, x2, x3):\n",
    "        \"\"\"\n",
    "        Make a prediction with uncertainty bounds\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (prediction, standard_deviation)\n",
    "        \"\"\"\n",
    "        # Apply scaling if enabled\n",
    "        x1_scaled = self._scale_x1(x1)\n",
    "        x2_scaled = self._scale_x2(x2)\n",
    "        x3_scaled = self._scale_x3(x3)\n",
    "        \n",
    "        # Reshape and encode\n",
    "        x1_scaled = np.asarray(x1_scaled).reshape(1, -1)\n",
    "        x2_scaled = np.asarray(x2_scaled).reshape(1, -1)\n",
    "        x3_scaled = np.asarray(x3_scaled).reshape(1, -1)\n",
    "\n",
    "        # Create arrays to stack with correct shapes\n",
    "        arrays_to_stack = [\n",
    "            np.ones(1).reshape(1, -1),        \n",
    "            x1_scaled,\n",
    "            x2_scaled,\n",
    "            x3_scaled,\n",
    "            x1_scaled.T @ x3_scaled                                  \n",
    "        ]\n",
    "        \n",
    "        X = np.hstack(arrays_to_stack)\n",
    "        \n",
    "        # Predict\n",
    "        pred = float(X @ self.mean_y1)\n",
    "        std = float(np.sqrt(X @ self.cov_y1 @ X.T))\n",
    "        # pred_scaled = float(X @ self.mean_y1)\n",
    "        # std_scaled = float(np.sqrt(X @ self.cov_y1 @ X.T))\n",
    "\n",
    "        # if self.scaling_applied:\n",
    "        #     std = std_scaled * self.y1_scaler.scale_[0]\n",
    "        # else:\n",
    "        #     std = std_scaled\n",
    "        \n",
    "        # Check for valid values\n",
    "        if not np.isfinite(pred) or not np.isfinite(std):\n",
    "            print(f\"Warning: Non-finite prediction or std: {pred}, {std}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        # pred = self._inverse_scale_y1(pred_scaled)\n",
    "\n",
    "        pred = float(X @ self.mean_y1)\n",
    "        pred = max(0, pred)\n",
    "        std = float(np.sqrt(X @ self.cov_y1 @ X.T))\n",
    "\n",
    "        if not np.isfinite(pred) or not np.isfinite(std):\n",
    "            print(f\"Warning: Non-finite prediction or std: {pred}, {std}\")\n",
    "            return None, None\n",
    "        # Convert back to original scale if scaling was applied\n",
    "        return pred, std\n",
    "    \n",
    "    \n",
    "    def plot_learning_curve_y1(self):\n",
    "        \"\"\"Plot how RMSE changes as more observations are added\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.rmse_history_y1, '-o')\n",
    "        plt.xlabel('Number of Observations')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('Learning Curve - Average Synapses to IHC Prediction')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data based on ABR recording frequencies**** and compare!!!!!!!\n",
    "# Because it doesn't make sense to have TT by subject for this given one model is specific to a subject...\n",
    "np.random.seed(42)\n",
    "groups = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'])\n",
    "sorted_freqs = sorted(np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)']))\n",
    "\n",
    "freq_groups = {}\n",
    "for i, freq in enumerate(sorted_freqs):\n",
    "    # Splits up the frequencies into n groups\n",
    "    group_idx = i % 3\n",
    "    if group_idx not in freq_groups:\n",
    "        freq_groups[group_idx] = []\n",
    "    freq_groups[group_idx].append(freq)\n",
    "\n",
    "print(freq_groups)\n",
    "\n",
    "recs_by_group = {}\n",
    "for group in final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'].unique():\n",
    "    recs_in_group = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'] == group]\n",
    "    recs_by_group[group] = recs_in_group\n",
    "\n",
    "train_freqs = []\n",
    "test_freqs = []\n",
    "\n",
    "for group_idx, freqs in freq_groups.items():\n",
    "    # Shuffle frequencies within this group (kinda like RF???? randomly splits on which freqs to use in train/test)\n",
    "    np.random.shuffle(freqs)\n",
    "    \n",
    "    n_test = max(1, round(len(freqs) * 0.2))  # Ensuring at least 2 frequencies are used for testing\n",
    "\n",
    "    \n",
    "    # Add to overall train/test sets\n",
    "    test_freqs.extend(freqs[:n_test])\n",
    "    train_freqs.extend(freqs[n_test:])\n",
    "\n",
    "# train_indices = []\n",
    "# test_indices = []\n",
    "\n",
    "# for group in np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group']):\n",
    "#     group_recs = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Group'] == group]\n",
    "#     train_group_indices = group_recs[group_recs['Freq(kHz) (x1)'].isin(train_freqs)].index.tolist()\n",
    "#     test_group_indices = group_recs[group_recs['Freq(kHz) (x1)'].isin(test_freqs)].index.tolist()\n",
    "\n",
    "#     test_indices.extend(test_group_indices)\n",
    "#     train_indices.extend(train_group_indices)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for subject in np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject']):\n",
    "    subject_recs = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == subject]\n",
    "\n",
    "    train_subject_indices = subject_recs[subject_recs['Freq(kHz) (x1)'].isin(train_freqs)].index.tolist()\n",
    "    test_subject_indices = subject_recs[subject_recs['Freq(kHz) (x1)'].isin(test_freqs)].index.tolist()\n",
    "\n",
    "    # Ensure the subject has data in both splits\n",
    "    if len(train_subject_indices) == 0 or len(test_subject_indices) == 0:\n",
    "        continue  # Skip this subject to avoid leakage problems\n",
    "\n",
    "    test_indices.extend(test_subject_indices)\n",
    "    train_indices.extend(train_subject_indices)\n",
    "\n",
    "\n",
    "predictors = ['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Group']\n",
    "X_train10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[train_indices, predictors]\n",
    "X_test10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[test_indices, predictors]\n",
    "y1_train10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[train_indices, 'Synapse to IHC Ratio per Freq (y2)']\n",
    "y1_test10 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[test_indices, 'Synapse to IHC Ratio per Freq (y2)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation with frequency-based splitting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "thresh = 0\n",
    "train_rmse_scores = []\n",
    "val_rmse_scores = []\n",
    "subjects_profiles_CVsplit_LR = {}\n",
    "all_train_data = {}\n",
    "all_val_data = {}\n",
    "\n",
    "# Set seeds for consistency\n",
    "np.random.seed(42)\n",
    "\n",
    "# Groups setup\n",
    "groups = [str(group) for group in np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed['Group'])]\n",
    "group_train_rmse = {group: [] for group in groups}\n",
    "group_val_rmse = {group: [] for group in groups}\n",
    "\n",
    "# Get all unique frequencies\n",
    "all_freqs = sorted(np.unique(X_train10['Freq(kHz) (x1)']))\n",
    "print(f\"All frequencies (kHz): {all_freqs}\")\n",
    "print(f\"Number of unique frequencies: {len(all_freqs)}\")\n",
    "\n",
    "# Determine number of splits based on available frequencies\n",
    "n_splits = min(5, len(all_freqs))\n",
    "print(f\"Using {n_splits} splits based on available frequencies\")\n",
    "\n",
    "# Custom function to create frequency-based folds\n",
    "def create_freq_based_folds(X, n_splits=5, random_state=42):\n",
    "    # Initialize randomness\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get all frequencies\n",
    "    all_freqs = sorted(np.unique(X['Freq(kHz) (x1)']))\n",
    "    \n",
    "    # Adjust n_splits if necessary\n",
    "    n_splits = min(n_splits, len(all_freqs))\n",
    "    \n",
    "    # Shuffle frequencies\n",
    "    shuffled_freqs = all_freqs.copy()\n",
    "    np.random.shuffle(shuffled_freqs)\n",
    "    \n",
    "    # Create n_splits approximately equal groups of frequencies\n",
    "    freq_groups = {}\n",
    "    for i, freq in enumerate(shuffled_freqs):\n",
    "        group_idx = i % n_splits\n",
    "        if group_idx not in freq_groups:\n",
    "            freq_groups[group_idx] = []\n",
    "        freq_groups[group_idx].append(freq)\n",
    "    \n",
    "    # Print frequency groups\n",
    "    print(\"Frequency groups:\")\n",
    "    for i in range(n_splits):\n",
    "        print(f\"  Group {i+1}: {sorted(freq_groups[i])}\")\n",
    "    \n",
    "    # Create folds\n",
    "    folds = []\n",
    "    for fold_idx in range(n_splits):\n",
    "        # Validation frequencies for this fold\n",
    "        val_freqs = freq_groups[fold_idx]\n",
    "        \n",
    "        # Training frequencies (all except validation)\n",
    "        train_freqs = []\n",
    "        for idx in range(n_splits):\n",
    "            if idx != fold_idx:\n",
    "                train_freqs.extend(freq_groups[idx])\n",
    "        \n",
    "        # Create train and validation indices\n",
    "        train_mask = X['Freq(kHz) (x1)'].isin(train_freqs)\n",
    "        val_mask = X['Freq(kHz) (x1)'].isin(val_freqs)\n",
    "        \n",
    "        train_indices = X[train_mask].index\n",
    "        val_indices = X[val_mask].index\n",
    "        \n",
    "        folds.append((train_indices, val_indices))\n",
    "        \n",
    "        # Print frequency distributions\n",
    "        print(f\"Fold {fold_idx+1}:\")\n",
    "        print(f\"  Training frequencies: {sorted(train_freqs)}\")\n",
    "        print(f\"  Validation frequencies: {sorted(val_freqs)}\")\n",
    "    \n",
    "    return folds\n",
    "\n",
    "# Create frequency-based folds\n",
    "freq_folds = create_freq_based_folds(X_train10, n_splits=n_splits)\n",
    "\n",
    "# Iterate through frequency-based folds\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(freq_folds):\n",
    "    X_fold_train, X_fold_val = X_train10.loc[train_idx], X_train10.loc[val_idx]\n",
    "    y_fold_train, y_fold_val = y1_train10.loc[train_idx], y1_train10.loc[val_idx]\n",
    "    \n",
    "    print(f\"\\nProcessing Fold {fold_idx+1}\")\n",
    "    train_subjects = set(X_fold_train['Subject'])\n",
    "    val_subjects = set(X_fold_val['Subject'])\n",
    "    \n",
    "    # Check subject overlap\n",
    "    shared_subjects = train_subjects.intersection(val_subjects)\n",
    "    print(f\"Training subjects: {len(train_subjects)}\")\n",
    "    print(f\"Validation subjects: {len(val_subjects)}\")\n",
    "    print(f\"Shared subjects: {len(shared_subjects)}/{len(val_subjects)} ({100*len(shared_subjects)/len(val_subjects):.1f}%)\")\n",
    "    \n",
    "    # Reset profiles for this fold\n",
    "    subjects_profiles_CVsplit_LR = {}\n",
    "    \n",
    "    # First, build the Bayesian profiles for each subject\n",
    "    for subject in train_subjects:\n",
    "        profile_key = f\"{subject}\"\n",
    "        subjects_profiles_CVsplit_LR[profile_key] = BayesianProfile_LRcompare()\n",
    "        \n",
    "        subject_data = X_fold_train[X_fold_train['Subject'] == subject]\n",
    "        \n",
    "        # Group by the combination of features that define unique conditions\n",
    "        grouped = subject_data.groupby([\n",
    "            'Freq(kHz) (x1)', 'Amplitude (x3)', 'Strain (x5)',   \n",
    "            'Group'\n",
    "        ])\n",
    "        \n",
    "        # Process each group (condition)\n",
    "        for group_key, group_data in grouped:\n",
    "            if group_data['Level(dB) (x2)'].min() < thresh:\n",
    "                continue\n",
    "                \n",
    "            # Get all y1 values for this condition\n",
    "            indices = group_data.index\n",
    "            y_values = y_fold_train.loc[indices]\n",
    "            \n",
    "            # Extract feature values (same for all rows in group)\n",
    "            x1_val = group_data['Freq(kHz) (x1)'].iloc[0]\n",
    "            x2_val = group_data['Level(dB) (x2)'].iloc[0]\n",
    "            x3_val = group_data['Amplitude (x3)'].iloc[0]\n",
    "            \n",
    "            # Add each observation with same features but different y values\n",
    "            for idx, y1_val in zip(indices, y_values):\n",
    "                try:\n",
    "                    subjects_profiles_CVsplit_LR[profile_key].add_observation_y1(\n",
    "                        x1=x1_val, x2=x2_val, x3=x3_val,\n",
    "                        y1=float(y1_val),\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error adding observation for subject {subject}: {e}\")\n",
    "    \n",
    "    # Now calculate RMSE for this fold (both training and validation)\n",
    "    # Store predictions in a dictionary keyed by index\n",
    "    train_predictions = {}\n",
    "    \n",
    "    for idx, row in X_fold_train.iterrows():\n",
    "        try:\n",
    "            # First define the subject and profile_key\n",
    "            subject = row['Subject']\n",
    "            profile_key = f\"{subject}\"\n",
    "            \n",
    "            # Then check conditions\n",
    "            if row['Level(dB) (x2)'] >= thresh and profile_key in subjects_profiles_CVsplit_LR:\n",
    "                x1_val = row['Freq(kHz) (x1)']\n",
    "                x2_val = row['Level(dB) (x2)']\n",
    "                x3_val = row['Amplitude (x3)']\n",
    "                \n",
    "                pred = subjects_profiles_CVsplit_LR[profile_key].predict_y1(x1=x1_val, x2=x2_val, x3=x3_val)\n",
    "                if pred is not None and np.isfinite(pred):\n",
    "                    train_predictions[idx] = float(pred)  # Ensure it's a scalar\n",
    "        except Exception as e:\n",
    "            print(f\"Error in train prediction for row {idx}: {e}\")\n",
    "\n",
    "    # Create pandas Series with predictions matched to proper indices\n",
    "    successful_train_indices = list(train_predictions.keys())\n",
    "    \n",
    "    if not successful_train_indices:\n",
    "        print(f\"Fold {fold_idx+1}: No successful training predictions\")\n",
    "        continue  # Skip to next fold\n",
    "        \n",
    "    y_train_true = y_fold_train.loc[successful_train_indices]\n",
    "    y_train_pred = pd.Series([train_predictions[idx] for idx in successful_train_indices], \n",
    "                            index=successful_train_indices)\n",
    "    \n",
    "    # Do the same for validation\n",
    "    val_predictions = {}\n",
    "    \n",
    "    for idx, row in X_fold_val.iterrows():\n",
    "        try:\n",
    "            # First define the subject and profile_key\n",
    "            subject = row['Subject']\n",
    "            profile_key = f\"{subject}\"\n",
    "            \n",
    "            # Then check conditions\n",
    "            if row['Level(dB) (x2)'] >= thresh and profile_key in subjects_profiles_CVsplit_LR:\n",
    "                x1_val = row['Freq(kHz) (x1)']\n",
    "                x2_val = row['Level(dB) (x2)']\n",
    "                x3_val = row['Amplitude (x3)']\n",
    "                \n",
    "                pred = subjects_profiles_CVsplit_LR[profile_key].predict_y1(x1=x1_val, x2=x2_val, x3=x3_val)\n",
    "                if pred is not None and np.isfinite(pred):\n",
    "                    val_predictions[idx] = float(pred)  # Ensure it's a scalar\n",
    "        except Exception as e:\n",
    "            print(f\"Error in validation prediction for row {idx}: {e}\")\n",
    "    \n",
    "    # Create properly indexed validation predictions\n",
    "    successful_val_indices = list(val_predictions.keys())\n",
    "    \n",
    "    if not successful_val_indices:\n",
    "        print(f\"Fold {fold_idx+1}: No successful validation predictions\")\n",
    "        continue  # Skip to next fold\n",
    "        \n",
    "    y_val_true = y_fold_val.loc[successful_val_indices]\n",
    "    y_val_pred = pd.Series([val_predictions[idx] for idx in successful_val_indices], \n",
    "                       index=successful_val_indices)\n",
    "    \n",
    "    # Print basic information\n",
    "    print(f\"Train: {len(successful_train_indices)} successful predictions out of {len(X_fold_train)}\")\n",
    "    print(f\"Validation: {len(successful_val_indices)} successful predictions out of {len(X_fold_val)}\")\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    true_name = f'{fold_idx} - train - true'\n",
    "    pred_name = f'{fold_idx} - train - pred'\n",
    "    all_train_data[true_name] = y_train_true\n",
    "    all_train_data[pred_name] = y_train_pred\n",
    "\n",
    "    fold_train_rmse = np.sqrt(np.mean((y_train_true - y_train_pred)**2))\n",
    "    train_rmse_scores.append(fold_train_rmse)\n",
    "    \n",
    "    true_name = f'{fold_idx} - val - true'\n",
    "    pred_name = f'{fold_idx} - val - pred'\n",
    "    all_val_data[true_name] = y_val_true\n",
    "    all_val_data[pred_name] = y_val_pred\n",
    "\n",
    "    fold_val_rmse = np.sqrt(np.mean((y_val_true - y_val_pred)**2))\n",
    "    val_rmse_scores.append(fold_val_rmse)\n",
    "    \n",
    "    print(f\"Fold {fold_idx+1}: Train RMSE = {fold_train_rmse:.4f}, Validation RMSE = {fold_val_rmse:.4f}\")\n",
    "    \n",
    "    # Calculate group-specific RMSE\n",
    "    for group in groups:\n",
    "        try:\n",
    "            # Get the subject groups\n",
    "            train_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed.loc[successful_train_indices]\n",
    "            \n",
    "            # Filter for successful predictions for this group\n",
    "            group_train_mask = train_subjects_df['Group'] == group\n",
    "            if group_train_mask.any():\n",
    "                group_indices = group_train_mask.index[group_train_mask]\n",
    "                group_rmse = np.sqrt(np.mean((y_train_true.loc[group_indices] - y_train_pred.loc[group_indices])**2))\n",
    "                group_train_rmse[group].append(group_rmse)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating train RMSE for group {group}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            val_subjects_df = final_clean_strained_grouped_pos_cleangroup_vs_timed.loc[successful_val_indices]\n",
    "            group_val_mask = val_subjects_df['Group'] == group\n",
    "            if group_val_mask.any():\n",
    "                group_indices = group_val_mask.index[group_val_mask]\n",
    "                group_rmse = np.sqrt(np.mean((y_val_true.loc[group_indices] - y_val_pred.loc[group_indices])**2))\n",
    "                group_val_rmse[group].append(group_rmse)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating validation RMSE for group {group}: {e}\")\n",
    "\n",
    "# Calculate average RMSE across all folds\n",
    "if train_rmse_scores:\n",
    "    avg_train_rmse = np.mean(train_rmse_scores)\n",
    "    std_train_rmse = np.std(train_rmse_scores)\n",
    "    print(f\"\\nAverage Train RMSE across folds: {avg_train_rmse:.4f} ± {std_train_rmse:.4f}\")\n",
    "\n",
    "if val_rmse_scores:\n",
    "    avg_val_rmse = np.mean(val_rmse_scores)\n",
    "    std_val_rmse = np.std(val_rmse_scores)\n",
    "    print(f\"Average Validation RMSE across folds: {avg_val_rmse:.4f} ± {std_val_rmse:.4f}\")\n",
    "\n",
    "# Calculate group-specific RMSE \n",
    "print(\"\\nGroup-specific RMSE:\")\n",
    "print(\"Group | Train RMSE (Mean ± Std) | Validation RMSE (Mean ± Std)\")\n",
    "print(\"----- | ----------------------- | -----------------------------\")\n",
    "for group in groups:\n",
    "    train_values = group_train_rmse[group]\n",
    "    val_values = group_val_rmse[group]\n",
    "    \n",
    "    if train_values:\n",
    "        train_mean = np.mean(train_values)\n",
    "        train_std = np.std(train_values)\n",
    "        train_str = f\"{train_mean:.6f} ± {train_std:.6f}\"\n",
    "    else:\n",
    "        train_str = \"N/A\"\n",
    "        \n",
    "    if val_values:\n",
    "        val_mean = np.mean(val_values)\n",
    "        val_std = np.std(val_values)\n",
    "        val_str = f\"{val_mean:.6f} ± {val_std:.6f}\"\n",
    "    else:\n",
    "        val_str = \"N/A\"\n",
    "        \n",
    "    print(f\"{group:15s} | {train_str:25s} | {val_str:25s}\")\n",
    "\n",
    "# Visualization of group-specific RMSE\n",
    "if any(group_train_rmse[group] for group in groups) and any(group_val_rmse[group] for group in groups):\n",
    "    # Prepare data for plotting\n",
    "    plot_groups = []\n",
    "    plot_train_rmse = []\n",
    "    plot_val_rmse = []\n",
    "    plot_train_std = []\n",
    "    plot_val_std = []\n",
    "    \n",
    "    for group in groups:\n",
    "        if group_train_rmse[group] and group_val_rmse[group]:\n",
    "            plot_groups.append(group)\n",
    "            plot_train_rmse.append(np.mean(group_train_rmse[group]))\n",
    "            plot_val_rmse.append(np.mean(group_val_rmse[group]))\n",
    "            plot_train_std.append(np.std(group_train_rmse[group]))\n",
    "            plot_val_std.append(np.std(group_val_rmse[group]))\n",
    "    \n",
    "    # Create plot if we have data\n",
    "    if plot_groups:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        x = np.arange(len(plot_groups))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, plot_train_rmse, width, yerr=plot_train_std, \n",
    "                label='Train RMSE', color='blue', alpha=0.7, capsize=5)\n",
    "        plt.bar(x + width/2, plot_val_rmse, width, yerr=plot_val_std,\n",
    "                label='Validation RMSE', color='red', alpha=0.7, capsize=5)\n",
    "        \n",
    "        plt.xlabel('Group')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.ylim((0,5))\n",
    "        plt.title('Bayesian Model Performance by Group (Mean ± Std)')\n",
    "        plt.xticks(x, plot_groups, rotation=90)\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create DataFrame with group-specific metrics\n",
    "groups_list = []\n",
    "train_mean_list = []\n",
    "train_std_list = []\n",
    "val_mean_list = []\n",
    "val_std_list = []\n",
    "\n",
    "# Extract values for each group (only once)\n",
    "for group in sorted(group_train_rmse.keys()):\n",
    "    train_values = group_train_rmse[group]\n",
    "    val_values = group_val_rmse[group]\n",
    "    \n",
    "    groups_list.append(group)\n",
    "    \n",
    "    if train_values:\n",
    "        train_mean_list.append(np.mean(train_values))\n",
    "        train_std_list.append(np.std(train_values))\n",
    "    else:\n",
    "        train_mean_list.append(np.nan)\n",
    "        train_std_list.append(np.nan)\n",
    "        \n",
    "    if val_values:\n",
    "        val_mean_list.append(np.mean(val_values))\n",
    "        val_std_list.append(np.std(val_values))\n",
    "    else:\n",
    "        val_mean_list.append(np.nan)\n",
    "        val_std_list.append(np.nan)\n",
    "\n",
    "group_metrics_df= pd.DataFrame({\n",
    "    'Group': groups_list,\n",
    "    'Train_RMSE_Mean': train_mean_list,\n",
    "    'Val_RMSE_Mean': val_mean_list,\n",
    "    'Train_RMSE_Std': train_std_list,\n",
    "    'Val_RMSE_Std': val_std_list\n",
    "})\n",
    "\n",
    "group_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db98data = pd.DataFrame(group_metrics_df.iloc[-1,:]).T\n",
    "group_metrics_df_copy = group_metrics_df.copy()\n",
    "group_metrics_df_copy.iloc[-1,:] = group_metrics_df_copy.iloc[0,:]\n",
    "group_metrics_df_copy = group_metrics_df_copy[1:]\n",
    "# group_metrics_df_copy.iloc[0,:] = group_metrics_df_copy.iloc[1,:]\n",
    "r14 = group_metrics_df_copy.iloc[0,:]\n",
    "r6 = group_metrics_df_copy.iloc[1,:]\n",
    "r8 = group_metrics_df_copy.iloc[2,:]\n",
    "group_metrics_df_copy.iloc[0,:] = r6\n",
    "group_metrics_df_copy.iloc[1,:] = r8\n",
    "group_metrics_df_copy.iloc[2,:] = r14\n",
    "# group_metrics_df_copy['Group'] = group_metrics_df_copy['Group'].apply(lambda x: f\"'{x}'\")\n",
    "db101 = pd.DataFrame(group_metrics_df_copy.iloc[-1,:]).T\n",
    "group_metrics_df_copy.drop(group_metrics_df_copy.tail(1).index,inplace=True)\n",
    "# group_metrics_df_copy = pd.concat([group_metrics_df_copy, db98data], axis=0)\n",
    "# group_metrics_df_copy = pd.concat([group_metrics_df_copy, db101], axis=0)\n",
    "group_metrics_df_copy_final = pd.concat([group_metrics_df_copy, db98data, db101], ignore_index=True)\n",
    "\n",
    "group_metrics_df_copy_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_manorimagepred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
