{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508e66ad",
   "metadata": {},
   "source": [
    "# 0. Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c615134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.distributed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ABRA_35 import interpolate_and_smooth, CNN, plot_wave, calculate_and_plot_wave, plot_waves_single_frequency, arfread, get_str, calculate_hearing_threshold, all_thresholds, peak_finding\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import os\n",
    "import struct\n",
    "import datetime\n",
    "# from skfda import FDataGrid\n",
    "# from skfda.preprocessing.dim_reduction import FPCA\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import torch.nn as nn\n",
    "import splitfolders\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.signal import find_peaks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np4\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "import skfda\n",
    "from skfda.ml.regression import LinearRegression \n",
    "from skfda.representation.basis import BSplineBasis, FourierBasis, FDataBasis\n",
    "from skfda.representation import FDataGrid\n",
    "from skfda.preprocessing.smoothing import BasisSmoother\n",
    "# from skfda.regression.linear_model import LinearFunctionalRegressor is no longer available? :(\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "69259896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(1, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (fc1): Linear(in_features=1952, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (dropout_fc): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter1 = 128\n",
    "filter2 = 32\n",
    "dropout1 = 0.5\n",
    "dropout2 = 0.3\n",
    "dropout_fc = 0.1\n",
    "\n",
    "# Model initialization\n",
    "peak_finding_model = CNN(filter1, filter2, dropout1, dropout2, dropout_fc)\n",
    "model_loader = torch.load('./models/waveI_cnn.pth')\n",
    "peak_finding_model.load_state_dict(model_loader)\n",
    "peak_finding_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "321670b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finding(wave):\n",
    "    # Prepare waveform\n",
    "    waveform=interpolate_and_smooth(wave) # Added indexing per calculate and plot wave function\n",
    "    # waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0) archived ABRA\n",
    "    waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).unsqueeze(0) #newer ABRA\n",
    "    # print(waveform_torch)\n",
    "    # Get prediction from model\n",
    "    outputs = peak_finding_model(waveform_torch)\n",
    "    prediction = int(round(outputs.detach().numpy()[0][0], 0))\n",
    "    # prediction_test = int(round(outputs.detach().numpy()[0], 0))\n",
    "    # print(\"Model output:\", outputs, \"Prediction true start:\", prediction)\n",
    "\n",
    "    # Apply Gaussian smoothing\n",
    "    smoothed_waveform = gaussian_filter1d(waveform, sigma=1)\n",
    "\n",
    "    # Find peaks and troughs\n",
    "    n = 18\n",
    "    t = 14\n",
    "    # start_point = prediction - 9 archived ABRA\n",
    "    start_point = prediction - 6 #newer ABRA\n",
    "    smoothed_peaks, _ = find_peaks(smoothed_waveform[start_point:], distance=n)\n",
    "    smoothed_troughs, _ = find_peaks(-smoothed_waveform, distance=t)\n",
    "    sorted_indices = np.argsort(smoothed_waveform[smoothed_peaks+start_point])\n",
    "    highest_smoothed_peaks = np.sort(smoothed_peaks[sorted_indices[-5:]] + start_point)\n",
    "    relevant_troughs = np.array([])\n",
    "    for p in range(len(highest_smoothed_peaks)):\n",
    "        c = 0\n",
    "        for t in smoothed_troughs:\n",
    "            if t > highest_smoothed_peaks[p]:\n",
    "                if p != 4:\n",
    "                    try:\n",
    "                        if t < highest_smoothed_peaks[p+1]:\n",
    "                            relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                else:\n",
    "                    relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                    break\n",
    "    relevant_troughs = relevant_troughs.astype('i')\n",
    "    return highest_smoothed_peaks, relevant_troughs\n",
    "\n",
    "def extract_metadata(metadata_lines):\n",
    "    # Dictionary to store extracted metadata\n",
    "    metadata = {}\n",
    "    \n",
    "    for line in metadata_lines:\n",
    "        # Extract SW FREQ\n",
    "        freq_match = re.search(r'SW FREQ:\\s*(\\d+\\.?\\d*)', line)\n",
    "        if freq_match:\n",
    "            metadata['SW_FREQ'] = float(freq_match.group(1))\n",
    "        \n",
    "        # Extract LEVELS\n",
    "        levels_match = re.search(r':LEVELS:\\s*([^:]+)', line)\n",
    "        if levels_match:\n",
    "            # Split levels and convert to list of floats\n",
    "            metadata['LEVELS'] = [float(level) for level in levels_match.group(1).split(';') if level]\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def read_custom_tsv(file_path):\n",
    "    # Read the entire file\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split the content into metadata and data sections\n",
    "    metadata_lines = []\n",
    "    data_section = None\n",
    "    \n",
    "    # Find the ':DATA' marker\n",
    "    data_start = content.find(':DATA')\n",
    "    \n",
    "    if data_start != -1:\n",
    "        # Extract metadata (lines before ':DATA')\n",
    "        metadata_lines = content[:data_start].split('\\n')\n",
    "        \n",
    "        # Extract data section\n",
    "        data_section = content[data_start:].split(':DATA')[1].strip()\n",
    "    \n",
    "    # Extract specific metadata\n",
    "    metadata = extract_metadata(metadata_lines)\n",
    "    \n",
    "    # Read the data section directly\n",
    "    try:\n",
    "        # Use StringIO to create a file-like object from the data section\n",
    "        raw_data = pd.read_csv(\n",
    "            io.StringIO(data_section), \n",
    "            sep='\\s+',  # Use whitespace as separator\n",
    "            header=None\n",
    "        )\n",
    "        raw_data = raw_data.T\n",
    "        # Add metadata columns to the DataFrame\n",
    "        if 'SW_FREQ' in metadata:\n",
    "            raw_data['Freq(kHz)'] = metadata['SW_FREQ']\n",
    "            # raw_data['Freq(Hz)'] = raw_data['Freq(Hz)'].apply(lambda x: x*1000)\n",
    "        \n",
    "        if 'LEVELS' in metadata:\n",
    "            # Repeat levels to match the number of rows\n",
    "            levels_repeated = metadata['LEVELS'] * (len(raw_data) // len(metadata['LEVELS']) + 1)\n",
    "            raw_data['Level(dB)'] = levels_repeated[:len(raw_data)]\n",
    "        \n",
    "        filtered_data = raw_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "        filtered_data.columns = filtered_data.columns.map(str)\n",
    "\n",
    "        columns = ['Freq(kHz)'] + ['Level(dB)'] + [col for col in filtered_data.columns if col.isnumeric() == True]\n",
    "        filtered_data = filtered_data[columns]\n",
    "        return filtered_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        return None, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "5d19998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks_troughs_amp_final(df, freq, db, time_scale=10, multiply_y_factor=1.0, units='Microvolts'):\n",
    "    db_column = 'Level(dB)'\n",
    "    \n",
    "    khz = df[(df['Freq(kHz)'] == freq) & (df[db_column] == db)]\n",
    "    if not khz.empty:\n",
    "        index = khz.index.values[0]\n",
    "        final = df.loc[index, '0':].dropna()\n",
    "        final = pd.to_numeric(final, errors='coerce').dropna()\n",
    "\n",
    "        target = int(244 * (time_scale / 10))\n",
    "        \n",
    "        # Process the wave as in calculate_and_plot_wave\n",
    "        y_values = interpolate_and_smooth(final, target)\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        y_values *= multiply_y_factor\n",
    "        \n",
    "        # Handle units conversion if needed\n",
    "        if units == 'Nanovolts':\n",
    "            y_values /= 1000\n",
    "            \n",
    "        # Generate normalized version for peak finding\n",
    "        y_values_fpf = interpolate_and_smooth(y_values[:244])\n",
    "        \n",
    "        # Standardize and normalize for peak finding, exactly as in the original\n",
    "        flattened_data = y_values_fpf.flatten().reshape(-1, 1)\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(flattened_data)\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = min_max_scaler.fit_transform(standardized_data).reshape(y_values_fpf.shape)\n",
    "        y_values_fpf = interpolate_and_smooth(scaled_data[:244])\n",
    "        \n",
    "        # Find peaks using the normalized data\n",
    "        highest_peaks, relevant_troughs = peak_finding(y_values_fpf)\n",
    "        \n",
    "        # Calculate amplitude on the processed but non-normalized data\n",
    "        if highest_peaks.size > 0 and relevant_troughs.size > 0:\n",
    "            # Following the same approach as in the display_metrics_table function\n",
    "            first_peak_amplitude = y_values[highest_peaks[0]] - y_values[relevant_troughs[0]]\n",
    "            return highest_peaks, y_values, first_peak_amplitude\n",
    "    \n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "dc38001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_interpolation(df, freq, db, time_scale=10, multiply_y_factor=1.0, units='Microvolts'):\n",
    "    \n",
    "    khz = df[(df['Freq(kHz)'] == freq) & (df['Level(dB)'] == db)]\n",
    "    # print(khz)\n",
    "    if not khz.empty:\n",
    "        index = khz.index.values[0]\n",
    "        final = df.loc[index, '0':].dropna()\n",
    "        final = pd.to_numeric(final, errors='coerce').dropna()\n",
    "\n",
    "        target = int(244 * (time_scale / 10))\n",
    "        \n",
    "        # Process the wave as in calculate_and_plot_wave\n",
    "        y_values = interpolate_and_smooth(final, target)\n",
    "\n",
    "        # print(f\"Interpolated y_values: {y_values[:5]}\")\n",
    "        # print(f\"Any NaNs? {np.isnan(y_values).any()}\")\n",
    "\n",
    "        if final.empty:\n",
    "            print(f\"Warning: Empty waveform for {freq}kHz @ {db}dB\")\n",
    "            return np.full((1, 244), np.nan)\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        y_values *= multiply_y_factor\n",
    "        \n",
    "        # Handle units conversion if needed\n",
    "        if units == 'Nanovolts':\n",
    "            y_values /= 1000\n",
    "            \n",
    "        # Generate normalized version for peak finding\n",
    "        y_values_fpf = interpolate_and_smooth(y_values[:244])\n",
    "        \n",
    "        # Standardize and normalize for peak finding, exactly as in the original\n",
    "        flattened_data = y_values_fpf.flatten().reshape(-1, 1)\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(flattened_data)\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = min_max_scaler.fit_transform(standardized_data).reshape(y_values_fpf.shape)\n",
    "    \n",
    "        return scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48a9ae",
   "metadata": {},
   "source": [
    "Latency detection model to parametrize functional model based on individual wave morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "1b048922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_all_peaks(highest_peaks, y_values, time_scale):\n",
    "    latencies = []\n",
    "    num_peaks = highest_peaks.size\n",
    "    if num_peaks > 0:  # Check if highest_peaks is not empty\n",
    "        for n in range(num_peaks): # SHOULD be 5 but there are cases where there are less. Will handle in later loops\n",
    "            lat = highest_peaks[n] * (time_scale / len(y_values)) # Based on ABRA logic\n",
    "            latencies.append(lat)\n",
    "        return latencies\n",
    "    else:\n",
    "        print(\"No peaks detected. Check input data\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373edae8",
   "metadata": {},
   "source": [
    "# 1. Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "6a4497b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WPZ62 45.2 [np.float64(3.3211845102505695), np.float64(5.207289293849659), np.float64(7.585421412300684), np.float64(9.225512528473804)]\n"
     ]
    }
   ],
   "source": [
    "# del int\n",
    "time_scale = 18\n",
    "amp_per_freq = {'Subject': [], 'Latencies' : [], 'Freq(kHz) (x1)': [], 'Level(dB) (x2)': [], 'Amplitude (x3)':[]}\n",
    "start_path = '/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/WPZ Electrophysiology'\n",
    "for subject in os.listdir(start_path):\n",
    "    # print(\"Subject:\",subject)\n",
    "    for fq in os.listdir(os.path.join(start_path,subject)):\n",
    "        # print(fq)\n",
    "        if fq.startswith('ABR') and fq.endswith('.tsv'):\n",
    "            path = os.path.join(start_path,subject,fq)\n",
    "            data_df = read_custom_tsv(path)\n",
    "            # print(data_df)\n",
    "            freqs = data_df['Freq(kHz)'].unique().tolist()\n",
    "            levels = data_df['Level(dB)'].unique().tolist()\n",
    "            for freq in freqs:\n",
    "                for lvl in levels:\n",
    "                    highest_peaks, y_values, amp = peaks_troughs_amp_final(df=data_df, freq=freq, db=lvl, time_scale=time_scale)\n",
    "                    latencies = latency_all_peaks(highest_peaks, y_values, time_scale)\n",
    "                    if len(latencies) < 5:\n",
    "                        print(subject, freq , latencies)\n",
    "                        continue\n",
    "                    amp_per_freq['Subject'].append(subject)\n",
    "                    amp_per_freq['Freq(kHz) (x1)'].append(freq)\n",
    "                    amp_per_freq['Level(dB) (x2)'].append(lvl)\n",
    "                    amp_per_freq['Amplitude (x3)'].append(amp)\n",
    "                    amp_per_freq['Latencies'].append(latencies)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "amp_df_full = pd.DataFrame(data=amp_per_freq)\n",
    "\n",
    "raw_synapse_counts = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Ribbon and Synapse Counts.xlsx')\n",
    "# raw_synapse_counts = raw_synapse_counts.mask(lambda x: x.isnull()).dropna() # old approach \n",
    "raw_synapse_counts = raw_synapse_counts.mask(lambda x: x.isna(),0)\n",
    "raw_synapse_counts['Synapses to IHC (y1)'] = raw_synapse_counts.iloc[:,6]\n",
    "raw_synapse_counts['vx (x4)'] = raw_synapse_counts['vx']\n",
    "raw_synapse_counts.drop(columns=['vx'], inplace=True)\n",
    "raw_synapse_counts.rename(columns={'Freq':'Freq(kHz) (x1)'}, inplace=True)\n",
    "raw_synapse_counts.rename(columns={'Case':'Subject'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "ce2120f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_synapse_counts[raw_synapse_counts['Subject'] == 'WPZ157']\n",
    "lens = [len(x) for x in amp_df_full['Latencies']]\n",
    "np.unique(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "7621c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 - values per vx\n",
    "\n",
    "paired = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(kHz) (x1)']), on=['Subject', 'Freq(kHz) (x1)'])\n",
    "# slice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs (y2)']]\n",
    "final = paired[['Subject', 'Latencies', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs']]\n",
    "final_clean = final.dropna()\n",
    "\n",
    "# adding in the strain feature\n",
    "strains = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Mouse groups.xlsx')\n",
    "final_clean_strained = final_clean.join(strains.set_index('ID#'), on='Subject')\n",
    "final_clean_strained['Strain'] = final_clean_strained['Strain'].str.strip()\n",
    "final_clean_strained = final_clean_strained.rename(columns={'Strain': 'Strain (x5)'})\n",
    "final_clean_strained = final_clean_strained.dropna()\n",
    "final_clean_strained = final_clean_strained[['Subject', 'Latencies', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Synapses to IHC (y1)', 'Group']]\n",
    "\n",
    "final_clean_strained_grouped = final_clean_strained.copy()\n",
    "final_clean_strained_grouped['Group - dB'] = final_clean_strained_grouped['Group'].apply(lambda x: x.split(' ')[0] if x.split(' ')[0].endswith('dB') else 'Control')\n",
    "final_clean_strained_grouped['Group - Time Elapsed'] = final_clean_strained_grouped['Group'].apply(lambda x: x.split(' ')[1] if x.split(' ')[1].endswith(('h', 'wks', 'w')) else x.split(' ')[0])\n",
    "final_clean_strained_grouped.head()\n",
    "\n",
    "final_clean_strained_grouped_pos = final_clean_strained_grouped.copy()\n",
    "final_clean_strained_grouped_pos['Amplitude (x3)'] = final_clean_strained_grouped['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup = final_clean_strained_grouped_pos.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup['Group'] = final_clean_strained_grouped_pos_cleangroup['Group'].apply(lambda x: x.strip())\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup.head()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs = final_clean_strained_grouped_pos_cleangroup.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - dB']\n",
    "# final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed']\n",
    "final_clean_strained_grouped_pos_cleangroup_vs = final_clean_strained_grouped_pos_cleangroup_vs[['Subject','Latencies', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)',\n",
    "       'vx (x4)', 'Strain (x5)','Group - dB (x6)', 'Group - Time Elapsed', 'Group','Synapses to IHC (y1)']]\n",
    "\n",
    "def split_on_number(input_string):\n",
    "    return re.findall(r\"[A-Za-z]+|\\d+\", input_string)\n",
    "\n",
    "hrs_week = 24*7\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed = final_clean_strained_grouped_pos_cleangroup_vs.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: '0dB' if x == 'Control' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'].apply(lambda x: x[1])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'].apply(lambda x: \"wks\" if x == 'w' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Hours Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed.apply(lambda row: row['Group - Time Elapsed - Magn.']* hrs_week if row['Group - Time Elapsed - Unit'] == 'wks' else row['Group - Time Elapsed - Magn.'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "3a58a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 - Averaged per Vx\n",
    "\n",
    "paired2 = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(kHz) (x1)']), on=['Subject', 'Freq(kHz) (x1)'])\n",
    "# lilslice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs']]\n",
    "final2 = paired2[['Subject', 'Latencies', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'Synapses', 'IHCs']]\n",
    "final_clean2 = final2.dropna()\n",
    "\n",
    "# adding in the strain feature\n",
    "strains = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Mouse groups.xlsx')\n",
    "final_clean_strained2 = final_clean2.join(strains.set_index('ID#'), on='Subject')\n",
    "final_clean_strained2['Strain'] = final_clean_strained2['Strain'].str.strip()\n",
    "final_clean_strained2 = final_clean_strained2.rename(columns={'Strain': 'Strain (x5)'})\n",
    "final_clean_strained2 = final_clean_strained2.dropna()\n",
    "final_clean_strained2 = final_clean_strained2[['Subject', 'Latencies', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Synapses to IHC (y1)', 'Group', 'Synapses', 'IHCs']]\n",
    "# np.unique(final_clean_strained2['Group'])\n",
    "\n",
    "# final_clean_70 = final_clean[final_clean['Level(dB) (x2)'] >= 70.0]\n",
    "# final_clean_strained_70 = final_clean_strained[final_clean_strained['Level(dB) (x2)'] >= 70.0]\n",
    "# # np.unique(final_clean['Level(dB) (x2)']) max level is 80 db\n",
    "# len(final_clean), len(final_clean_70) # 10000 less data points!!!\n",
    "\n",
    "final_clean_strained_grouped2 = final_clean_strained2.copy()\n",
    "final_clean_strained_grouped2['Group - dB'] = final_clean_strained_grouped2['Group'].apply(lambda x: x.split(' ')[0] if x.split(' ')[0].endswith('dB') else 'Control')\n",
    "final_clean_strained_grouped2['Group - Time Elapsed'] = final_clean_strained_grouped2['Group'].apply(lambda x: x.split(' ')[1] if x.split(' ')[1].endswith(('h', 'wks', 'w')) else x.split(' ')[0])\n",
    "final_clean_strained_grouped2.head()\n",
    "\n",
    "final_clean_strained_grouped_pos2 = final_clean_strained_grouped2.copy()\n",
    "final_clean_strained_grouped_pos2['Amplitude (x3)'] = final_clean_strained_grouped2['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "len(final_clean_strained_grouped_pos2[final_clean_strained_grouped_pos2['Amplitude (x3)'] < 0])\n",
    "\n",
    "final_clean_strained_grouped_pos2['Amplitude (x3)'] = final_clean_strained_grouped2['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "# final_clean_strained_grouped_pos[(final_clean_strained_grouped_pos['Subject'] == 'WPZ66') & (final_clean_strained_grouped_pos['Amplitude (x3)'] ==0.055901451434921576)\n",
    "final_clean_strained_grouped_pos_cleangroup2 = final_clean_strained_grouped_pos2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup2['Group'] = final_clean_strained_grouped_pos_cleangroup2['Group'].apply(lambda x: x.strip())\n",
    "np.unique(final_clean_strained_grouped_pos_cleangroup2['Group'])\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup2.head()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2 = final_clean_strained_grouped_pos_cleangroup2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs2['Group - dB']\n",
    "# final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed']\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2 = final_clean_strained_grouped_pos_cleangroup_vs2[['Subject',  'Latencies', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)',\n",
    "       'vx (x4)', 'Strain (x5)','Group - dB (x6)', 'Group - Time Elapsed', 'Group','Synapses to IHC (y1)', 'Synapses', 'IHCs']]\n",
    "\n",
    "def split_on_number(input_string):\n",
    "    return re.findall(r\"[A-Za-z]+|\\d+\", input_string)\n",
    "\n",
    "hrs_week = 24*7\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2 = final_clean_strained_grouped_pos_cleangroup_vs2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: '0dB' if x == 'Control' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'].apply(lambda x: x[1])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'].apply(lambda x: \"wks\" if x == 'w' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Hours Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2.apply(lambda row: row['Group - Time Elapsed - Magn.']* hrs_week if row['Group - Time Elapsed - Unit'] == 'wks' else row['Group - Time Elapsed - Magn.'], axis = 1)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2\n",
    "\n",
    "freqs = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2['Freq(kHz) (x1)'])\n",
    "subs = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2['Subject'])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx = final_clean_strained_grouped_pos_cleangroup_vs_timed2.copy()\n",
    "for freq in freqs:\n",
    "    for sub in subs:\n",
    "        mask = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)] # global for updates\n",
    "        if len(mask) > 0:\n",
    "\n",
    "            mask1 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['vx (x4)'] == 'v1')]\n",
    "            mask2 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['vx (x4)'] == 'v2')]\n",
    "\n",
    "            if not mask1.empty and not mask2.empty:\n",
    "                mask1 = mask1.reset_index().iloc[0,:]\n",
    "                mask2 = mask2.reset_index().iloc[0,:]\n",
    "\n",
    "                total_syns = float(mask1['Synapses'] + mask2['Synapses'])\n",
    "                total_ihcs = float(mask1['IHCs'] + mask2['IHCs'])\n",
    "\n",
    "                if total_ihcs != 0:\n",
    "                    ratio = total_syns / total_ihcs\n",
    "\n",
    "                # if total_ihcs == 0 or total_syns == 0:\n",
    "                #     print(sub, freq, total_syns, total_ihcs)\n",
    "                # if total_syns == 0.0 or total_ihcs == 0.0:\n",
    "                #     print(sub, freq)\n",
    "                mask_index = mask.index\n",
    "                final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[mask_index,'Synapse to IHC Ratio per Freq (y2)'] = ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45ab6f",
   "metadata": {},
   "source": [
    "# 2. Waves as Inputs (FLM) - no additional predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf123df",
   "metadata": {},
   "source": [
    "## a) Extract data as waves from ABR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "15b62ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping subject WPZ144, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ161, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ156, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ146, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ178, frequency 11.3 due to mismatch.\n",
      "Skipping subject WPZ40, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ98, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ155, frequency 8.0 due to mismatch.\n"
     ]
    }
   ],
   "source": [
    "time_scale = 18\n",
    "start_path = '/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/WPZ Electrophysiology'\n",
    "subject_ABRs = {}\n",
    "\n",
    "for subject in os.listdir(start_path):\n",
    "    for fq in os.listdir(os.path.join(start_path,subject)):\n",
    "        if fq.startswith('ABR') and fq.endswith('.tsv'):\n",
    "            match = re.search(r'-L-([\\d.]+)\\.tsv$', fq)\n",
    "            if match:\n",
    "                freq = float(match.group(1))\n",
    "                # if freq == 6.0 or freq == 7.0:\n",
    "                #     print(subject, freq)\n",
    "                # freqs.add(freq)\n",
    "                path = os.path.join(start_path,subject,fq)\n",
    "                data_df = read_custom_tsv(path)\n",
    "                if data_df['Freq(kHz)'].iloc[0] == freq:\n",
    "                    subject_ABRs[(subject, freq)] = data_df\n",
    "                else:\n",
    "                    print(f\"Skipping subject {subject}, frequency {freq} due to mismatch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad796ba",
   "metadata": {},
   "source": [
    "## b) Transform using skfda.DataGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "0337324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_scale = 18\n",
    "subjects = []\n",
    "frequencies = []\n",
    "levels = []\n",
    "waves = []\n",
    "ys = []\n",
    "fails = []\n",
    "\n",
    "for (sub, freq), df in subject_ABRs.items():\n",
    "    for lvl in np.unique(subject_ABRs[(sub, freq)]['Level(dB)']):\n",
    "        lvl = float(lvl)\n",
    "        wave = full_interpolation(df, freq, lvl, time_scale)\n",
    "        wave = np.asarray(wave, dtype=float)\n",
    "        # if wave.size != 244:\n",
    "        #     fails.append((sub, freq, lvl))\n",
    "        # else:\n",
    "        wave_input = skfda.FDataGrid(\n",
    "            data_matrix=wave,\n",
    "            grid_points=time_scale * np.arange(0, 244) / 244\n",
    "        )\n",
    "        y = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Level(dB) (x2)'] == lvl)]\\\n",
    "            ['Synapse to IHC Ratio per Freq (y2)']\n",
    "        \n",
    "        if len(y) > 0:\n",
    "            y = float(y.iloc[0])\n",
    "\n",
    "        # print((sub, freq, lvl, y))\n",
    "        subjects.append(sub)\n",
    "        frequencies.append(freq)\n",
    "        levels.append(lvl)\n",
    "        waves.append(wave_input)\n",
    "        ys.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "d3dec48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_subs = set()\n",
    "for fail in fails:\n",
    "    # print(fail[0])\n",
    "    fail_subs.add(fail[1])\n",
    "# fail_subs\n",
    "len(fails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d973b",
   "metadata": {},
   "source": [
    "## c) Create and fit data to new bases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05bacc",
   "metadata": {},
   "source": [
    "Creating a new basis is key for a functional regression. It brings the various waveforms into lower dimensional space by isolating the signal and filtering out noise. Basis-spline or B-spline is particularly useful for ABR waveforms:\n",
    "- Piecewise function = can control amount of smoothing and coefficient weighting *locally* (great for analyzing distinct peaks/troughs)\n",
    "    - We should use 5 knots or \"pieces\", 1 knot for each wave. \n",
    "- Does not assume periodicity like Fourier basis, giving a more natural fit\n",
    "- Cubic spline is standard for waveform data to capture wave variability without overly smoothing\n",
    "    - zB: 5 knots and 3 degrees (cubic splining) -> 7 basis pieces (5 + 3 -1) AKA we are splitting each waveform into 7 independent blocks\n",
    "\n",
    "https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-6/Functional-linear-regression-analysis-for-longitudinal-data/10.1214/009053605000000660.full\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/B-spline\n",
    "\n",
    "\n",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC5598560/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fad54b",
   "metadata": {},
   "source": [
    "$f(t) = c_1 \\, \\phi_1(t) + c_2 \\, \\phi_2(t) + c_3 \\, \\phi_3(t) + c_4 \\, \\phi_4(t) + c_5 \\, \\phi_5(t) + c_6 \\, \\phi_6(t) + c_7 \\, \\phi_7(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "81e90532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update timepoints post discussion/research!!!!!\n",
    "\n",
    "# wave_timepoints = []\n",
    "# basis = BSplineBasis(domain_range=(0,244), knots=wave_timepoints, order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "0d62d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda.misc.regularization import L2Regularization\n",
    "from skfda.misc.operators import LinearDifferentialOperator\n",
    "\n",
    "penalty = L2Regularization(linear_operator=LinearDifferentialOperator(2))\n",
    "shared_grid = np.linspace(0, 18, 244)\n",
    "basis = BSplineBasis(n_basis=7, domain_range=(0,18))\n",
    "smoother = BasisSmoother(basis=basis, return_basis=True) # , regularization=penalty, smoothing_parameter=0.1, \n",
    "time_scale = 18\n",
    "subjects = []\n",
    "frequencies = []\n",
    "levels = []\n",
    "raw_waves = []\n",
    "newbasis_waves = []\n",
    "Xs = []\n",
    "ys = []\n",
    "fails = []\n",
    "\n",
    "for (sub, freq), df in subject_ABRs.items():\n",
    "    for lvl in np.unique(subject_ABRs[(sub, freq)]['Level(dB)']):\n",
    "        lvl = float(lvl)\n",
    "        wave = full_interpolation(df, freq, lvl, time_scale)\n",
    "        wave = np.asarray(wave, dtype=float)\n",
    "        wave = wave.reshape(1, -1)\n",
    "        \n",
    "        grid = time_scale * np.arange(0, 244) / 244\n",
    "        wave_input = skfda.FDataGrid(data_matrix=wave,grid_points=shared_grid)\n",
    "\n",
    "        wave_newbasis = smoother.fit_transform(wave_input)[0] # smoother will allow regularization for further tuning down the line...\n",
    "\n",
    "        X = wave_newbasis.coefficients\n",
    "\n",
    "        y_series = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Level(dB) (x2)'] == lvl)]\\\n",
    "            ['Synapse to IHC Ratio per Freq (y2)']\n",
    "        \n",
    "        # print(y_series)\n",
    "        \n",
    "        if len(y_series) == 0 or pd.isna(y_series.iloc[0]):\n",
    "            # print(f'N/A y: ({sub}, {freq}, {lvl})')\n",
    "            continue\n",
    "\n",
    "        y = float(y_series.iloc[0])\n",
    "\n",
    "        # print((sub, freq, lvl, y))\n",
    "        subjects.append(sub)\n",
    "        frequencies.append(freq)\n",
    "        levels.append(lvl)\n",
    "        raw_waves.append(wave_input)\n",
    "        newbasis_waves.append(wave_newbasis)\n",
    "        Xs.append(X.flatten()) # used for model fitting, same as for OLS!\n",
    "        ys.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "c53e85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5952 entries, 0 to 5951\n",
      "Data columns (total 5 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   Subject                             5952 non-null   object \n",
      " 1   Freq(kHz)                           5952 non-null   float64\n",
      " 2   Level(dB)                           5952 non-null   float64\n",
      " 3   Transformed Waves (X)               5952 non-null   object \n",
      " 4   Synapse to IHC Ratio per Freq (y2)  5952 non-null   float64\n",
      "dtypes: float64(3), object(2)\n",
      "memory usage: 232.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5952"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_waves_df = pd.DataFrame(data = {'Subject' : subjects, 'Freq(kHz)' : frequencies, 'Level(dB)' : levels, 'Transformed Waves (X)' : Xs, 'Synapse to IHC Ratio per Freq (y2)' : ys})\n",
    "final_waves_df_clean = final_waves_df.dropna().reset_index(drop=True)\n",
    "final_waves_df_clean.info()\n",
    "len(final_waves_df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c1f21",
   "metadata": {},
   "source": [
    "## d) Initial Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "53e3e074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.8984591728881317), np.float64(2.925489395711595))"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = final_waves_df_clean['Transformed Waves (X)']\n",
    "X_new = np.vstack([x.flatten() for x in X])\n",
    "\n",
    "y = final_waves_df_clean['Synapse to IHC Ratio per Freq (y2)']\n",
    "\n",
    "kf = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_rmse, test_rmse = [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X=X_new,y=y,groups=final_waves_df_clean['Subject']):\n",
    "    X_train, X_test = X_new[train_idx], X_new[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_hat = model.predict(X_train)\n",
    "    y_test_hat = model.predict(X_test)\n",
    "\n",
    "    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_hat)))\n",
    "    test_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_hat)))\n",
    "\n",
    "np.mean(train_rmse), np.mean(test_rmse)\n",
    "# print(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84791c1",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "- Add regularization\n",
    "- Check why so many N/A\n",
    "- Fix the knots to fit to 5 waves\n",
    "    - If bad, experiment with different settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb89d12",
   "metadata": {},
   "source": [
    "# 3. Add Frequency as Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3551c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.8610348719204337), np.float64(2.904426822640569))"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = final_waves_df_clean['Transformed Waves (X)']\n",
    "X_func = np.vstack([x.flatten() for x in X])\n",
    "X_nonfunc = pd.DataFrame(final_waves_df_clean['Freq(kHz)'])\n",
    "\n",
    "y = final_waves_df_clean['Synapse to IHC Ratio per Freq (y2)']\n",
    "\n",
    "kf = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_rmse, test_rmse = [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X=X_nonfunc ,y=y,groups=final_waves_df_clean['Subject']):\n",
    "    X_train_func, X_test_func = X_func[train_idx], X_func[test_idx]\n",
    "    X_train_nonfunc, X_test_nonfunc = X_nonfunc.iloc[train_idx], X_nonfunc.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit([X_train_func,X_train_nonfunc], y_train)\n",
    "\n",
    "    y_train_hat = model.predict([X_train_func,X_train_nonfunc])\n",
    "    y_test_hat = model.predict([X_test_func,X_test_nonfunc])\n",
    "\n",
    "    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_hat)))\n",
    "    test_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_hat)))\n",
    "\n",
    "np.mean(train_rmse), np.mean(test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092706c6",
   "metadata": {},
   "source": [
    "# 4. Tuning # of knots. Individualize this based on the peaks detected by ABRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c362fd",
   "metadata": {},
   "source": [
    "Now that we have the waves segmented via latency, we can use this to parametrize knots per wave and hopefully improve our predictions!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28776f83",
   "metadata": {},
   "source": [
    "## a) Extract data as waves from ABR (filtering out where freq filename != freq in-file AND if N/A in synapse file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "f0df5556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping subject WPZ144, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ161, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ156, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ146, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ178, frequency 11.3 due to mismatch.\n",
      "Skipping subject WPZ98, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ155, frequency 8.0 due to mismatch.\n"
     ]
    }
   ],
   "source": [
    "time_scale = 18\n",
    "start_path = '/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/WPZ Electrophysiology'\n",
    "subject_ABRs = {}\n",
    "\n",
    "for subject in os.listdir(start_path):\n",
    "    if subject in raw_synapse_counts['Subject'].values: # excluding subjects not in synapse count file\n",
    "        for fq in os.listdir(os.path.join(start_path,subject)):\n",
    "            if fq.startswith('ABR') and fq.endswith('.tsv'):\n",
    "                match = re.search(r'-L-([\\d.]+)\\.tsv$', fq)\n",
    "                if match:\n",
    "                    freq = float(match.group(1))\n",
    "                    if freq in raw_synapse_counts[raw_synapse_counts['Subject'] == subject]['Freq(kHz) (x1)'].values:\n",
    "                        # if freq == 6.0 or freq == 7.0:\n",
    "                        #     print(subject, freq)\n",
    "                        # freqs.add(freq)\n",
    "                        path = os.path.join(start_path,subject,fq)\n",
    "                        data_df = read_custom_tsv(path)\n",
    "                        if data_df['Freq(kHz)'].iloc[0] == freq:\n",
    "                            subject_ABRs[(subject, freq)] = data_df\n",
    "                        else:\n",
    "                            print(f\"Skipping subject {subject}, frequency {freq} due to mismatch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0cb9c",
   "metadata": {},
   "source": [
    "## b) Transform using skfda.DataGrid and set new basis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4db428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda.misc.regularization import L2Regularization\n",
    "from skfda.misc.operators import LinearDifferentialOperator\n",
    "\n",
    "penalty = L2Regularization(linear_operator=LinearDifferentialOperator(2))\n",
    "shared_grid = np.linspace(0, 18, 244)\n",
    "# basis = BSplineBasis(n_basis=7, domain_range=(0,18)) # old\n",
    "# smoother = BasisSmoother(basis=basis, return_basis=True) # , regularization=penalty, smoothing_parameter=0.1, \n",
    "time_scale = 18\n",
    "subjects = []\n",
    "frequencies = []\n",
    "levels = []\n",
    "raw_waves = []\n",
    "newbasis_waves = []\n",
    "Xs = []\n",
    "ys = []\n",
    "fails = []\n",
    "\n",
    "for (sub, freq), df in subject_ABRs.items():\n",
    "    for lvl in np.unique(df['Level(dB)']):\n",
    "        latencies_series = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq)\\\n",
    "                & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Level(dB) (x2)'] == lvl)]\\\n",
    "            ['Latencies']\n",
    "        \n",
    "        if len(latencies_series) == 0:\n",
    "            print(f'N/A latencies: ({sub}, {freq}, {lvl}) : {latencies_series}')\n",
    "            continue\n",
    "\n",
    "        latencies = latencies_series.values[0]\n",
    "        latencies = [float(x) for x in latencies]\n",
    "        # print(latencies)\n",
    "\n",
    "        lvl = float(lvl)\n",
    "        wave = full_interpolation(df, freq, lvl, time_scale)\n",
    "        wave = np.asarray(wave, dtype=float)\n",
    "        wave = wave.reshape(1, -1)\n",
    "        \n",
    "        grid = time_scale * np.arange(0, 244) / 244\n",
    "        wave_input = skfda.FDataGrid(data_matrix=wave,grid_points=shared_grid)\n",
    "\n",
    "        basis = BSplineBasis(domain_range=(latencies[0], latencies[-1]),knots = latencies, order = 3)\n",
    "        smoother = BasisSmoother(basis=basis, return_basis=True)\n",
    "\n",
    "        wave_newbasis = smoother.fit_transform(wave_input)[0] # smoother will allow regularization for further tuning down the line...\n",
    "\n",
    "        X = wave_newbasis.coefficients\n",
    "\n",
    "        y_series = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Level(dB) (x2)'] == lvl)]\\\n",
    "            ['Synapse to IHC Ratio per Freq (y2)']\n",
    "        \n",
    "        # print(y_series)\n",
    "\n",
    "        if len(y_series) == 0 or pd.isna(y_series.iloc[0]):\n",
    "            print(f'N/A y: ({sub}, {freq}, {lvl})')\n",
    "            continue\n",
    "\n",
    "        y = float(y_series.iloc[0])\n",
    "\n",
    "        # print((sub, freq, lvl, y))\n",
    "        subjects.append(sub)\n",
    "        frequencies.append(freq)\n",
    "        levels.append(lvl)\n",
    "        raw_waves.append(wave_input)\n",
    "        newbasis_waves.append(wave_newbasis)\n",
    "        Xs.append(X.flatten()) # used for model fitting, same as for OLS!\n",
    "        ys.append(y)\n",
    "\n",
    "final_waves_df_new_basis = pd.DataFrame(data = {'Subject' : subjects, 'Freq(kHz)' : frequencies, 'Level(dB)' : levels, 'Transformed Waves (X)' : Xs, 'Synapse to IHC Ratio per Freq (y2)' : ys})\n",
    "final_waves_df_new_basis_clean = final_waves_df_new_basis.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec89f37",
   "metadata": {},
   "source": [
    "## c) TT-Split with only individualized basis waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "8b67dbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.9085465449348047), np.float64(2.919585490404064))"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = final_waves_df_new_basis_clean['Transformed Waves (X)']\n",
    "X_relevant_waves = [x[1:] for x in X]\n",
    "X_new = np.vstack([x.flatten() for x in X_relevant_waves])\n",
    "\n",
    "y = final_waves_df_new_basis_clean['Synapse to IHC Ratio per Freq (y2)']\n",
    "\n",
    "kf = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_rmse, test_rmse = [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X=X_new,y=y,groups=final_waves_df_new_basis_clean['Subject']):\n",
    "    X_train, X_test = X_new[train_idx], X_new[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_hat = model.predict(X_train)\n",
    "    y_test_hat = model.predict(X_test)\n",
    "\n",
    "    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_hat)))\n",
    "    test_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_hat)))\n",
    "\n",
    "np.mean(train_rmse), np.mean(test_rmse)\n",
    "# print(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19edf1d3",
   "metadata": {},
   "source": [
    "## d) Parametrized waves + frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "b3e91dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.868142699267886), np.float64(2.899335287819347))"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = final_waves_df_new_basis_clean['Transformed Waves (X)']\n",
    "X_relevant_waves = [x[:5] for x in X]\n",
    "X_func = np.vstack([x.flatten() for x in X_relevant_waves])\n",
    "X_nonfunc = pd.DataFrame(final_waves_df_new_basis_clean['Freq(kHz)'])\n",
    "\n",
    "y = final_waves_df_new_basis_clean['Synapse to IHC Ratio per Freq (y2)']\n",
    "\n",
    "kf = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_rmse, test_rmse = [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X=X_nonfunc ,y=y,groups=final_waves_df_new_basis_clean['Subject']):\n",
    "    X_train_func, X_test_func = X_func[train_idx], X_func[test_idx]\n",
    "    X_train_nonfunc, X_test_nonfunc = X_nonfunc.iloc[train_idx], X_nonfunc.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_nonfunc_scaled = scaler.fit_transform(X_train_nonfunc)\n",
    "    X_test_nonfunc_scaled = scaler.transform(X_test_nonfunc)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit([X_train_func,X_train_nonfunc_scaled], y_train)\n",
    "\n",
    "    y_train_hat = model.predict([X_train_func,X_train_nonfunc_scaled])\n",
    "    y_test_hat = model.predict([X_test_func,X_test_nonfunc_scaled])\n",
    "\n",
    "    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_hat)))\n",
    "    test_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_hat)))\n",
    "\n",
    "np.mean(train_rmse), np.mean(test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de111431",
   "metadata": {},
   "source": [
    "# 5. Finetuning Process (based on 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "fce7b808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 1.0000e+02 with RMSE 2.9009\n"
     ]
    }
   ],
   "source": [
    "from skfda.misc.regularization import L2Regularization\n",
    "from skfda.misc.operators import LinearDifferentialOperator\n",
    "\n",
    "shared_grid = np.linspace(0, 18, 244)\n",
    "penalty = L2Regularization(linear_operator=LinearDifferentialOperator(2))\n",
    "time_scale = 18\n",
    "\n",
    "# tuning penalty_amount and smoother!!!!! Set up the for loop\n",
    "for alpha in np.logspace(-4,2,10):\n",
    "    subjects = []\n",
    "    frequencies = []\n",
    "    levels = []\n",
    "    raw_waves = []\n",
    "    newbasis_waves = []\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    fails = []\n",
    "\n",
    "    for (sub, freq), df in subject_ABRs.items():\n",
    "        for lvl in np.unique(df['Level(dB)']):\n",
    "            latencies_series = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)\\\n",
    "                & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq)\\\n",
    "                    & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Level(dB) (x2)'] == lvl)]\\\n",
    "                ['Latencies']\n",
    "            \n",
    "            if len(latencies_series) == 0:\n",
    "                print(f'N/A latencies: ({sub}, {freq}, {lvl}) : {latencies_series}')\n",
    "                continue\n",
    "\n",
    "            latencies = latencies_series.values[0]\n",
    "            latencies = [float(x) for x in latencies]\n",
    "            # print(latencies)\n",
    "\n",
    "            lvl = float(lvl)\n",
    "            wave = full_interpolation(df, freq, lvl, time_scale)\n",
    "            wave = np.asarray(wave, dtype=float)\n",
    "            wave = wave.reshape(1, -1)\n",
    "            \n",
    "            grid = time_scale * np.arange(0, 244) / 244\n",
    "            wave_input = skfda.FDataGrid(data_matrix=wave,grid_points=shared_grid)\n",
    "\n",
    "            basis = BSplineBasis(domain_range=(latencies[0], latencies[-1]),knots = latencies, order = 3)\n",
    "            smoother = BasisSmoother(basis=basis, return_basis=True, regularization=penalty, smoothing_parameter=alpha)\n",
    "\n",
    "            wave_newbasis = smoother.fit_transform(wave_input)[0] # smoother will allow regularization for further tuning down the line...\n",
    "\n",
    "            X = wave_newbasis.coefficients\n",
    "\n",
    "            y_series = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)\\\n",
    "                & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq)\\\n",
    "                & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Level(dB) (x2)'] == lvl)]\\\n",
    "                ['Synapse to IHC Ratio per Freq (y2)']\n",
    "            \n",
    "            # print(y_series)\n",
    "\n",
    "            if len(y_series) == 0 or pd.isna(y_series.iloc[0]):\n",
    "                print(f'N/A y: ({sub}, {freq}, {lvl})')\n",
    "                continue\n",
    "\n",
    "            y = float(y_series.iloc[0])\n",
    "\n",
    "            # print((sub, freq, lvl, y))\n",
    "            subjects.append(sub)\n",
    "            frequencies.append(freq)\n",
    "            levels.append(lvl)\n",
    "            raw_waves.append(wave_input)\n",
    "            newbasis_waves.append(wave_newbasis)\n",
    "            Xs.append(X.flatten()) # used for model fitting, same as for OLS!\n",
    "            ys.append(y)\n",
    "            \n",
    "    final_waves_df_new_basis = pd.DataFrame(data = {'Subject' : subjects, 'Freq(kHz)' : frequencies, 'Level(dB)' : levels, 'Transformed Waves (X)' : Xs, 'Synapse to IHC Ratio per Freq (y2)' : ys})\n",
    "    final_waves_df_new_basis_clean = final_waves_df_new_basis.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Start CV!!\n",
    "    X = final_waves_df_new_basis_clean['Transformed Waves (X)']\n",
    "    X_relevant_waves = [x for x in X]\n",
    "    X_func = np.vstack([x.flatten() for x in X_relevant_waves])\n",
    "    X_nonfunc = pd.DataFrame(final_waves_df_new_basis_clean['Freq(kHz)'])\n",
    "\n",
    "    y = final_waves_df_new_basis_clean['Synapse to IHC Ratio per Freq (y2)']\n",
    "\n",
    "    kf = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    best_alpha = None\n",
    "    best_score = np.inf\n",
    "\n",
    "    train_rmse, test_rmse = [], []  \n",
    "\n",
    "    # for train_idx, test_idx in kf.split(X=X_nonfunc ,y=y,groups=final_waves_df_new_basis_clean['Subject']):\n",
    "    for train_idx, test_idx in kf.split(X=X_func ,y=y,groups=final_waves_df_new_basis_clean['Subject']):\n",
    "        X_train_func, X_test_func = X_func[train_idx], X_func[test_idx]\n",
    "        X_train_nonfunc, X_test_nonfunc = X_nonfunc.iloc[train_idx], X_nonfunc.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_nonfunc_scaled = scaler.fit_transform(X_train_nonfunc)\n",
    "        X_test_nonfunc_scaled = scaler.transform(X_test_nonfunc)\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit([X_train_func,X_train_nonfunc_scaled], y_train)\n",
    "\n",
    "        y_train_hat = model.predict([X_train_func,X_train_nonfunc_scaled])\n",
    "        y_test_hat = model.predict([X_test_func,X_test_nonfunc_scaled])\n",
    "\n",
    "        train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_hat)))\n",
    "        test_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_hat)))\n",
    "\n",
    "    avg_train_rmse = np.mean(train_rmse)\n",
    "    avg_test_rmse = np.mean(test_rmse)\n",
    "\n",
    "    if avg_test_rmse < best_score:\n",
    "        best_score = avg_test_rmse\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(f\"Best alpha: {best_alpha:.4e} with RMSE {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc560fe",
   "metadata": {},
   "source": [
    "# 6. Adding strain as a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c907f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
