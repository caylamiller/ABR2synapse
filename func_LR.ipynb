{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508e66ad",
   "metadata": {},
   "source": [
    "# 0. Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4c615134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.distributed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ABRA_35 import interpolate_and_smooth, CNN, plot_wave, calculate_and_plot_wave, plot_waves_single_frequency, arfread, get_str, calculate_hearing_threshold, all_thresholds, peak_finding\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import os\n",
    "import struct\n",
    "import datetime\n",
    "# from skfda import FDataGrid\n",
    "# from skfda.preprocessing.dim_reduction import FPCA\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import torch.nn as nn\n",
    "import splitfolders\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.signal import find_peaks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np4\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "import skfda\n",
    "from skfda.ml.regression import LinearRegression\n",
    "from skfda.representation.basis import BSplineBasis, FourierBasis, FDataBasis\n",
    "from skfda.representation import FDataGrid\n",
    "from skfda.preprocessing.smoothing import BasisSmoother\n",
    "# from skfda.regression.linear_model import LinearFunctionalRegressor is no longer available? :(\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "69259896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(1, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (fc1): Linear(in_features=1952, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (dropout_fc): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter1 = 128\n",
    "filter2 = 32\n",
    "dropout1 = 0.5\n",
    "dropout2 = 0.3\n",
    "dropout_fc = 0.1\n",
    "\n",
    "# Model initialization\n",
    "peak_finding_model = CNN(filter1, filter2, dropout1, dropout2, dropout_fc)\n",
    "model_loader = torch.load('./models/waveI_cnn.pth')\n",
    "peak_finding_model.load_state_dict(model_loader)\n",
    "peak_finding_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "321670b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finding(wave):\n",
    "    # Prepare waveform\n",
    "    waveform=interpolate_and_smooth(wave) # Added indexing per calculate and plot wave function\n",
    "    # waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0) archived ABRA\n",
    "    waveform_torch = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0).unsqueeze(0) #newer ABRA\n",
    "    # print(waveform_torch)\n",
    "    # Get prediction from model\n",
    "    outputs = peak_finding_model(waveform_torch)\n",
    "    prediction = int(round(outputs.detach().numpy()[0][0], 0))\n",
    "    # prediction_test = int(round(outputs.detach().numpy()[0], 0))\n",
    "    # print(\"Model output:\", outputs, \"Prediction true start:\", prediction)\n",
    "\n",
    "    # Apply Gaussian smoothing\n",
    "    smoothed_waveform = gaussian_filter1d(waveform, sigma=1)\n",
    "\n",
    "    # Find peaks and troughs\n",
    "    n = 18\n",
    "    t = 14\n",
    "    # start_point = prediction - 9 archived ABRA\n",
    "    start_point = prediction - 6 #newer ABRA\n",
    "    smoothed_peaks, _ = find_peaks(smoothed_waveform[start_point:], distance=n)\n",
    "    smoothed_troughs, _ = find_peaks(-smoothed_waveform, distance=t)\n",
    "    sorted_indices = np.argsort(smoothed_waveform[smoothed_peaks+start_point])\n",
    "    highest_smoothed_peaks = np.sort(smoothed_peaks[sorted_indices[-5:]] + start_point)\n",
    "    relevant_troughs = np.array([])\n",
    "    for p in range(len(highest_smoothed_peaks)):\n",
    "        c = 0\n",
    "        for t in smoothed_troughs:\n",
    "            if t > highest_smoothed_peaks[p]:\n",
    "                if p != 4:\n",
    "                    try:\n",
    "                        if t < highest_smoothed_peaks[p+1]:\n",
    "                            relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                else:\n",
    "                    relevant_troughs = np.append(relevant_troughs, int(t))\n",
    "                    break\n",
    "    relevant_troughs = relevant_troughs.astype('i')\n",
    "    return highest_smoothed_peaks, relevant_troughs\n",
    "\n",
    "def extract_metadata(metadata_lines):\n",
    "    # Dictionary to store extracted metadata\n",
    "    metadata = {}\n",
    "    \n",
    "    for line in metadata_lines:\n",
    "        # Extract SW FREQ\n",
    "        freq_match = re.search(r'SW FREQ:\\s*(\\d+\\.?\\d*)', line)\n",
    "        if freq_match:\n",
    "            metadata['SW_FREQ'] = float(freq_match.group(1))\n",
    "        \n",
    "        # Extract LEVELS\n",
    "        levels_match = re.search(r':LEVELS:\\s*([^:]+)', line)\n",
    "        if levels_match:\n",
    "            # Split levels and convert to list of floats\n",
    "            metadata['LEVELS'] = [float(level) for level in levels_match.group(1).split(';') if level]\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def read_custom_tsv(file_path):\n",
    "    # Read the entire file\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split the content into metadata and data sections\n",
    "    metadata_lines = []\n",
    "    data_section = None\n",
    "    \n",
    "    # Find the ':DATA' marker\n",
    "    data_start = content.find(':DATA')\n",
    "    \n",
    "    if data_start != -1:\n",
    "        # Extract metadata (lines before ':DATA')\n",
    "        metadata_lines = content[:data_start].split('\\n')\n",
    "        \n",
    "        # Extract data section\n",
    "        data_section = content[data_start:].split(':DATA')[1].strip()\n",
    "    \n",
    "    # Extract specific metadata\n",
    "    metadata = extract_metadata(metadata_lines)\n",
    "    \n",
    "    # Read the data section directly\n",
    "    try:\n",
    "        # Use StringIO to create a file-like object from the data section\n",
    "        raw_data = pd.read_csv(\n",
    "            io.StringIO(data_section), \n",
    "            sep='\\s+',  # Use whitespace as separator\n",
    "            header=None\n",
    "        )\n",
    "        raw_data = raw_data.T\n",
    "        # Add metadata columns to the DataFrame\n",
    "        if 'SW_FREQ' in metadata:\n",
    "            raw_data['Freq(kHz)'] = metadata['SW_FREQ']\n",
    "            # raw_data['Freq(Hz)'] = raw_data['Freq(Hz)'].apply(lambda x: x*1000)\n",
    "        \n",
    "        if 'LEVELS' in metadata:\n",
    "            # Repeat levels to match the number of rows\n",
    "            levels_repeated = metadata['LEVELS'] * (len(raw_data) // len(metadata['LEVELS']) + 1)\n",
    "            raw_data['Level(dB)'] = levels_repeated[:len(raw_data)]\n",
    "        \n",
    "        filtered_data = raw_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "        filtered_data.columns = filtered_data.columns.map(str)\n",
    "\n",
    "        columns = ['Freq(kHz)'] + ['Level(dB)'] + [col for col in filtered_data.columns if col.isnumeric() == True]\n",
    "        filtered_data = filtered_data[columns]\n",
    "        return filtered_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data: {e}\")\n",
    "        return None, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d19998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks_troughs_amp_final(df, freq, db, time_scale=10, multiply_y_factor=1.0, units='Microvolts'):\n",
    "    db_column = 'Level(dB)'\n",
    "    \n",
    "    khz = df[(df['Freq(kHz)'] == freq) & (df[db_column] == db)]\n",
    "    if not khz.empty:\n",
    "        index = khz.index.values[0]\n",
    "        final = df.loc[index, '0':].dropna()\n",
    "        final = pd.to_numeric(final, errors='coerce').dropna()\n",
    "\n",
    "        target = int(244 * (time_scale / 10))\n",
    "        \n",
    "        # Process the wave as in calculate_and_plot_wave\n",
    "        y_values = interpolate_and_smooth(final, target)\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        y_values *= multiply_y_factor\n",
    "        \n",
    "        # Handle units conversion if needed\n",
    "        if units == 'Nanovolts':\n",
    "            y_values /= 1000\n",
    "            \n",
    "        # Generate normalized version for peak finding\n",
    "        y_values_fpf = interpolate_and_smooth(y_values[:244])\n",
    "        \n",
    "        # Standardize and normalize for peak finding, exactly as in the original\n",
    "        flattened_data = y_values_fpf.flatten().reshape(-1, 1)\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(flattened_data)\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = min_max_scaler.fit_transform(standardized_data).reshape(y_values_fpf.shape)\n",
    "        y_values_fpf = interpolate_and_smooth(scaled_data[:244])\n",
    "        \n",
    "        # Find peaks using the normalized data\n",
    "        highest_peaks, relevant_troughs = peak_finding(y_values_fpf)\n",
    "        \n",
    "        # Calculate amplitude on the processed but non-normalized data\n",
    "        if highest_peaks.size > 0 and relevant_troughs.size > 0:\n",
    "            # Following the same approach as in the display_metrics_table function\n",
    "            first_peak_amplitude = y_values[highest_peaks[0]] - y_values[relevant_troughs[0]]\n",
    "            return highest_peaks, relevant_troughs, first_peak_amplitude\n",
    "    \n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "dc38001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_interpolation(df, freq, db, time_scale=10, multiply_y_factor=1.0, units='Microvolts'):\n",
    "    \n",
    "    khz = df[(df['Freq(kHz)'] == freq) & (df['Level(dB)'] == db)]\n",
    "    # print(khz)\n",
    "    if not khz.empty:\n",
    "        index = khz.index.values[0]\n",
    "        final = df.loc[index, '0':].dropna()\n",
    "        final = pd.to_numeric(final, errors='coerce').dropna()\n",
    "\n",
    "        target = int(244 * (time_scale / 10))\n",
    "        \n",
    "        # Process the wave as in calculate_and_plot_wave\n",
    "        y_values = interpolate_and_smooth(final, target)\n",
    "\n",
    "        # print(f\"Interpolated y_values: {y_values[:5]}\")\n",
    "        # print(f\"Any NaNs? {np.isnan(y_values).any()}\")\n",
    "\n",
    "        if final.empty:\n",
    "            print(f\"Warning: Empty waveform for {freq}kHz @ {db}dB\")\n",
    "            return np.full((1, 244), np.nan)\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        y_values *= multiply_y_factor\n",
    "        \n",
    "        # Handle units conversion if needed\n",
    "        if units == 'Nanovolts':\n",
    "            y_values /= 1000\n",
    "            \n",
    "        # Generate normalized version for peak finding\n",
    "        y_values_fpf = interpolate_and_smooth(y_values[:244])\n",
    "        \n",
    "        # Standardize and normalize for peak finding, exactly as in the original\n",
    "        flattened_data = y_values_fpf.flatten().reshape(-1, 1)\n",
    "        scaler = StandardScaler()\n",
    "        standardized_data = scaler.fit_transform(flattened_data)\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = min_max_scaler.fit_transform(standardized_data).reshape(y_values_fpf.shape)\n",
    "    \n",
    "        return scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373edae8",
   "metadata": {},
   "source": [
    "# 1. Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6a4497b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del int\n",
    "time_scale = 18\n",
    "amp_per_freq = {'Subject': [], 'Freq(kHz) (x1)': [], 'Level(dB) (x2)': [], 'Amplitude (x3)':[]}\n",
    "start_path = '/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/WPZ Electrophysiology'\n",
    "for subject in os.listdir(start_path):\n",
    "    # print(\"Subject:\",subject)\n",
    "    for fq in os.listdir(os.path.join(start_path,subject)):\n",
    "        # print(fq)\n",
    "        if fq.startswith('ABR') and fq.endswith('.tsv'):\n",
    "            path = os.path.join(start_path,subject,fq)\n",
    "            data_df = read_custom_tsv(path)\n",
    "            # print(data_df)\n",
    "            freqs = data_df['Freq(kHz)'].unique().tolist()\n",
    "            levels = data_df['Level(dB)'].unique().tolist()\n",
    "            for freq in freqs:\n",
    "                for lvl in levels:\n",
    "                    # print(\"Frequency=\",freq, \"Level=\", lvl)\n",
    "                    _, _, amp = peaks_troughs_amp_final(df=data_df, freq=freq, db=lvl, time_scale=time_scale)\n",
    "                    # print(f'Amplitude: {amp}\\n')\n",
    "                    amp_per_freq['Subject'].append(subject)\n",
    "                    amp_per_freq['Freq(kHz) (x1)'].append(freq)\n",
    "                    amp_per_freq['Level(dB) (x2)'].append(lvl)\n",
    "                    amp_per_freq['Amplitude (x3)'].append(amp)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "amp_df_full = pd.DataFrame(data=amp_per_freq)\n",
    "\n",
    "raw_synapse_counts = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Ribbon and Synapse Counts.xlsx')\n",
    "raw_synapse_counts = raw_synapse_counts.mask(lambda x: x.isnull()).dropna()\n",
    "raw_synapse_counts['Synapses to IHC (y1)'] = raw_synapse_counts.iloc[:,6]\n",
    "raw_synapse_counts['vx (x4)'] = raw_synapse_counts['vx']\n",
    "raw_synapse_counts.drop(columns=['vx'], inplace=True)\n",
    "raw_synapse_counts.rename(columns={'Freq':'Freq(kHz) (x1)'}, inplace=True)\n",
    "# raw_synapse_counts['Freq(Hz) (x1)'] = raw_synapse_counts['Freq(Hz) (x1)'].apply(lambda x: x*1000) # PUTTING BACK\n",
    "raw_synapse_counts.rename(columns={'Case':'Subject'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7621c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 - values per vx\n",
    "\n",
    "paired = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(kHz) (x1)']), on=['Subject', 'Freq(kHz) (x1)'])\n",
    "# slice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs (y2)']]\n",
    "final = paired[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs']]\n",
    "final_clean = final.dropna()\n",
    "\n",
    "# adding in the strain feature\n",
    "strains = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Mouse groups.xlsx')\n",
    "final_clean_strained = final_clean.join(strains.set_index('ID#'), on='Subject')\n",
    "final_clean_strained['Strain'] = final_clean_strained['Strain'].str.strip()\n",
    "final_clean_strained = final_clean_strained.rename(columns={'Strain': 'Strain (x5)'})\n",
    "final_clean_strained = final_clean_strained.dropna()\n",
    "final_clean_strained = final_clean_strained[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Synapses to IHC (y1)', 'Group']]\n",
    "\n",
    "final_clean_strained_grouped = final_clean_strained.copy()\n",
    "final_clean_strained_grouped['Group - dB'] = final_clean_strained_grouped['Group'].apply(lambda x: x.split(' ')[0] if x.split(' ')[0].endswith('dB') else 'Control')\n",
    "final_clean_strained_grouped['Group - Time Elapsed'] = final_clean_strained_grouped['Group'].apply(lambda x: x.split(' ')[1] if x.split(' ')[1].endswith(('h', 'wks', 'w')) else x.split(' ')[0])\n",
    "final_clean_strained_grouped.head()\n",
    "\n",
    "final_clean_strained_grouped_pos = final_clean_strained_grouped.copy()\n",
    "final_clean_strained_grouped_pos['Amplitude (x3)'] = final_clean_strained_grouped['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup = final_clean_strained_grouped_pos.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup['Group'] = final_clean_strained_grouped_pos_cleangroup['Group'].apply(lambda x: x.strip())\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup.head()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs = final_clean_strained_grouped_pos_cleangroup.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - dB']\n",
    "# final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed']\n",
    "final_clean_strained_grouped_pos_cleangroup_vs = final_clean_strained_grouped_pos_cleangroup_vs[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)',\n",
    "       'vx (x4)', 'Strain (x5)','Group - dB (x6)', 'Group - Time Elapsed', 'Group','Synapses to IHC (y1)']]\n",
    "\n",
    "def split_on_number(input_string):\n",
    "    return re.findall(r\"[A-Za-z]+|\\d+\", input_string)\n",
    "\n",
    "hrs_week = 24*7\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed = final_clean_strained_grouped_pos_cleangroup_vs.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: '0dB' if x == 'Control' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - dB (x6)'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Magn.'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Split'].apply(lambda x: x[1])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Time Elapsed - Unit'].apply(lambda x: \"wks\" if x == 'w' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed['Group - Hours Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed.apply(lambda row: row['Group - Time Elapsed - Magn.']* hrs_week if row['Group - Time Elapsed - Unit'] == 'wks' else row['Group - Time Elapsed - Magn.'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a58a461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        9.831461\n",
       "0        9.831461\n",
       "1        9.831461\n",
       "1        9.831461\n",
       "2        9.831461\n",
       "          ...    \n",
       "7328    16.170213\n",
       "7329    16.170213\n",
       "7329    16.170213\n",
       "7330    16.170213\n",
       "7330    16.170213\n",
       "Name: Synapse to IHC Ratio per Freq (y2), Length: 12187, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2 - Averaged per Vx\n",
    "\n",
    "paired2 = amp_df_full.join(raw_synapse_counts.set_index(['Subject', 'Freq(kHz) (x1)']), on=['Subject', 'Freq(kHz) (x1)'])\n",
    "# lilslice = paired[paired['Subject']=='WPZ174'][['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'IHCs']]\n",
    "final2 = paired2[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)','Synapses to IHC (y1)', 'Synapses', 'IHCs']]\n",
    "final_clean2 = final2.dropna()\n",
    "\n",
    "# adding in the strain feature\n",
    "strains = pd.read_excel('/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/WPZ Mouse groups.xlsx')\n",
    "final_clean_strained2 = final_clean2.join(strains.set_index('ID#'), on='Subject')\n",
    "final_clean_strained2['Strain'] = final_clean_strained2['Strain'].str.strip()\n",
    "final_clean_strained2 = final_clean_strained2.rename(columns={'Strain': 'Strain (x5)'})\n",
    "final_clean_strained2 = final_clean_strained2.dropna()\n",
    "final_clean_strained2 = final_clean_strained2[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)', 'vx (x4)', 'Strain (x5)', 'Synapses to IHC (y1)', 'Group', 'Synapses', 'IHCs']]\n",
    "# np.unique(final_clean_strained2['Group'])\n",
    "\n",
    "# final_clean_70 = final_clean[final_clean['Level(dB) (x2)'] >= 70.0]\n",
    "# final_clean_strained_70 = final_clean_strained[final_clean_strained['Level(dB) (x2)'] >= 70.0]\n",
    "# # np.unique(final_clean['Level(dB) (x2)']) max level is 80 db\n",
    "# len(final_clean), len(final_clean_70) # 10000 less data points!!!\n",
    "\n",
    "final_clean_strained_grouped2 = final_clean_strained2.copy()\n",
    "final_clean_strained_grouped2['Group - dB'] = final_clean_strained_grouped2['Group'].apply(lambda x: x.split(' ')[0] if x.split(' ')[0].endswith('dB') else 'Control')\n",
    "final_clean_strained_grouped2['Group - Time Elapsed'] = final_clean_strained_grouped2['Group'].apply(lambda x: x.split(' ')[1] if x.split(' ')[1].endswith(('h', 'wks', 'w')) else x.split(' ')[0])\n",
    "final_clean_strained_grouped2.head()\n",
    "\n",
    "final_clean_strained_grouped_pos2 = final_clean_strained_grouped2.copy()\n",
    "final_clean_strained_grouped_pos2['Amplitude (x3)'] = final_clean_strained_grouped2['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "len(final_clean_strained_grouped_pos2[final_clean_strained_grouped_pos2['Amplitude (x3)'] < 0])\n",
    "\n",
    "final_clean_strained_grouped_pos2['Amplitude (x3)'] = final_clean_strained_grouped2['Amplitude (x3)'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "# final_clean_strained_grouped_pos[(final_clean_strained_grouped_pos['Subject'] == 'WPZ66') & (final_clean_strained_grouped_pos['Amplitude (x3)'] ==0.055901451434921576)\n",
    "final_clean_strained_grouped_pos_cleangroup2 = final_clean_strained_grouped_pos2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup2['Group'] = final_clean_strained_grouped_pos_cleangroup2['Group'].apply(lambda x: x.strip())\n",
    "np.unique(final_clean_strained_grouped_pos_cleangroup2['Group'])\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup2.head()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2 = final_clean_strained_grouped_pos_cleangroup2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs2['Group - dB']\n",
    "# final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs['Group - Time Elapsed']\n",
    "final_clean_strained_grouped_pos_cleangroup_vs2 = final_clean_strained_grouped_pos_cleangroup_vs2[['Subject', 'Freq(kHz) (x1)', 'Level(dB) (x2)', 'Amplitude (x3)',\n",
    "       'vx (x4)', 'Strain (x5)','Group - dB (x6)', 'Group - Time Elapsed', 'Group','Synapses to IHC (y1)', 'Synapses', 'IHCs']]\n",
    "\n",
    "def split_on_number(input_string):\n",
    "    return re.findall(r\"[A-Za-z]+|\\d+\", input_string)\n",
    "\n",
    "hrs_week = 24*7\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2 = final_clean_strained_grouped_pos_cleangroup_vs2.copy()\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: '0dB' if x == 'Control' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - dB (x6)'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed'].apply(split_on_number)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'].apply(lambda x: x[0])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Magn.'].apply(lambda x: int(x.strip()))\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Split'].apply(lambda x: x[1])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Time Elapsed - Unit'].apply(lambda x: \"wks\" if x == 'w' else x)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2['Group - Hours Elapsed (x7)'] = final_clean_strained_grouped_pos_cleangroup_vs_timed2.apply(lambda row: row['Group - Time Elapsed - Magn.']* hrs_week if row['Group - Time Elapsed - Unit'] == 'wks' else row['Group - Time Elapsed - Magn.'], axis = 1)\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2\n",
    "\n",
    "freqs = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2['Freq(kHz) (x1)'])\n",
    "subs = np.unique(final_clean_strained_grouped_pos_cleangroup_vs_timed2['Subject'])\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx = final_clean_strained_grouped_pos_cleangroup_vs_timed2.copy()\n",
    "for freq in freqs:\n",
    "    for sub in subs:\n",
    "        mask = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)] # global for updates\n",
    "        if len(mask) > 0:\n",
    "\n",
    "            mask1 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['vx (x4)'] == 'v1')]\n",
    "            mask2 = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub) & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['vx (x4)'] == 'v2')]\n",
    "\n",
    "            if not mask1.empty and not mask2.empty:\n",
    "                mask1 = mask1.reset_index().iloc[0,:]\n",
    "                mask2 = mask2.reset_index().iloc[0,:]\n",
    "\n",
    "                total_syns = float(mask1['Synapses'] + mask2['Synapses'])\n",
    "                total_ihcs = float(mask1['IHCs'] + mask2['IHCs'])\n",
    "                ratio = total_syns / total_ihcs\n",
    "                # print(total_syns, total_ihcs)\n",
    "                # if total_syns == 0.0 or total_ihcs == 0.0:\n",
    "                #     print(sub, freq)\n",
    "                mask_index = mask.index\n",
    "                final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx.loc[mask_index,'Synapse to IHC Ratio per Freq (y2)'] = ratio\n",
    "\n",
    "final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Synapse to IHC Ratio per Freq (y2)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45ab6f",
   "metadata": {},
   "source": [
    "# 2. Waves as Inputs (FLM) - no additional predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf123df",
   "metadata": {},
   "source": [
    "## a) Extract data as waves from ABR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b62ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping subject WPZ144, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ161, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ156, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ146, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ178, frequency 11.3 due to mismatch.\n",
      "Skipping subject WPZ40, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ98, frequency 45.2 due to mismatch.\n",
      "Skipping subject WPZ155, frequency 8.0 due to mismatch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_scale = 18\n",
    "start_path = '/Users/leahashebir/Downloads/Manor_Practicum/liberman_data/abr_data/WPZ Electrophysiology'\n",
    "subject_ABRs = {}\n",
    "\n",
    "for subject in os.listdir(start_path):\n",
    "    for fq in os.listdir(os.path.join(start_path,subject)):\n",
    "        if fq.startswith('ABR') and fq.endswith('.tsv'):\n",
    "            match = re.search(r'-L-([\\d.]+)\\.tsv$', fq)\n",
    "            if match:\n",
    "                freq = float(match.group(1))\n",
    "                # if freq == 6.0 or freq == 7.0:\n",
    "                #     print(subject, freq)\n",
    "                # freqs.add(freq)\n",
    "                path = os.path.join(start_path,subject,fq)\n",
    "                data_df = read_custom_tsv(path)\n",
    "                if data_df['Freq(kHz)'].iloc[0] == freq:\n",
    "                    subject_ABRs[(subject, freq)] = data_df\n",
    "                else:\n",
    "                    print(f\"Skipping subject {subject}, frequency {freq} due to mismatch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad796ba",
   "metadata": {},
   "source": [
    "## b) Transform using skfda.DataGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0337324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_scale = 18\n",
    "subjects = []\n",
    "frequencies = []\n",
    "levels = []\n",
    "waves = []\n",
    "ys = []\n",
    "fails = []\n",
    "\n",
    "for (sub, freq), df in subject_ABRs.items():\n",
    "    for lvl in np.unique(subject_ABRs[(sub, freq)]['Level(dB)']):\n",
    "        lvl = float(lvl)\n",
    "        wave = full_interpolation(df, freq, lvl, time_scale)\n",
    "        wave = np.asarray(wave, dtype=float)\n",
    "        # if wave.size != 244:\n",
    "        #     fails.append((sub, freq, lvl))\n",
    "        # else:\n",
    "        wave_input = skfda.FDataGrid(\n",
    "            data_matrix=wave,\n",
    "            grid_points=time_scale * np.arange(0, 244) / 244\n",
    "        )\n",
    "        y = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Level(dB) (x2)'] == lvl)]\\\n",
    "            ['Synapse to IHC Ratio per Freq (y2)']\n",
    "        \n",
    "        if len(y) > 0:\n",
    "            y = float(y.iloc[0])\n",
    "\n",
    "        # print((sub, freq, lvl, y))\n",
    "        subjects.append(sub)\n",
    "        frequencies.append(freq)\n",
    "        levels.append(lvl)\n",
    "        waves.append(wave_input)\n",
    "        ys.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d3dec48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_subs = set()\n",
    "for fail in fails:\n",
    "    # print(fail[0])\n",
    "    fail_subs.add(fail[1])\n",
    "# fail_subs\n",
    "len(fails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d973b",
   "metadata": {},
   "source": [
    "## c) Create and fit data to new bases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05bacc",
   "metadata": {},
   "source": [
    "Creating a new basis is key for a functional regression. It brings the various waveforms into lower dimensional space by isolating the signal and filtering out noise. Basis-spline or B-spline is particularly useful for ABR waveforms:\n",
    "- Piecewise function = can control amount of smoothing and coefficient weighting *locally* (great for analyzing distinct peaks/troughs)\n",
    "    - We should use 5 knots or \"pieces\", 1 knot for each wave. \n",
    "- Does not assume periodicity like Fourier basis, giving a more natural fit\n",
    "- Cubic spline is standard for waveform data to capture wave variability without overly smoothing\n",
    "    - 7 basis pieces (5 + 3 -1) AKA we are splitting each waveform into 7 independent blocks\n",
    "\n",
    "https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-6/Functional-linear-regression-analysis-for-longitudinal-data/10.1214/009053605000000660.full\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/B-spline\n",
    "\n",
    "\n",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC5598560/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fad54b",
   "metadata": {},
   "source": [
    "$f(t) = c_1 \\, \\phi_1(t) + c_2 \\, \\phi_2(t) + c_3 \\, \\phi_3(t) + c_4 \\, \\phi_4(t) + c_5 \\, \\phi_5(t) + c_6 \\, \\phi_6(t) + c_7 \\, \\phi_7(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "81e90532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update timepoints post discussion/research!!!!!\n",
    "\n",
    "# wave_timepoints = []\n",
    "# basis = BSplineBasis(domain_range=(0,244), knots=wave_timepoints, order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "0d62d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfda.misc.regularization import L2Regularization\n",
    "from skfda.misc.operators import LinearDifferentialOperator\n",
    "\n",
    "penalty = L2Regularization(linear_operator=LinearDifferentialOperator(2))\n",
    "shared_grid = np.linspace(0, 18, 244)\n",
    "basis = BSplineBasis(n_basis=7, domain_range=(0,18))\n",
    "smoother = BasisSmoother(basis=basis, return_basis=True) # , regularization=penalty, smoothing_parameter=0.1, \n",
    "time_scale = 18\n",
    "subjects = []\n",
    "frequencies = []\n",
    "levels = []\n",
    "raw_waves = []\n",
    "newbasis_waves = []\n",
    "Xs = []\n",
    "ys = []\n",
    "fails = []\n",
    "\n",
    "for (sub, freq), df in subject_ABRs.items():\n",
    "    for lvl in np.unique(subject_ABRs[(sub, freq)]['Level(dB)']):\n",
    "        lvl = float(lvl)\n",
    "        wave = full_interpolation(df, freq, lvl, time_scale)\n",
    "        wave = np.asarray(wave, dtype=float)\n",
    "        wave = wave.reshape(1, -1)\n",
    "        \n",
    "        grid = time_scale * np.arange(0, 244) / 244\n",
    "        wave_input = skfda.FDataGrid(data_matrix=wave,grid_points=shared_grid)\n",
    "\n",
    "        wave_newbasis = smoother.fit_transform(wave_input)[0] # smoother will allow regularization for further tuning down the line...\n",
    "\n",
    "        X = wave_newbasis.coefficients\n",
    "\n",
    "        y_series = final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx[(final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Subject'] == sub)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Freq(kHz) (x1)'] == freq)\\\n",
    "            & (final_clean_strained_grouped_pos_cleangroup_vs_timed2_avgvx['Level(dB) (x2)'] == lvl)]\\\n",
    "            ['Synapse to IHC Ratio per Freq (y2)']\n",
    "        \n",
    "        # print(y_series)\n",
    "        \n",
    "        if len(y_series) == 0 or pd.isna(y_series.iloc[0]):\n",
    "            # print(f'N/A y: ({sub}, {freq}, {lvl})')\n",
    "            continue\n",
    "\n",
    "        y = float(y_series.iloc[0])\n",
    "\n",
    "        # print((sub, freq, lvl, y))\n",
    "        subjects.append(sub)\n",
    "        frequencies.append(freq)\n",
    "        levels.append(lvl)\n",
    "        raw_waves.append(wave_input)\n",
    "        newbasis_waves.append(wave_newbasis)\n",
    "        Xs.append(X.flatten()) # used for model fitting, same as for OLS!\n",
    "        ys.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "c53e85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5869 entries, 0 to 5868\n",
      "Data columns (total 5 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   Subject                             5869 non-null   object \n",
      " 1   Freq(kHz)                           5869 non-null   float64\n",
      " 2   Level(dB)                           5869 non-null   float64\n",
      " 3   Transformed Waves (X)               5869 non-null   object \n",
      " 4   Synapse to IHC Ratio per Freq (y2)  5869 non-null   float64\n",
      "dtypes: float64(3), object(2)\n",
      "memory usage: 229.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5869"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_waves_df = pd.DataFrame(data = {'Subject' : subjects, 'Freq(kHz)' : frequencies, 'Level(dB)' : levels, 'Transformed Waves (X)' : Xs, 'Synapse to IHC Ratio per Freq (y2)' : ys})\n",
    "final_waves_df_clean = final_waves_df.dropna().reset_index(drop=True)\n",
    "final_waves_df_clean.info()\n",
    "len(final_waves_df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c1f21",
   "metadata": {},
   "source": [
    "# 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "53e3e074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.float64(2.9577290244143044),\n",
       "  np.float64(2.841690398298992),\n",
       "  np.float64(2.6813749183069255),\n",
       "  np.float64(2.9363979625918573),\n",
       "  np.float64(2.9116541112265084)],\n",
       " [np.float64(2.5184042883420505),\n",
       "  np.float64(3.0010926672019416),\n",
       "  np.float64(3.589774857373482),\n",
       "  np.float64(2.6247595515730144),\n",
       "  np.float64(2.715894584594723)])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = final_waves_df_clean['Transformed Waves (X)']\n",
    "X_new = np.vstack([x.flatten() for x in X])\n",
    "\n",
    "y = final_waves_df_clean['Synapse to IHC Ratio per Freq (y2)']\n",
    "\n",
    "kf = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_rmse, test_rmse = [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X=X_new,y=y,groups=final_waves_df_clean['Subject']):\n",
    "    X_train, X_test = X_new[train_idx], X_new[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_hat = model.predict(X_train)\n",
    "    y_test_hat = model.predict(X_test)\n",
    "\n",
    "    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_hat)))\n",
    "    test_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_hat)))\n",
    "\n",
    "train_rmse, test_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84791c1",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "- Add regularization\n",
    "- Check why so many N/A\n",
    "- Fix the knots to fit to 5 waves\n",
    "    - If bad, experiment with different settings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
